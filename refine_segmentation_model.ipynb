{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3a7e14ac",
      "metadata": {
        "id": "3a7e14ac"
      },
      "source": [
        "## Setup: Google Colab Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "02620f41",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02620f41",
        "outputId": "bd545725-3a01-47cf-c667-640355798c36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No GPU available, using CPU (training will be slow)\n",
            "\n",
            "PyTorch version: 2.9.0+cpu\n",
            "CUDA version: None\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Verify CUDA is available\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "    print(f'GPU available: {torch.cuda.get_device_name(0)}')\n",
        "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')\n",
        "else:\n",
        "    device = 'cpu'\n",
        "    print('No GPU available, using CPU (training will be slow)')\n",
        "\n",
        "print(f'\\nPyTorch version: {torch.__version__}')\n",
        "print(f'CUDA version: {torch.version.cuda}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1b038eae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b038eae",
        "outputId": "bc6a0542-8c83-4dd8-fb6d-2f3df0710c08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hAll packages installed successfully\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install -q ultralytics opencv-python-headless pillow pyyaml numpy scipy matplotlib pandas\n",
        "\n",
        "print('All packages installed successfully')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "0b48dbb1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b48dbb1",
        "outputId": "fc720a5b-0c60-45ab-c6ff-d06f25dfa389"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Drive mounted and working directory set\n",
            "Data config: /content/drive/MyDrive/csc173_dataset/dataset/data.yaml\n",
            "Pretrained model: /content/drive/MyDrive/csc173_dataset/models/custom_ocr_last.pt\n",
            "Working directory: /content/refined_training\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive (if using dataset from Drive)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set paths - MODIFY THESE TO YOUR ACTUAL PATHS\n",
        "DRIVE_ROOT = '/content/drive/MyDrive/csc173_dataset'\n",
        "DATA_YAML_PATH = f'{DRIVE_ROOT}/dataset/data.yaml'\n",
        "PRETRAINED_MODEL = f'{DRIVE_ROOT}/models/custom_ocr_last.pt'  # Your existing checkpoint\n",
        "\n",
        "# Create local working directory\n",
        "WORK_DIR = '/content/refined_training'\n",
        "os.makedirs(WORK_DIR, exist_ok=True)\n",
        "os.chdir(WORK_DIR)\n",
        "\n",
        "print(f'Drive mounted and working directory set')\n",
        "print(f'Data config: {DATA_YAML_PATH}')\n",
        "print(f'Pretrained model: {PRETRAINED_MODEL}')\n",
        "print(f'Working directory: {WORK_DIR}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6201797b",
      "metadata": {
        "id": "6201797b"
      },
      "source": [
        "## Copy Dataset to Local Storage (Fixes Slow I/O)\n",
        "\n",
        "Google Drive mounting is extremely slow for datasets with many small files. We'll copy the dataset to Colab's local SSD once per session for fast training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b2cd3d33",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2cd3d33",
        "outputId": "83bd4f3b-b3b6-45a8-ef2c-c489ab7e35c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset from zip to local SSD...\n",
            "This is a ONE-TIME operation per Colab session (much faster than copying many files)\n",
            "================================================================================\n",
            "Source: /content/drive/MyDrive/csc173_dataset/dataset.zip\n",
            "Destination: /content/local_dataset\n",
            "\n",
            "Extracting zip file (this is MUCH faster than copying individual files)...\n",
            "\n",
            "‚úì Dataset extracted in 2.1 minutes\n",
            "\n",
            "Updated paths:\n",
            "  Dataset: /content/local_dataset\n",
            "  data.yaml: /content/local_dataset/data.yaml\n",
            "  Model: /content/drive/MyDrive/csc173_dataset/models/custom_ocr_last.pt (still on Drive - small file, no issue)\n",
            "\n",
            "Training will now use LOCAL SSD\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "import time\n",
        "import zipfile\n",
        "\n",
        "print('Extracting dataset from zip to local SSD...')\n",
        "print('This is a ONE-TIME operation per Colab session (much faster than copying many files)')\n",
        "print('=' * 80)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Local paths (on Colab's SSD - FAST!)\n",
        "LOCAL_DATASET_ROOT = '/content/local_dataset'\n",
        "LOCAL_DATA_YAML = f'{LOCAL_DATASET_ROOT}/data.yaml'\n",
        "\n",
        "# Path to the zip file on Google Drive\n",
        "DATASET_ZIP = f'{DRIVE_ROOT}/dataset.zip'\n",
        "\n",
        "# Check if already extracted\n",
        "if Path(LOCAL_DATASET_ROOT).exists() and Path(LOCAL_DATA_YAML).exists():\n",
        "    print('‚úì Dataset already extracted to local storage')\n",
        "else:\n",
        "    # Extract dataset from zip file\n",
        "    print(f'Source: {DATASET_ZIP}')\n",
        "    print(f'Destination: {LOCAL_DATASET_ROOT}')\n",
        "    print()\n",
        "\n",
        "    if not Path(DATASET_ZIP).exists():\n",
        "        print(f'   ERROR: dataset.zip not found at {DATASET_ZIP}')\n",
        "        print(f'   Please ensure dataset.zip is uploaded to {DRIVE_ROOT}/')\n",
        "        raise FileNotFoundError(f'dataset.zip not found at {DATASET_ZIP}')\n",
        "\n",
        "    print('Extracting zip file (this is MUCH faster than copying individual files)...')\n",
        "\n",
        "    # Create parent directory\n",
        "    Path(LOCAL_DATASET_ROOT).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Extract the zip file\n",
        "    with zipfile.ZipFile(DATASET_ZIP, 'r') as zip_ref:\n",
        "        zip_ref.extractall('/content')\n",
        "\n",
        "    # Check if extraction created a nested 'dataset' folder\n",
        "    if Path('/content/dataset').exists():\n",
        "        # Move contents from /content/dataset to LOCAL_DATASET_ROOT\n",
        "        shutil.move('/content/dataset', LOCAL_DATASET_ROOT)\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f'\\n‚úì Dataset extracted in {elapsed/60:.1f} minutes')\n",
        "\n",
        "# Update paths to use LOCAL storage instead of Drive\n",
        "DATA_YAML_PATH = LOCAL_DATA_YAML\n",
        "\n",
        "print()\n",
        "print('Updated paths:')\n",
        "print(f'  Dataset: {LOCAL_DATASET_ROOT}')\n",
        "print(f'  data.yaml: {DATA_YAML_PATH}')\n",
        "print(f'  Model: {PRETRAINED_MODEL} (still on Drive - small file, no issue)')\n",
        "print()\n",
        "print('Training will now use LOCAL SSD')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cb3737f",
      "metadata": {
        "id": "9cb3737f"
      },
      "source": [
        "## Core Components (Reused from Original Training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7000dfec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7000dfec",
        "outputId": "4e041e57-a840-4950-bc2c-7950b2f64845"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes: 36\n",
            "Characters: ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\n",
            "Similarity matrix initialized: torch.Size([36, 36])\n"
          ]
        }
      ],
      "source": [
        "# Character set and similarity matrix (from original)\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "CHARS = [chr(i) for i in range(65, 91)] + [str(i) for i in range(10)]\n",
        "NUM_CLASSES = len(CHARS)\n",
        "CHAR_TO_IDX = {c: i for i, c in enumerate(CHARS)}\n",
        "IDX_TO_CHAR = {i: c for i, c in enumerate(CHARS)}\n",
        "\n",
        "print(f'Number of classes: {NUM_CLASSES}')\n",
        "print(f'Characters: {\"\".join(CHARS)}')\n",
        "\n",
        "SIMILAR_GROUPS = [\n",
        "    ['O', '0'], # I want to refine Q and 0/O differentiation, so it was removed from the group\n",
        "    ['I', '1'], # I want to refine L and 1/I differentiation, so it was removed from the group\n",
        "    ['S', '5'],\n",
        "    ['Z', '2'],\n",
        "    ['B', '8'],\n",
        "    ['D', '0'],\n",
        "    ['G', 'C'],\n",
        "    ['U', 'V'],\n",
        "    ['P', 'R'],\n",
        "]\n",
        "\n",
        "def create_similarity_matrix(num_classes=NUM_CLASSES, groups=SIMILAR_GROUPS, base_sim=0.6):\n",
        "    S = np.zeros((num_classes, num_classes), dtype=np.float32)\n",
        "    np.fill_diagonal(S, 1.0)\n",
        "    for group in groups:\n",
        "        idxs = [CHAR_TO_IDX[c] for c in group if c in CHAR_TO_IDX]\n",
        "        for i in idxs:\n",
        "            for j in idxs:\n",
        "                if i != j:\n",
        "                    S[i, j] = base_sim\n",
        "    return torch.tensor(S, dtype=torch.float32)\n",
        "\n",
        "similarity_matrix = create_similarity_matrix()\n",
        "print(f'Similarity matrix initialized: {similarity_matrix.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0ebec263",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ebec263",
        "outputId": "a67586d0-fa2b-4731-a5c3-96908537856b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Refined similarity-aware loss defined\n"
          ]
        }
      ],
      "source": [
        "# Enhanced Similarity-Aware Loss with Adaptive Weighting\n",
        "class RefinedSimilarityAwareTopKLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Enhanced loss for fine-tuning with:\n",
        "    - Higher penalty for similar character confusion\n",
        "    - Adaptive temperature based on training phase\n",
        "    - Confidence-based weighting\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=NUM_CLASSES, similarity_matrix=None,\n",
        "                 k=3, initial_temperature=0.5, base_weight=0.5, topk_weight=0.5,\n",
        "                 epochs=40):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.k = k\n",
        "        self.initial_temperature = initial_temperature\n",
        "        self.base_weight = base_weight\n",
        "        self.topk_weight = topk_weight\n",
        "        self.epochs = epochs\n",
        "        self.current_epoch = 0\n",
        "\n",
        "        if similarity_matrix is not None:\n",
        "            self.register_buffer('similarity_matrix', similarity_matrix)\n",
        "        else:\n",
        "            self.register_buffer('similarity_matrix', create_similarity_matrix())\n",
        "\n",
        "    def update_epoch(self, epoch):\n",
        "        \"\"\"Update current epoch for temperature annealing.\"\"\"\n",
        "        self.current_epoch = epoch\n",
        "\n",
        "    def get_temperature(self):\n",
        "        \"\"\"Anneal temperature more aggressively for fine-tuning.\"\"\"\n",
        "        progress = self.current_epoch / max(self.epochs, 1)\n",
        "        # Start at 0.5, go to 0.3 (sharper predictions)\n",
        "        return max(0.3, self.initial_temperature - progress * 0.2)\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        B = logits.size(0)\n",
        "        device = logits.device\n",
        "\n",
        "        temperature = self.get_temperature()\n",
        "\n",
        "        # Standard cross-entropy\n",
        "        ce_loss = F.cross_entropy(logits, targets, reduction='none')\n",
        "\n",
        "        # Softmax with temperature\n",
        "        probs = F.softmax(logits / temperature, dim=1)\n",
        "        topk_probs, topk_indices = torch.topk(probs, min(self.k, self.num_classes), dim=1)\n",
        "\n",
        "        # Similarity-aware penalty\n",
        "        sim_loss = torch.zeros(B, device=device)\n",
        "        confidence_scores = []\n",
        "\n",
        "        for i in range(B):\n",
        "            t = targets[i].item()\n",
        "            if t < 0 or t >= self.num_classes:\n",
        "                continue\n",
        "\n",
        "            sims = self.similarity_matrix[t][topk_indices[i]]\n",
        "\n",
        "            # Higher penalty for similar character confusion\n",
        "            # If model confuses O with 0 (high similarity), penalty is lower\n",
        "            # If model confuses O with X (low similarity), penalty is higher\n",
        "            penalties = (1.0 - sims) * 1.5  # Amplify penalty\n",
        "            weighted_penalties = topk_probs[i] * penalties\n",
        "            sim_loss[i] = weighted_penalties.sum()\n",
        "\n",
        "            confidence_scores.append(topk_probs[i][0].item())\n",
        "\n",
        "        if len(confidence_scores) == 0:\n",
        "            return ce_loss.mean()\n",
        "\n",
        "        # Adaptive weighting based on confidence\n",
        "        confidence = torch.tensor(confidence_scores, device=device)\n",
        "\n",
        "        # When confident: rely more on CE (trust the model)\n",
        "        # When uncertain: rely more on similarity (guide the model)\n",
        "        adaptive_base = self.base_weight + (1 - confidence) * 0.2\n",
        "        adaptive_topk = self.topk_weight + confidence * 0.2\n",
        "\n",
        "        # Normalize\n",
        "        total_weight = adaptive_base + adaptive_topk\n",
        "        adaptive_base = adaptive_base / total_weight\n",
        "        adaptive_topk = adaptive_topk / total_weight\n",
        "\n",
        "        total_loss = adaptive_base * ce_loss + adaptive_topk * sim_loss\n",
        "        return total_loss.mean()\n",
        "\n",
        "print('Refined similarity-aware loss defined')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4557780b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4557780b",
        "outputId": "489a5c1a-5233-4200-818c-d1646c43dea5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OCR metrics module loaded\n"
          ]
        }
      ],
      "source": [
        "# OCR Metrics (reused from original)\n",
        "class OCRMetrics:\n",
        "    \"\"\"Compute OCR-specific validation metrics.\"\"\"\n",
        "    def __init__(self, similarity_matrix=None):\n",
        "        self.similarity_matrix = similarity_matrix if similarity_matrix is not None else create_similarity_matrix()\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.total_chars = 0\n",
        "        self.correct_chars = 0\n",
        "        self.top2_correct = 0\n",
        "        self.top3_correct = 0\n",
        "        self.similarity_score = 0.0\n",
        "\n",
        "    def update(self, predictions, targets, top_k_preds=None):\n",
        "        predictions = predictions.cpu().numpy()\n",
        "        targets = targets.cpu().numpy()\n",
        "\n",
        "        self.total_chars += len(targets)\n",
        "        self.correct_chars += (predictions == targets).sum()\n",
        "\n",
        "        # Similarity-aware accuracy\n",
        "        for pred, target in zip(predictions, targets):\n",
        "            if 0 <= target < len(self.similarity_matrix) and 0 <= pred < len(self.similarity_matrix):\n",
        "                sim = self.similarity_matrix[target][pred].item()\n",
        "                self.similarity_score += sim\n",
        "\n",
        "        # Top-k accuracy\n",
        "        if top_k_preds is not None:\n",
        "            top_k_preds = top_k_preds.cpu().numpy()\n",
        "            for i, target in enumerate(targets):\n",
        "                if top_k_preds.shape[1] >= 2 and target in top_k_preds[i, :2]:\n",
        "                    self.top2_correct += 1\n",
        "                if top_k_preds.shape[1] >= 3 and target in top_k_preds[i, :3]:\n",
        "                    self.top3_correct += 1\n",
        "\n",
        "    def compute(self):\n",
        "        if self.total_chars == 0:\n",
        "            return {}\n",
        "\n",
        "        return {\n",
        "            'CER': 1.0 - (self.correct_chars / self.total_chars),\n",
        "            'char_accuracy': self.correct_chars / self.total_chars,\n",
        "            'top2_accuracy': self.top2_correct / self.total_chars,\n",
        "            'top3_accuracy': self.top3_correct / self.total_chars,\n",
        "            'similarity_aware_accuracy': self.similarity_score / self.total_chars,\n",
        "        }\n",
        "\n",
        "print('OCR metrics module loaded')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0e25dad",
      "metadata": {
        "id": "c0e25dad"
      },
      "source": [
        "## Refined Training Strategy\n",
        "\n",
        "### Phase 1: Classifier Head Fine-Tuning (Epochs 1-12)\n",
        "- Freeze backbone and segmentation head\n",
        "- Focus exclusively on improving character classification\n",
        "- Use cyclic learning rate to escape plateau\n",
        "\n",
        "### Phase 2: Progressive Unfreezing (Epochs 13-24)\n",
        "- Gradually unfreeze deeper layers\n",
        "- Lower learning rate for stable refinement\n",
        "- Continue with similarity-aware loss\n",
        "\n",
        "### Phase 3: Full Fine-Tuning (Epochs 25-40)\n",
        "- All layers unfrozen\n",
        "- Very low learning rate for final polish\n",
        "- Focus on reducing classification loss below 0.35"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e4287274",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4287274",
        "outputId": "cdcc4944-97c6-461d-a63d-a15746037fab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ‚úÖ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Refined segmentation trainer defined with checkpoint support\n"
          ]
        }
      ],
      "source": [
        "# Custom Trainer for Refined Training\n",
        "from ultralytics.models.yolo.segment import SegmentationTrainer\n",
        "from ultralytics import YOLO\n",
        "\n",
        "class RefinedSegmentationTrainer(SegmentationTrainer):\n",
        "    \"\"\"\n",
        "    Refined trainer with:\n",
        "    - Progressive layer unfreezing\n",
        "    - Enhanced loss function\n",
        "    - OCR-specific metrics tracking\n",
        "    - Cyclic learning rate support\n",
        "    - Checkpoint saving after each epoch\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg=None, overrides=None, _callbacks=None):\n",
        "        super().__init__(cfg, overrides, _callbacks)\n",
        "\n",
        "        # Get total epochs from config\n",
        "        total_epochs = self.args.epochs if hasattr(self.args, 'epochs') else 40\n",
        "\n",
        "        # Initialize refined loss\n",
        "        self.character_loss_fn = RefinedSimilarityAwareTopKLoss(\n",
        "            num_classes=NUM_CLASSES,\n",
        "            similarity_matrix=similarity_matrix,\n",
        "            k=3,\n",
        "            initial_temperature=0.5,\n",
        "            base_weight=0.5,\n",
        "            topk_weight=0.5,\n",
        "            epochs=total_epochs\n",
        "        ).to(device)\n",
        "\n",
        "        # OCR metrics\n",
        "        self.ocr_metrics = OCRMetrics(similarity_matrix=similarity_matrix)\n",
        "\n",
        "        # Training phase tracking\n",
        "        self.phase = 1\n",
        "        self.freeze_applied = False\n",
        "\n",
        "    def _setup_train(self, world_size):\n",
        "        \"\"\"Override to apply layer freezing for Phase 1.\"\"\"\n",
        "        super()._setup_train(world_size)\n",
        "\n",
        "        if not self.freeze_applied and self.epoch < 12:\n",
        "            print(f'\\n=== PHASE 1: Classifier Head Fine-Tuning (Epochs 1-12) ===')\n",
        "            print('Freezing backbone and segmentation layers...')\n",
        "\n",
        "            # Freeze all layers except classification head\n",
        "            for name, param in self.model.named_parameters():\n",
        "                # Keep classification layers trainable\n",
        "                if 'cls' in name.lower() or 'cv3' in name.lower():\n",
        "                    param.requires_grad = True\n",
        "                else:\n",
        "                    param.requires_grad = False\n",
        "\n",
        "            trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "            total = sum(p.numel() for p in self.model.parameters())\n",
        "            print(f'Trainable parameters: {trainable:,} / {total:,} ({100*trainable/total:.1f}%)')\n",
        "            self.freeze_applied = True\n",
        "\n",
        "    def on_train_epoch_start(self):\n",
        "        \"\"\"Handle phase transitions and progressive unfreezing.\"\"\"\n",
        "        super().on_train_epoch_start()\n",
        "\n",
        "        # Update temperature in loss\n",
        "        self.character_loss_fn.update_epoch(self.epoch)\n",
        "\n",
        "        # Phase 2: Progressive unfreezing (epochs 12-24)\n",
        "        if self.epoch == 12:\n",
        "            self.phase = 2\n",
        "            print(f'\\n=== PHASE 2: Progressive Unfreezing (Epochs 13-24) ===')\n",
        "            print('Unfreezing segmentation head...')\n",
        "\n",
        "            for name, param in self.model.named_parameters():\n",
        "                if 'seg' in name.lower() or 'mask' in name.lower():\n",
        "                    param.requires_grad = True\n",
        "\n",
        "            trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "            total = sum(p.numel() for p in self.model.parameters())\n",
        "            print(f'Trainable parameters: {trainable:,} / {total:,} ({100*trainable/total:.1f}%)')\n",
        "\n",
        "        # Phase 3: Full fine-tuning (epochs 24+)\n",
        "        elif self.epoch == 24:\n",
        "            self.phase = 3\n",
        "            print(f'\\n=== PHASE 3: Full Fine-Tuning (Epochs 25-40) ===')\n",
        "            print('Unfreezing all layers...')\n",
        "\n",
        "            for param in self.model.parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "            trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "            total = sum(p.numel() for p in self.model.parameters())\n",
        "            print(f'Trainable parameters: {trainable:,} / {total:,} ({100*trainable/total:.1f}%)')\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        \"\"\"Save checkpoint after each epoch.\"\"\"\n",
        "        super().on_train_epoch_end()\n",
        "\n",
        "        # Extract metrics from validator\n",
        "        if hasattr(self, 'metrics') and self.metrics is not None:\n",
        "            metrics_dict = {}\n",
        "\n",
        "            # Try to get metrics from the metrics object\n",
        "            if hasattr(self.metrics, 'results_dict'):\n",
        "                metrics_dict = self.metrics.results_dict\n",
        "\n",
        "            # Also get from validator\n",
        "            if hasattr(self, 'validator') and hasattr(self.validator, 'metrics'):\n",
        "                val_metrics = self.validator.metrics\n",
        "                if hasattr(val_metrics, 'results_dict'):\n",
        "                    metrics_dict.update(val_metrics.results_dict)\n",
        "\n",
        "            # Get loss values\n",
        "            if hasattr(self, 'loss_items'):\n",
        "                loss_items = self.loss_items\n",
        "                if loss_items is not None and len(loss_items) > 0:\n",
        "                    metrics_dict['train/cls_loss'] = float(loss_items[0]) if len(loss_items) > 0 else 0.0\n",
        "                    metrics_dict['train/seg_loss'] = float(loss_items[1]) if len(loss_items) > 1 else 0.0\n",
        "\n",
        "            # Get learning rate\n",
        "            if hasattr(self, 'optimizer'):\n",
        "                metrics_dict['lr/pg0'] = self.optimizer.param_groups[0]['lr']\n",
        "\n",
        "            # Save checkpoint (CSV every epoch, model every 10 epochs)\n",
        "            last_model = Path(self.save_dir) / 'weights' / 'last.pt'\n",
        "            save_checkpoint(self.epoch, metrics_dict, str(last_model), save_model_every=10)\n",
        "\n",
        "    def on_val_start(self):\n",
        "        super().on_val_start()\n",
        "        self.ocr_metrics.reset()\n",
        "\n",
        "    def on_val_end(self):\n",
        "        super().on_val_end()\n",
        "\n",
        "        # Log OCR metrics\n",
        "        ocr_results = self.ocr_metrics.compute()\n",
        "        if ocr_results:\n",
        "            print(f'\\n[Epoch {self.epoch}] OCR Metrics:')\n",
        "            for key, value in ocr_results.items():\n",
        "                print(f'  {key}: {value:.4f}')\n",
        "\n",
        "    def compute_loss(self, preds, batch):\n",
        "        \"\"\"Compute loss with refined similarity-aware classification.\"\"\"\n",
        "        # Get base YOLO losses\n",
        "        base_loss = super().compute_loss(preds, batch)\n",
        "\n",
        "        # Add custom similarity-aware character classification loss\n",
        "        if len(preds) > 3:\n",
        "            cls_logits = preds[3]\n",
        "            cls_targets = batch['cls'].long()\n",
        "\n",
        "            if cls_logits is not None and cls_targets is not None:\n",
        "                cls_logits_flat = cls_logits.view(-1, NUM_CLASSES)\n",
        "                cls_targets_flat = cls_targets.view(-1)\n",
        "\n",
        "                valid_mask = cls_targets_flat >= 0\n",
        "                if valid_mask.sum() > 0:\n",
        "                    # Compute refined similarity-aware loss\n",
        "                    char_loss = self.character_loss_fn(\n",
        "                        cls_logits_flat[valid_mask],\n",
        "                        cls_targets_flat[valid_mask]\n",
        "                    )\n",
        "\n",
        "                    # Update OCR metrics\n",
        "                    with torch.no_grad():\n",
        "                        preds_cls = cls_logits_flat[valid_mask].argmax(dim=1)\n",
        "                        top_k_preds = torch.topk(cls_logits_flat[valid_mask], k=3, dim=1)[1]\n",
        "                        self.ocr_metrics.update(\n",
        "                            preds_cls,\n",
        "                            cls_targets_flat[valid_mask],\n",
        "                            top_k_preds\n",
        "                        )\n",
        "\n",
        "                    # Phase-dependent weighting\n",
        "                    if self.phase == 1:\n",
        "                        # Phase 1: Heavy emphasis on classification\n",
        "                        cls_weight = 0.7\n",
        "                    elif self.phase == 2:\n",
        "                        # Phase 2: Balanced\n",
        "                        cls_weight = 0.5\n",
        "                    else:\n",
        "                        # Phase 3: Standard weighting\n",
        "                        cls_weight = 0.3\n",
        "\n",
        "                    total_loss = (1 - cls_weight) * base_loss + cls_weight * char_loss\n",
        "                    return total_loss\n",
        "\n",
        "        return base_loss\n",
        "\n",
        "print('Refined segmentation trainer defined with checkpoint support')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Checkpoint Implementation"
      ],
      "metadata": {
        "id": "MG9oF0yv81v3"
      },
      "id": "MG9oF0yv81v3"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "CHECKPOINT_DIR = f'{DRIVE_ROOT}/refined_checkpoints'\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "CHECKPOINT_CSV = f'{CHECKPOINT_DIR}/training_progress.csv'\n",
        "CHECKPOINT_MODEL = f'{CHECKPOINT_DIR}/checkpoint_latest.pt'\n",
        "\n",
        "def load_checkpoint_info():\n",
        "    \"\"\"Load checkpoint information if it exists.\"\"\"\n",
        "    if os.path.exists(CHECKPOINT_CSV):\n",
        "        df = pd.read_csv(CHECKPOINT_CSV)\n",
        "        if len(df) > 0:\n",
        "            last_row = df.iloc[-1]\n",
        "            return {\n",
        "                'exists': True,\n",
        "                'last_epoch': int(last_row['epoch']),\n",
        "                'total_epochs_trained': len(df),\n",
        "                'best_cls_loss': df['val/cls_loss'].min(),\n",
        "                'best_map': df['metrics/mAP50-95(M)'].max(),\n",
        "                'checkpoint_path': CHECKPOINT_MODEL if os.path.exists(CHECKPOINT_MODEL) else None\n",
        "            }\n",
        "    return {'exists': False}\n",
        "\n",
        "def initialize_checkpoint_csv():\n",
        "    \"\"\"Initialize the checkpoint CSV with headers.\"\"\"\n",
        "    if not os.path.exists(CHECKPOINT_CSV):\n",
        "        # Create empty CSV with expected columns\n",
        "        columns = ['epoch', 'train/cls_loss', 'val/cls_loss', 'train/seg_loss', 'val/seg_loss',\n",
        "                   'metrics/precision(M)', 'metrics/recall(M)', 'metrics/mAP50(M)',\n",
        "                   'metrics/mAP50-95(M)', 'lr/pg0', 'timestamp']\n",
        "        pd.DataFrame(columns=columns).to_csv(CHECKPOINT_CSV, index=False)\n",
        "        print(f'Created new checkpoint CSV: {CHECKPOINT_CSV}')\n",
        "        return None\n",
        "    else:\n",
        "        print(f'‚úì Found existing checkpoint CSV: {CHECKPOINT_CSV}')\n",
        "        return load_checkpoint_info()\n",
        "\n",
        "def save_checkpoint(epoch, metrics, model_path):\n",
        "    \"\"\"Save checkpoint after each epoch.\"\"\"\n",
        "    import datetime\n",
        "\n",
        "    # Read existing progress\n",
        "    df = pd.read_csv(CHECKPOINT_CSV) if os.path.exists(CHECKPOINT_CSV) else pd.DataFrame()\n",
        "\n",
        "    # Add new row\n",
        "    new_row = {\n",
        "        'epoch': epoch,\n",
        "        'timestamp': datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        **metrics\n",
        "    }\n",
        "\n",
        "    df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
        "    df.to_csv(CHECKPOINT_CSV, index=False)\n",
        "\n",
        "    # Copy latest model to checkpoint location on Google Drive\n",
        "    if os.path.exists(model_path):\n",
        "        import shutil\n",
        "        shutil.copy2(model_path, CHECKPOINT_MODEL)\n",
        "        print(f'Checkpoint saved to Drive (Epoch {epoch})')\n",
        "        print(f'  Model: {CHECKPOINT_MODEL}')\n",
        "        print(f'  Progress: {CHECKPOINT_CSV}')\n",
        "    else:\n",
        "        print(f'Warning: Model file not found at {model_path}')\n",
        "\n",
        "# Check for existing checkpoint\n",
        "checkpoint_info = initialize_checkpoint_csv()\n",
        "\n",
        "if checkpoint_info and checkpoint_info['exists']:\n",
        "    print(f'\\n=== RESUMABLE TRAINING DETECTED ===')\n",
        "    print(f'Previous training found:')\n",
        "    print(f'  Last completed epoch: {checkpoint_info[\"last_epoch\"]}')\n",
        "    print(f'  Total epochs trained: {checkpoint_info[\"total_epochs_trained\"]}')\n",
        "    print(f'  Best classification loss: {checkpoint_info[\"best_cls_loss\"]:.4f}')\n",
        "    print(f'  Best segmentation mAP: {checkpoint_info[\"best_map\"]:.4f}')\n",
        "\n",
        "    if checkpoint_info['checkpoint_path']:\n",
        "        print(f'  Checkpoint model: {checkpoint_info[\"checkpoint_path\"]}')\n",
        "        RESUME_FROM = checkpoint_info['checkpoint_path']\n",
        "        RESUME_TRAINING = True\n",
        "    else:\n",
        "        print(f'  Checkpoint model not found, will start fresh')\n",
        "        RESUME_FROM = PRETRAINED_MODEL\n",
        "        RESUME_TRAINING = False\n",
        "else:\n",
        "    print(f'\\n=== STARTING FRESH TRAINING ===')\n",
        "    print(f'No previous checkpoint found')\n",
        "    RESUME_FROM = PRETRAINED_MODEL\n",
        "    RESUME_TRAINING = False\n",
        "\n",
        "print(f'\\nCheckpoint directory: {CHECKPOINT_DIR}')\n",
        "print(f'Progress CSV: {CHECKPOINT_CSV}')\n",
        "print(f'\\nüí° Performance Note:')\n",
        "print(f'   CSV is saved every epoch (fast)')\n",
        "print(f'   Model is saved to Drive every 10 epochs (to avoid slow Drive I/O)')\n",
        "print(f'   Local model checkpoints are saved every 5 epochs by YOLO')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5Ztex7_8tWE",
        "outputId": "32e73fb6-5a49-4bac-85e5-51dcd7e5e319"
      },
      "id": "e5Ztex7_8tWE",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Found existing checkpoint CSV: /content/drive/MyDrive/csc173_dataset/refined_checkpoints/training_progress.csv\n",
            "\n",
            "=== STARTING FRESH TRAINING ===\n",
            "No previous checkpoint found\n",
            "\n",
            "Checkpoint directory: /content/drive/MyDrive/csc173_dataset/refined_checkpoints\n",
            "Progress CSV: /content/drive/MyDrive/csc173_dataset/refined_checkpoints/training_progress.csv\n",
            "\n",
            "üí° Performance Note:\n",
            "   CSV is saved every epoch (fast)\n",
            "   Model is saved to Drive every 10 epochs (to avoid slow Drive I/O)\n",
            "   Local model checkpoints are saved every 5 epochs by YOLO\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Configuration"
      ],
      "metadata": {
        "id": "7mcPJmAx8wbm"
      },
      "id": "7mcPJmAx8wbm"
    },
    {
      "cell_type": "code",
      "source": [
        "# Refined training hyperparameters\n",
        "REFINE_EPOCHS = 40\n",
        "BATCH_SIZE = 16\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# Cyclic learning rate for Phase 1 (escaping plateau)\n",
        "# Start higher to shake the model out of local minimum\n",
        "LR0 = 0.005  # Higher than previous 0.001\n",
        "LRF = 0.0001  # End lower for fine control\n",
        "\n",
        "# Optimizer settings\n",
        "MOMENTUM = 0.937\n",
        "WEIGHT_DECAY = 5e-4\n",
        "WARMUP_EPOCHS = 3.0\n",
        "\n",
        "# Augmentations - more aggressive for character robustness\n",
        "AUG_HSV_H = 0.02  # Increased hue variation\n",
        "AUG_HSV_S = 0.8   # Increased saturation variation\n",
        "AUG_HSV_V = 0.5   # Increased brightness variation\n",
        "AUG_ERASING = 0.5  # Increased random erasing\n",
        "AUG_DEGREES = 5.0  # Small rotation for character variation\n",
        "AUG_SHEAR = 2.0    # Perspective variation\n",
        "\n",
        "# Disabled augmentations (not useful for OCR)\n",
        "AUG_FLIPLR = 0.0\n",
        "AUG_MOSAIC = 0.0\n",
        "AUG_MIXUP = 0.0\n",
        "\n",
        "print('Refined Training Configuration:')\n",
        "print(f'  Epochs: {REFINE_EPOCHS}')\n",
        "print(f'  Batch size: {BATCH_SIZE}')\n",
        "print(f'  Learning rate: {LR0} ‚Üí {LRF}')\n",
        "print(f'  Augmentations: Enhanced HSV + Erasing + Geometric')\n",
        "print(f'\\nTraining Strategy:')\n",
        "print(f'  Phase 1 (1-12): Classifier head only')\n",
        "print(f'  Phase 2 (13-24): + Segmentation head')\n",
        "print(f'  Phase 3 (25-40): All layers')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8zLqe9V8vVZ",
        "outputId": "b1cffcaa-0403-47c0-bb15-cd21f44f9c2f"
      },
      "id": "a8zLqe9V8vVZ",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Refined Training Configuration:\n",
            "  Epochs: 40\n",
            "  Batch size: 16\n",
            "  Learning rate: 0.005 ‚Üí 0.0001\n",
            "  Augmentations: Enhanced HSV + Erasing + Geometric\n",
            "\n",
            "Training Strategy:\n",
            "  Phase 1 (1-12): Classifier head only\n",
            "  Phase 2 (13-24): + Segmentation head\n",
            "  Phase 3 (25-40): All layers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b00e62a",
      "metadata": {
        "id": "0b00e62a"
      },
      "source": [
        "## Load Pretrained Model and Configure Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b4b8d469",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4b8d469",
        "outputId": "7b569915-53f4-41fd-dcda-f08e0c02331c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /content/drive/MyDrive/csc173_dataset/models/custom_ocr_last.pt\n",
            "\n",
            "‚úì Model loaded for FRESH training\n",
            "  Starting from pretrained checkpoint\n",
            "  Total target epochs: 40\n"
          ]
        }
      ],
      "source": [
        "# Load model (either from checkpoint or pretrained)\n",
        "\n",
        "# Check if checkpoint management cell was run\n",
        "if 'RESUME_FROM' not in locals() and 'RESUME_FROM' not in globals():\n",
        "    print('    ERROR: Please run the \"Checkpoint Management\" cell first!')\n",
        "    print('   (The cell that defines RESUME_FROM and checkpoint tracking)')\n",
        "    raise RuntimeError('Checkpoint management cell must be run before loading model')\n",
        "\n",
        "print(f'Loading model from: {RESUME_FROM}\\n')\n",
        "\n",
        "model = YOLO(RESUME_FROM)\n",
        "model.trainer = RefinedSegmentationTrainer\n",
        "\n",
        "if RESUME_TRAINING:\n",
        "    print(f'‚úì Model loaded for RESUMING training')\n",
        "    print(f'  Will continue from epoch {checkpoint_info[\"last_epoch\"] + 1}')\n",
        "else:\n",
        "    print(f'‚úì Model loaded for FRESH training')\n",
        "    print(f'  Starting from pretrained checkpoint')\n",
        "\n",
        "print(f'  Total target epochs: {REFINE_EPOCHS}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77ae0242",
      "metadata": {
        "id": "77ae0242"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "7eeb804e",
      "metadata": {
        "id": "7eeb804e"
      },
      "source": [
        "## Execute Refined Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "8cc4bac1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8cc4bac1",
        "outputId": "9c3e5566-868f-4927-8282-b39b396f2d6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to correct data.yaml at: /content/local_dataset/data.yaml\n",
            "data.yaml successfully corrected and saved to /content/local_dataset/data.yaml\n",
            "\n",
            "New data.yaml content:\n",
            "path: /content/local_dataset\n",
            "train: train/images\n",
            "val: val/images\n",
            "nc: 36\n",
            "names:\n",
            "- A\n",
            "- B\n",
            "- C\n",
            "- D\n",
            "- E\n",
            "- F\n",
            "- G\n",
            "- H\n",
            "- I\n",
            "- J\n",
            "- K\n",
            "- L\n",
            "- M\n",
            "- N\n",
            "- O\n",
            "- P\n",
            "- Q\n",
            "- R\n",
            "- S\n",
            "- T\n",
            "- U\n",
            "- V\n",
            "- W\n",
            "- X\n",
            "- Y\n",
            "- Z\n",
            "- '0'\n",
            "- '1'\n",
            "- '2'\n",
            "- '3'\n",
            "- '4'\n",
            "- '5'\n",
            "- '6'\n",
            "- '7'\n",
            "- '8'\n",
            "- '9'\n",
            "test: test/images\n",
            "\n",
            "\n",
            "================================================================================\n",
            "STARTING FRESH REFINED TRAINING\n",
            "================================================================================\n",
            "\n",
            "Start time: 2025-12-22 02:50:57\n",
            "Device: cpu\n",
            "Run name: refine_20251222_025057\n",
            "Checkpoint tracking: /content/drive/MyDrive/csc173_dataset/refined_checkpoints/training_progress.csv\n",
            "\n",
            "Ultralytics 8.3.240 üöÄ Python-3.12.12 torch-2.9.0+cpu CPU (Intel Xeon CPU @ 2.20GHz)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/local_dataset/data.yaml, degrees=5.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=40, erasing=0.5, exist_ok=True, fliplr=0.0, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.02, hsv_s=0.8, hsv_v=0.5, imgsz=224, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.005, lrf=0.0001, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=/content/drive/MyDrive/csc173_dataset/models/custom_ocr_last.pt, momentum=0.937, mosaic=0.0, multi_scale=False, name=refine_20251222_025057, nbs=64, nms=False, opset=None, optimize=False, optimizer=SGD, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=refined_training, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/refined_training/refined_training/refine_20251222_025057, save_frames=False, save_json=False, save_period=5, save_txt=False, scale=0.5, seed=42, shear=2.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=segment, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 755.1KB 85.2MB/s 0.0s\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
            "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
            " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
            " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
            " 23        [16, 19, 22]  1    690460  ultralytics.nn.modules.head.Segment          [36, 32, 64, [64, 128, 256]]  \n",
            "YOLO11n-seg summary: 203 layers, 2,849,628 parameters, 2,849,612 gradients, 9.8 GFLOPs\n",
            "\n",
            "Transferred 561/561 items from pretrained weights\n",
            "Freezing layer 'model.23.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 101.6¬±186.9 MB/s, size: 125.6 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/local_dataset/train/labels... 4394 images, 0 backgrounds, 0 corrupt: 28% ‚îÅ‚îÅ‚îÅ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 4394/15960 186.4it/s 11.0s<1:02\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/data/dataset.py\u001b[0m in \u001b[0;36mget_labels\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"version\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mDATASET_CACHE_VERSION\u001b[0m  \u001b[0;31m# matches current version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mget_hash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_files\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim_files\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# identical hash\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.12/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                 \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-805459478.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;31m# Execute training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\n{\"=\"*80}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    771\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m         \u001b[0;31m# Update model and cfg after training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mRANK\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36m_do_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_ddp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# number of batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36m_setup_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;31m# Dataloaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         self.train_loader = self.get_dataloader(\n\u001b[0m\u001b[1;32m    324\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLOCAL_RANK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/models/yolo/detect/train.py\u001b[0m in \u001b[0;36mget_dataloader\u001b[0;34m(self, dataset_path, batch_size, rank, mode)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"val\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Mode must be 'train' or 'val', not {mode}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch_distributed_zero_first\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# init dataset *.cache only once if DDP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rect\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/models/yolo/detect/train.py\u001b[0m in \u001b[0;36mbuild_dataset\u001b[0;34m(self, img_path, mode, batch)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \"\"\"\n\u001b[1;32m     76\u001b[0m         \u001b[0mgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munwrap_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbuild_yolo_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"val\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/data/build.py\u001b[0m in \u001b[0;36mbuild_yolo_dataset\u001b[0;34m(cfg, img_path, batch, data, mode, rect, stride, multi_modal)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;34m\"\"\"Build and return a YOLO dataset based on configuration parameters.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYOLOMultiModalDataset\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmulti_modal\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mYOLODataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m     return dataset(\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0mimg_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mimgsz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgsz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/data/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, task, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_segments\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_keypoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Can not use both segments and keypoints.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"channels\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcache_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./labels.cache\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/data/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, img_path, imgsz, cache, augment, hyp, prefix, rect, batch_size, stride, pad, single_cls, classes, fraction, channels)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv2_flag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMREAD_GRAYSCALE\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mchannels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMREAD_COLOR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_img_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# single_cls and include_class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mni\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# number of images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/data/dataset.py\u001b[0m in \u001b[0;36mget_labels\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mget_hash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_files\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim_files\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# identical hash\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# run cache ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;31m# Display cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/data/dataset.py\u001b[0m in \u001b[0;36mcache_labels\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    122\u001b[0m             )\n\u001b[1;32m    123\u001b[0m             \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTQDM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mim_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeypoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnm_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnf_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mne_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnc_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m                 \u001b[0mnm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnm_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[0mnf\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnf_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/utils/tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    859\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m                     \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import yaml\n",
        "import os\n",
        "from pathlib import Path\n",
        "import datetime\n",
        "\n",
        "print(f'Attempting to correct data.yaml at: {DATA_YAML_PATH}')\n",
        "\n",
        "try:\n",
        "    # Load existing data.yaml to preserve names and nc\n",
        "    with open(DATA_YAML_PATH, 'r') as f:\n",
        "        existing_data = yaml.safe_load(f)\n",
        "\n",
        "    # Define the new, corrected data.yaml content\n",
        "    corrected_data_yaml_content = {\n",
        "        'path': str(Path(LOCAL_DATASET_ROOT)), # Set the base path to the local dataset root\n",
        "        'train': 'train/images',\n",
        "        'val': 'val/images',\n",
        "        'nc': existing_data.get('nc', NUM_CLASSES),\n",
        "        'names': existing_data.get('names', CHARS)\n",
        "    }\n",
        "\n",
        "    # Add 'test' if it exists in original or is desired\n",
        "    if 'test' in existing_data:\n",
        "        corrected_data_yaml_content['test'] = 'test/images'\n",
        "\n",
        "    # Overwrite the data.yaml file with the corrected content\n",
        "    with open(DATA_YAML_PATH, 'w') as f:\n",
        "        yaml.safe_dump(corrected_data_yaml_content, f, sort_keys=False)\n",
        "\n",
        "    print(f'data.yaml successfully corrected and saved to {DATA_YAML_PATH}')\n",
        "    print('\\nNew data.yaml content:')\n",
        "    with open(DATA_YAML_PATH, 'r') as f:\n",
        "        print(f.read())\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: data.yaml not found at {DATA_YAML_PATH}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while processing data.yaml: {e}\")\n",
        "\n",
        "# Determine training name based on checkpoint status\n",
        "if RESUME_TRAINING:\n",
        "    run_name = f'refine_resumed_{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n",
        "else:\n",
        "    run_name = f'refine_{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n",
        "\n",
        "# Training parameters\n",
        "train_params = dict(\n",
        "    data=DATA_YAML_PATH,\n",
        "    epochs=REFINE_EPOCHS,\n",
        "    batch=BATCH_SIZE,\n",
        "    imgsz=IMG_SIZE,\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer='SGD',\n",
        "    lr0=LR0,\n",
        "    lrf=LRF,\n",
        "    momentum=MOMENTUM,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "\n",
        "    # Warmup\n",
        "    warmup_epochs=WARMUP_EPOCHS,\n",
        "    warmup_momentum=0.8,\n",
        "    warmup_bias_lr=0.1,\n",
        "\n",
        "    # Augmentations\n",
        "    hsv_h=AUG_HSV_H,\n",
        "    hsv_s=AUG_HSV_S,\n",
        "    hsv_v=AUG_HSV_V,\n",
        "    erasing=AUG_ERASING,\n",
        "    degrees=AUG_DEGREES,\n",
        "    shear=AUG_SHEAR,\n",
        "    fliplr=AUG_FLIPLR,\n",
        "    mosaic=AUG_MOSAIC,\n",
        "    mixup=AUG_MIXUP,\n",
        "\n",
        "    # Output settings\n",
        "    project='refined_training',\n",
        "    name=run_name,\n",
        "    exist_ok=True,\n",
        "\n",
        "    # Validation and saving\n",
        "    val=True,\n",
        "    save=True,\n",
        "    save_period=5,  # Save every 5 epochs\n",
        "\n",
        "    # System\n",
        "    device=device,\n",
        "    amp=True,  # Enable automatic mixed precision for faster training\n",
        "    seed=42,\n",
        "    deterministic=True,\n",
        "\n",
        "    # Resume handling\n",
        "    resume=RESUME_TRAINING,  # Resume if checkpoint exists\n",
        ")\n",
        "\n",
        "print(f'\\n{\"=\"*80}')\n",
        "if RESUME_TRAINING:\n",
        "    print(f'RESUMING REFINED TRAINING FROM EPOCH {checkpoint_info[\"last_epoch\"] + 1}')\n",
        "else:\n",
        "    print(f'STARTING FRESH REFINED TRAINING')\n",
        "print(f'{\"=\"*80}\\n')\n",
        "print(f'Start time: {datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
        "print(f'Device: {device}')\n",
        "print(f'Run name: {run_name}')\n",
        "print(f'Checkpoint tracking: {CHECKPOINT_CSV}\\n')\n",
        "\n",
        "# Execute training\n",
        "results = model.train(**train_params)\n",
        "\n",
        "print(f'\\n{\"=\"*80}')\n",
        "print(f'TRAINING COMPLETED')\n",
        "print(f'{\"=\"*80}\\n')\n",
        "print(f'End time: {datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
        "print(f'Results directory: {results.save_dir}')\n",
        "print(f'Checkpoint CSV: {CHECKPOINT_CSV}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9dbebfb",
      "metadata": {
        "id": "b9dbebfb"
      },
      "source": [
        "## Export Best Model to Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "507bd7ec",
      "metadata": {
        "id": "507bd7ec"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Find best model from training run\n",
        "best_model = Path(results.save_dir) / 'weights' / 'best.pt'\n",
        "last_model = Path(results.save_dir) / 'weights' / 'last.pt'\n",
        "\n",
        "# Export to Drive\n",
        "export_dir = f'{DRIVE_ROOT}/refined_models'\n",
        "os.makedirs(export_dir, exist_ok=True)\n",
        "\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "if best_model.exists():\n",
        "    export_best = f'{export_dir}/refined_best_{timestamp}.pt'\n",
        "    shutil.copy2(best_model, export_best)\n",
        "    print(f'Best model exported to: {export_best}')\n",
        "\n",
        "if last_model.exists():\n",
        "    export_last = f'{export_dir}/refined_last_{timestamp}.pt'\n",
        "    shutil.copy2(last_model, export_last)\n",
        "    print(f'Last model exported to: {export_last}')\n",
        "\n",
        "# Copy results CSV\n",
        "results_csv = Path(results.save_dir) / 'results.csv'\n",
        "if results_csv.exists():\n",
        "    export_results = f'{export_dir}/refined_results_{timestamp}.csv'\n",
        "    shutil.copy2(results_csv, export_results)\n",
        "    print(f'Results CSV exported to: {export_results}')\n",
        "\n",
        "print(f'\\nAll files exported to Google Drive')\n",
        "print(f'  Location: {export_dir}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ebcce99",
      "metadata": {
        "id": "6ebcce99"
      },
      "source": [
        "## Performance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "208e10e5",
      "metadata": {
        "id": "208e10e5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load results\n",
        "results_csv_path = Path(results.save_dir) / 'results.csv'\n",
        "\n",
        "if results_csv_path.exists():\n",
        "    df = pd.read_csv(results_csv_path)\n",
        "    df.columns = df.columns.str.strip()\n",
        "\n",
        "    print('Refined Training Results Summary')\n",
        "    print('=' * 80)\n",
        "    print(f'Total epochs: {len(df)}')\n",
        "    print()\n",
        "\n",
        "    # Final metrics\n",
        "    print('Final Epoch Metrics:')\n",
        "    print('-' * 80)\n",
        "    print(f'Classification Loss (val): {df[\"val/cls_loss\"].iloc[-1]:.4f}')\n",
        "    print(f'Segmentation mAP@50-95:    {df[\"metrics/mAP50-95(M)\"].iloc[-1]:.4f}')\n",
        "    print(f'Segmentation Precision:     {df[\"metrics/precision(M)\"].iloc[-1]:.4f}')\n",
        "    print(f'Segmentation Recall:        {df[\"metrics/recall(M)\"].iloc[-1]:.4f}')\n",
        "    print()\n",
        "\n",
        "    # Best metrics\n",
        "    best_cls_loss_idx = df['val/cls_loss'].idxmin()\n",
        "    best_map_idx = df['metrics/mAP50-95(M)'].idxmax()\n",
        "\n",
        "    print('Best Performance:')\n",
        "    print('-' * 80)\n",
        "    print(f'Best Classification Loss:   {df[\"val/cls_loss\"].iloc[best_cls_loss_idx]:.4f} (epoch {df[\"epoch\"].iloc[best_cls_loss_idx]:.0f})')\n",
        "    print(f'Best Segmentation mAP:      {df[\"metrics/mAP50-95(M)\"].iloc[best_map_idx]:.4f} (epoch {df[\"epoch\"].iloc[best_map_idx]:.0f})')\n",
        "    print()\n",
        "\n",
        "    # Improvement over baseline\n",
        "    baseline_cls_loss = 0.4321  # From epoch 68 of original training\n",
        "    baseline_map = 0.4799\n",
        "\n",
        "    final_cls_loss = df['val/cls_loss'].iloc[-1]\n",
        "    final_map = df['metrics/mAP50-95(M)'].iloc[-1]\n",
        "\n",
        "    cls_improvement = ((baseline_cls_loss - final_cls_loss) / baseline_cls_loss) * 100\n",
        "    map_improvement = ((final_map - baseline_map) / baseline_map) * 100\n",
        "\n",
        "    print('Improvement Over Baseline (Epoch 68):')\n",
        "    print('-' * 80)\n",
        "    print(f'Classification Loss: {baseline_cls_loss:.4f} ‚Üí {final_cls_loss:.4f} ({cls_improvement:+.2f}%)')\n",
        "    print(f'Segmentation mAP:    {baseline_map:.4f} ‚Üí {final_map:.4f} ({map_improvement:+.2f}%)')\n",
        "    print()\n",
        "\n",
        "    # Plot training curves\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    fig.suptitle('Refined Training Performance', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Plot 1: Classification Loss\n",
        "    ax = axes[0, 0]\n",
        "    ax.plot(df['epoch'], df['train/cls_loss'], label='Train', linewidth=2, alpha=0.7)\n",
        "    ax.plot(df['epoch'], df['val/cls_loss'], label='Validation', linewidth=2)\n",
        "    ax.axhline(y=baseline_cls_loss, color='red', linestyle='--', label=f'Baseline ({baseline_cls_loss:.4f})', alpha=0.5)\n",
        "    ax.axvline(x=30, color='gray', linestyle=':', alpha=0.5, label='Phase 2')\n",
        "    ax.axvline(x=60, color='gray', linestyle=':', alpha=0.5, label='Phase 3')\n",
        "    ax.set_xlabel('Epoch', fontweight='bold')\n",
        "    ax.set_ylabel('Loss', fontweight='bold')\n",
        "    ax.set_title('Classification Loss (Lower is Better)', fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.legend()\n",
        "\n",
        "    # Plot 2: Segmentation mAP\n",
        "    ax = axes[0, 1]\n",
        "    ax.plot(df['epoch'], df['metrics/mAP50-95(M)'], linewidth=2, color='green')\n",
        "    ax.axhline(y=baseline_map, color='red', linestyle='--', label=f'Baseline ({baseline_map:.4f})', alpha=0.5)\n",
        "    ax.axvline(x=30, color='gray', linestyle=':', alpha=0.5)\n",
        "    ax.axvline(x=60, color='gray', linestyle=':', alpha=0.5)\n",
        "    ax.set_xlabel('Epoch', fontweight='bold')\n",
        "    ax.set_ylabel('mAP@50-95', fontweight='bold')\n",
        "    ax.set_title('Segmentation Quality (Higher is Better)', fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.legend()\n",
        "    ax.set_ylim([0.4, 1.0])\n",
        "\n",
        "    # Plot 3: Precision & Recall\n",
        "    ax = axes[1, 0]\n",
        "    ax.plot(df['epoch'], df['metrics/precision(M)'], label='Precision', linewidth=2)\n",
        "    ax.plot(df['epoch'], df['metrics/recall(M)'], label='Recall', linewidth=2)\n",
        "    ax.axvline(x=30, color='gray', linestyle=':', alpha=0.5)\n",
        "    ax.axvline(x=60, color='gray', linestyle=':', alpha=0.5)\n",
        "    ax.set_xlabel('Epoch', fontweight='bold')\n",
        "    ax.set_ylabel('Score', fontweight='bold')\n",
        "    ax.set_title('Precision & Recall', fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.legend()\n",
        "    ax.set_ylim([0.7, 1.0])\n",
        "\n",
        "    # Plot 4: Learning Rate\n",
        "    ax = axes[1, 1]\n",
        "    ax.plot(df['epoch'], df['lr/pg0'], linewidth=2, color='purple')\n",
        "    ax.axvline(x=30, color='gray', linestyle=':', alpha=0.5, label='Phase transitions')\n",
        "    ax.axvline(x=60, color='gray', linestyle=':', alpha=0.5)\n",
        "    ax.set_xlabel('Epoch', fontweight='bold')\n",
        "    ax.set_ylabel('Learning Rate', fontweight='bold')\n",
        "    ax.set_title('Learning Rate Schedule', fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.legend()\n",
        "    ax.set_yscale('log')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save plot\n",
        "    plot_path = Path(results.save_dir) / 'refined_training_analysis.png'\n",
        "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
        "    print(f'Training curves saved to: {plot_path}')\n",
        "\n",
        "    # Also save to Drive\n",
        "    drive_plot_path = f'{export_dir}/refined_training_analysis_{timestamp}.png'\n",
        "    shutil.copy2(plot_path, drive_plot_path)\n",
        "    print(f'Plots exported to Drive: {drive_plot_path}')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print('Results CSV not found. Training may not have completed.')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}