{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a7e14ac",
   "metadata": {},
   "source": [
    "## Setup: Google Colab Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02620f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Verify CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print(f'GPU available: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('No GPU available, using CPU (training will be slow)')\n",
    "\n",
    "print(f'\\nPyTorch version: {torch.__version__}')\n",
    "print(f'CUDA version: {torch.version.cuda}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b038eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q ultralytics opencv-python-headless pillow pyyaml numpy scipy matplotlib pandas\n",
    "\n",
    "print('All packages installed successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b48dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (if using dataset from Drive)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set paths - MODIFY THESE TO YOUR ACTUAL PATHS\n",
    "DRIVE_ROOT = '/content/drive/MyDrive/csc173_dataset'\n",
    "DATA_YAML_PATH = f'{DRIVE_ROOT}/dataset/data.yaml'\n",
    "PRETRAINED_MODEL = f'{DRIVE_ROOT}/models/custom_ocr_last.pt'  # Your existing checkpoint\n",
    "\n",
    "# Create local working directory\n",
    "WORK_DIR = '/content/refined_training'\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "os.chdir(WORK_DIR)\n",
    "\n",
    "print(f'Drive mounted and working directory set')\n",
    "print(f'Data config: {DATA_YAML_PATH}')\n",
    "print(f'Pretrained model: {PRETRAINED_MODEL}')\n",
    "print(f'Working directory: {WORK_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb3737f",
   "metadata": {},
   "source": [
    "## Core Components (Reused from Original Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7000dfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character set and similarity matrix (from original)\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "CHARS = [chr(i) for i in range(65, 91)] + [str(i) for i in range(10)]\n",
    "NUM_CLASSES = len(CHARS)\n",
    "CHAR_TO_IDX = {c: i for i, c in enumerate(CHARS)}\n",
    "IDX_TO_CHAR = {i: c for i, c in enumerate(CHARS)}\n",
    "\n",
    "print(f'Number of classes: {NUM_CLASSES}')\n",
    "print(f'Characters: {\"\".join(CHARS)}')\n",
    "\n",
    "SIMILAR_GROUPS = [\n",
    "    ['O', '0'], # I want to refine Q and 0/O differentiation, so it was removed from the group\n",
    "    ['I', '1'], # I want to refine L and 1/I differentiation, so it was removed from the group\n",
    "    ['S', '5'],\n",
    "    ['Z', '2'],\n",
    "    ['B', '8'],\n",
    "    ['D', '0'],\n",
    "    ['G', 'C'],\n",
    "    ['U', 'V'],\n",
    "    ['P', 'R'],\n",
    "]\n",
    "\n",
    "def create_similarity_matrix(num_classes=NUM_CLASSES, groups=SIMILAR_GROUPS, base_sim=0.6):\n",
    "    S = np.zeros((num_classes, num_classes), dtype=np.float32)\n",
    "    np.fill_diagonal(S, 1.0)\n",
    "    for group in groups:\n",
    "        idxs = [CHAR_TO_IDX[c] for c in group if c in CHAR_TO_IDX]\n",
    "        for i in idxs:\n",
    "            for j in idxs:\n",
    "                if i != j:\n",
    "                    S[i, j] = base_sim\n",
    "    return torch.tensor(S, dtype=torch.float32)\n",
    "\n",
    "similarity_matrix = create_similarity_matrix()\n",
    "print(f'Similarity matrix initialized: {similarity_matrix.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebec263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Similarity-Aware Loss with Adaptive Weighting\n",
    "class RefinedSimilarityAwareTopKLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Enhanced loss for fine-tuning with:\n",
    "    - Higher penalty for similar character confusion\n",
    "    - Adaptive temperature based on training phase\n",
    "    - Confidence-based weighting\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=NUM_CLASSES, similarity_matrix=None,\n",
    "                 k=3, initial_temperature=0.5, base_weight=0.5, topk_weight=0.5,\n",
    "                 epochs=40):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.k = k\n",
    "        self.initial_temperature = initial_temperature\n",
    "        self.base_weight = base_weight\n",
    "        self.topk_weight = topk_weight\n",
    "        self.epochs = epochs\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "        if similarity_matrix is not None:\n",
    "            self.register_buffer('similarity_matrix', similarity_matrix)\n",
    "        else:\n",
    "            self.register_buffer('similarity_matrix', create_similarity_matrix())\n",
    "\n",
    "    def update_epoch(self, epoch):\n",
    "        \"\"\"Update current epoch for temperature annealing.\"\"\"\n",
    "        self.current_epoch = epoch\n",
    "    \n",
    "    def get_temperature(self):\n",
    "        \"\"\"Anneal temperature more aggressively for fine-tuning.\"\"\"\n",
    "        progress = self.current_epoch / max(self.epochs, 1)\n",
    "        # Start at 0.5, go to 0.3 (sharper predictions)\n",
    "        return max(0.3, self.initial_temperature - progress * 0.2)\n",
    "    \n",
    "    def forward(self, logits, targets):\n",
    "        B = logits.size(0)\n",
    "        device = logits.device\n",
    "        \n",
    "        temperature = self.get_temperature()\n",
    "        \n",
    "        # Standard cross-entropy\n",
    "        ce_loss = F.cross_entropy(logits, targets, reduction='none')\n",
    "        \n",
    "        # Softmax with temperature\n",
    "        probs = F.softmax(logits / temperature, dim=1)\n",
    "        topk_probs, topk_indices = torch.topk(probs, min(self.k, self.num_classes), dim=1)\n",
    "        \n",
    "        # Similarity-aware penalty\n",
    "        sim_loss = torch.zeros(B, device=device)\n",
    "        confidence_scores = []\n",
    "        \n",
    "        for i in range(B):\n",
    "            t = targets[i].item()\n",
    "            if t < 0 or t >= self.num_classes:\n",
    "                continue\n",
    "                \n",
    "            sims = self.similarity_matrix[t][topk_indices[i]]\n",
    "            \n",
    "            # Higher penalty for similar character confusion\n",
    "            # If model confuses O with 0 (high similarity), penalty is lower\n",
    "            # If model confuses O with X (low similarity), penalty is higher\n",
    "            penalties = (1.0 - sims) * 1.5  # Amplify penalty\n",
    "            weighted_penalties = topk_probs[i] * penalties\n",
    "            sim_loss[i] = weighted_penalties.sum()\n",
    "            \n",
    "            confidence_scores.append(topk_probs[i][0].item())\n",
    "        \n",
    "        if len(confidence_scores) == 0:\n",
    "            return ce_loss.mean()\n",
    "        \n",
    "        # Adaptive weighting based on confidence\n",
    "        confidence = torch.tensor(confidence_scores, device=device)\n",
    "        \n",
    "        # When confident: rely more on CE (trust the model)\n",
    "        # When uncertain: rely more on similarity (guide the model)\n",
    "        adaptive_base = self.base_weight + (1 - confidence) * 0.2\n",
    "        adaptive_topk = self.topk_weight + confidence * 0.2\n",
    "        \n",
    "        # Normalize\n",
    "        total_weight = adaptive_base + adaptive_topk\n",
    "        adaptive_base = adaptive_base / total_weight\n",
    "        adaptive_topk = adaptive_topk / total_weight\n",
    "        \n",
    "        total_loss = adaptive_base * ce_loss + adaptive_topk * sim_loss\n",
    "        return total_loss.mean()\n",
    "\n",
    "print('Refined similarity-aware loss defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4557780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OCR Metrics (reused from original)\n",
    "class OCRMetrics:\n",
    "    \"\"\"Compute OCR-specific validation metrics.\"\"\"\n",
    "    def __init__(self, similarity_matrix=None):\n",
    "        self.similarity_matrix = similarity_matrix if similarity_matrix is not None else create_similarity_matrix()\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.total_chars = 0\n",
    "        self.correct_chars = 0\n",
    "        self.top2_correct = 0\n",
    "        self.top3_correct = 0\n",
    "        self.similarity_score = 0.0\n",
    "    \n",
    "    def update(self, predictions, targets, top_k_preds=None):\n",
    "        predictions = predictions.cpu().numpy()\n",
    "        targets = targets.cpu().numpy()\n",
    "        \n",
    "        self.total_chars += len(targets)\n",
    "        self.correct_chars += (predictions == targets).sum()\n",
    "        \n",
    "        # Similarity-aware accuracy\n",
    "        for pred, target in zip(predictions, targets):\n",
    "            if 0 <= target < len(self.similarity_matrix) and 0 <= pred < len(self.similarity_matrix):\n",
    "                sim = self.similarity_matrix[target][pred].item()\n",
    "                self.similarity_score += sim\n",
    "        \n",
    "        # Top-k accuracy\n",
    "        if top_k_preds is not None:\n",
    "            top_k_preds = top_k_preds.cpu().numpy()\n",
    "            for i, target in enumerate(targets):\n",
    "                if top_k_preds.shape[1] >= 2 and target in top_k_preds[i, :2]:\n",
    "                    self.top2_correct += 1\n",
    "                if top_k_preds.shape[1] >= 3 and target in top_k_preds[i, :3]:\n",
    "                    self.top3_correct += 1\n",
    "    \n",
    "    def compute(self):\n",
    "        if self.total_chars == 0:\n",
    "            return {}\n",
    "        \n",
    "        return {\n",
    "            'CER': 1.0 - (self.correct_chars / self.total_chars),\n",
    "            'char_accuracy': self.correct_chars / self.total_chars,\n",
    "            'top2_accuracy': self.top2_correct / self.total_chars,\n",
    "            'top3_accuracy': self.top3_correct / self.total_chars,\n",
    "            'similarity_aware_accuracy': self.similarity_score / self.total_chars,\n",
    "        }\n",
    "\n",
    "print('OCR metrics module loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e25dad",
   "metadata": {},
   "source": [
    "## Refined Training Strategy\n",
    "\n",
    "### Phase 1: Classifier Head Fine-Tuning (Epochs 1-12)\n",
    "- Freeze backbone and segmentation head\n",
    "- Focus exclusively on improving character classification\n",
    "- Use cyclic learning rate to escape plateau\n",
    "\n",
    "### Phase 2: Progressive Unfreezing (Epochs 13-24)\n",
    "- Gradually unfreeze deeper layers\n",
    "- Lower learning rate for stable refinement\n",
    "- Continue with similarity-aware loss\n",
    "\n",
    "### Phase 3: Full Fine-Tuning (Epochs 25-40)\n",
    "- All layers unfrozen\n",
    "- Very low learning rate for final polish\n",
    "- Focus on reducing classification loss below 0.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4287274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Trainer for Refined Training\n",
    "from ultralytics.models.yolo.segment import SegmentationTrainer\n",
    "from ultralytics import YOLO\n",
    "\n",
    "class RefinedSegmentationTrainer(SegmentationTrainer):\n",
    "    \"\"\"\n",
    "    Refined trainer with:\n",
    "    - Progressive layer unfreezing\n",
    "    - Enhanced loss function\n",
    "    - OCR-specific metrics tracking\n",
    "    - Cyclic learning rate support\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg=None, overrides=None, _callbacks=None):\n",
    "        super().__init__(cfg, overrides, _callbacks)\n",
    "        \n",
    "        # Get total epochs from config\n",
    "        total_epochs = self.args.epochs if hasattr(self.args, 'epochs') else 40\n",
    "        \n",
    "        # Initialize refined loss\n",
    "        self.character_loss_fn = RefinedSimilarityAwareTopKLoss(\n",
    "            num_classes=NUM_CLASSES,\n",
    "            similarity_matrix=similarity_matrix,\n",
    "            k=3,\n",
    "            initial_temperature=0.5,\n",
    "            base_weight=0.5,\n",
    "            topk_weight=0.5,\n",
    "            epochs=total_epochs\n",
    "        ).to(device)\n",
    "        \n",
    "        # OCR metrics\n",
    "        self.ocr_metrics = OCRMetrics(similarity_matrix=similarity_matrix)\n",
    "        \n",
    "        # Training phase tracking\n",
    "        self.phase = 1\n",
    "        self.freeze_applied = False\n",
    "    \n",
    "    def _setup_train(self, world_size):\n",
    "        \"\"\"Override to apply layer freezing for Phase 1.\"\"\"\n",
    "        super()._setup_train(world_size)\n",
    "        \n",
    "        if not self.freeze_applied and self.epoch < 12:\n",
    "            print(f'\\n=== PHASE 1: Classifier Head Fine-Tuning (Epochs 1-12) ===')\n",
    "            print('Freezing backbone and segmentation layers...')\n",
    "            \n",
    "            # Freeze all layers except classification head\n",
    "            for name, param in self.model.named_parameters():\n",
    "                # Keep classification layers trainable\n",
    "                if 'cls' in name.lower() or 'cv3' in name.lower():\n",
    "                    param.requires_grad = True\n",
    "                else:\n",
    "                    param.requires_grad = False\n",
    "            \n",
    "            trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "            total = sum(p.numel() for p in self.model.parameters())\n",
    "            print(f'Trainable parameters: {trainable:,} / {total:,} ({100*trainable/total:.1f}%)')\n",
    "            self.freeze_applied = True\n",
    "    \n",
    "    def on_train_epoch_start(self):\n",
    "        \"\"\"Handle phase transitions and progressive unfreezing.\"\"\"\n",
    "        super().on_train_epoch_start()\n",
    "        \n",
    "        # Update temperature in loss\n",
    "        self.character_loss_fn.update_epoch(self.epoch)\n",
    "        \n",
    "        # Phase 2: Progressive unfreezing (epochs 12-24)\n",
    "        if self.epoch == 12:\n",
    "            self.phase = 2\n",
    "            print(f'\\n=== PHASE 2: Progressive Unfreezing (Epochs 13-24) ===')\n",
    "            print('Unfreezing segmentation head...')\n",
    "            \n",
    "            for name, param in self.model.named_parameters():\n",
    "                if 'seg' in name.lower() or 'mask' in name.lower():\n",
    "                    param.requires_grad = True\n",
    "            \n",
    "            trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "            total = sum(p.numel() for p in self.model.parameters())\n",
    "            print(f'Trainable parameters: {trainable:,} / {total:,} ({100*trainable/total:.1f}%)')\n",
    "        \n",
    "        # Phase 3: Full fine-tuning (epochs 24+)\n",
    "        elif self.epoch == 24:\n",
    "            self.phase = 3\n",
    "            print(f'\\n=== PHASE 3: Full Fine-Tuning (Epochs 25-40) ===')\n",
    "            print('Unfreezing all layers...')\n",
    "            \n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = True\n",
    "            \n",
    "            trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "            total = sum(p.numel() for p in self.model.parameters())\n",
    "            print(f'Trainable parameters: {trainable:,} / {total:,} ({100*trainable/total:.1f}%)')\n",
    "    \n",
    "    def on_val_start(self):\n",
    "        super().on_val_start()\n",
    "        self.ocr_metrics.reset()\n",
    "    \n",
    "    def on_val_end(self):\n",
    "        super().on_val_end()\n",
    "        \n",
    "        # Log OCR metrics\n",
    "        ocr_results = self.ocr_metrics.compute()\n",
    "        if ocr_results:\n",
    "            print(f'\\n[Epoch {self.epoch}] OCR Metrics:')\n",
    "            for key, value in ocr_results.items():\n",
    "                print(f'  {key}: {value:.4f}')\n",
    "    \n",
    "    def compute_loss(self, preds, batch):\n",
    "        \"\"\"Compute loss with refined similarity-aware classification.\"\"\"\n",
    "        # Get base YOLO losses\n",
    "        base_loss = super().compute_loss(preds, batch)\n",
    "        \n",
    "        # Add custom similarity-aware character classification loss\n",
    "        if len(preds) > 3:\n",
    "            cls_logits = preds[3]\n",
    "            cls_targets = batch['cls'].long()\n",
    "            \n",
    "            if cls_logits is not None and cls_targets is not None:\n",
    "                cls_logits_flat = cls_logits.view(-1, NUM_CLASSES)\n",
    "                cls_targets_flat = cls_targets.view(-1)\n",
    "                \n",
    "                valid_mask = cls_targets_flat >= 0\n",
    "                if valid_mask.sum() > 0:\n",
    "                    # Compute refined similarity-aware loss\n",
    "                    char_loss = self.character_loss_fn(\n",
    "                        cls_logits_flat[valid_mask],\n",
    "                        cls_targets_flat[valid_mask]\n",
    "                    )\n",
    "                    \n",
    "                    # Update OCR metrics\n",
    "                    with torch.no_grad():\n",
    "                        preds_cls = cls_logits_flat[valid_mask].argmax(dim=1)\n",
    "                        top_k_preds = torch.topk(cls_logits_flat[valid_mask], k=3, dim=1)[1]\n",
    "                        self.ocr_metrics.update(\n",
    "                            preds_cls,\n",
    "                            cls_targets_flat[valid_mask],\n",
    "                            top_k_preds\n",
    "                        )\n",
    "                    \n",
    "                    # Phase-dependent weighting\n",
    "                    if self.phase == 1:\n",
    "                        # Phase 1: Heavy emphasis on classification\n",
    "                        cls_weight = 0.7\n",
    "                    elif self.phase == 2:\n",
    "                        # Phase 2: Balanced\n",
    "                        cls_weight = 0.5\n",
    "                    else:\n",
    "                        # Phase 3: Standard weighting\n",
    "                        cls_weight = 0.3\n",
    "                    \n",
    "                    total_loss = (1 - cls_weight) * base_loss + cls_weight * char_loss\n",
    "                    return total_loss\n",
    "        \n",
    "        return base_loss\n",
    "\n",
    "print('Refined segmentation trainer defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b00e62a",
   "metadata": {},
   "source": [
    "## Load Pretrained Model and Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b8d469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your existing trained model\n",
    "print(f'Loading pretrained model from: {PRETRAINED_MODEL}\\n')\n",
    "\n",
    "model = YOLO(PRETRAINED_MODEL)\n",
    "model.trainer = RefinedSegmentationTrainer\n",
    "\n",
    "print('Model loaded with refined trainer attached')\n",
    "print(f'  Starting from epoch 68 checkpoint')\n",
    "print(f'  Will train for 100 additional epochs with progressive refinement')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ae0242",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bada4767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refined training hyperparameters\n",
    "REFINE_EPOCHS = 40\n",
    "BATCH_SIZE = 16\n",
    "IMG_SIZE = 224\n",
    "\n",
    "# Cyclic learning rate for Phase 1 (escaping plateau)\n",
    "# Start higher to shake the model out of local minimum\n",
    "LR0 = 0.005  # Higher than previous 0.001\n",
    "LRF = 0.0001  # End lower for fine control\n",
    "\n",
    "# Optimizer settings\n",
    "MOMENTUM = 0.937\n",
    "WEIGHT_DECAY = 5e-4\n",
    "WARMUP_EPOCHS = 3.0\n",
    "\n",
    "# Augmentations - more aggressive for character robustness\n",
    "AUG_HSV_H = 0.02  # Increased hue variation\n",
    "AUG_HSV_S = 0.8   # Increased saturation variation\n",
    "AUG_HSV_V = 0.5   # Increased brightness variation\n",
    "AUG_ERASING = 0.5  # Increased random erasing\n",
    "AUG_DEGREES = 5.0  # Small rotation for character variation\n",
    "AUG_SHEAR = 2.0    # Perspective variation\n",
    "\n",
    "# Disabled augmentations (not useful for OCR)\n",
    "AUG_FLIPLR = 0.0\n",
    "AUG_MOSAIC = 0.0\n",
    "AUG_MIXUP = 0.0\n",
    "\n",
    "print('Refined Training Configuration:')\n",
    "print(f'  Epochs: {REFINE_EPOCHS}')\n",
    "print(f'  Batch size: {BATCH_SIZE}')\n",
    "print(f'  Learning rate: {LR0} → {LRF}')\n",
    "print(f'  Augmentations: Enhanced HSV + Erasing + Geometric')\n",
    "print(f'\\nTraining Strategy:')\n",
    "print(f'  Phase 1 (1-12): Classifier head only')\n",
    "print(f'  Phase 2 (13-24): + Segmentation head')\n",
    "print(f'  Phase 3 (25-40): All layers')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeb804e",
   "metadata": {},
   "source": [
    "## Execute Refined Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc4bac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Training parameters\n",
    "train_params = dict(\n",
    "    data=DATA_YAML_PATH,\n",
    "    epochs=REFINE_EPOCHS,\n",
    "    batch=BATCH_SIZE,\n",
    "    imgsz=IMG_SIZE,\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer='SGD',\n",
    "    lr0=LR0,\n",
    "    lrf=LRF,\n",
    "    momentum=MOMENTUM,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    \n",
    "    # Warmup\n",
    "    warmup_epochs=WARMUP_EPOCHS,\n",
    "    warmup_momentum=0.8,\n",
    "    warmup_bias_lr=0.1,\n",
    "    \n",
    "    # Augmentations\n",
    "    hsv_h=AUG_HSV_H,\n",
    "    hsv_s=AUG_HSV_S,\n",
    "    hsv_v=AUG_HSV_V,\n",
    "    erasing=AUG_ERASING,\n",
    "    degrees=AUG_DEGREES,\n",
    "    shear=AUG_SHEAR,\n",
    "    fliplr=AUG_FLIPLR,\n",
    "    mosaic=AUG_MOSAIC,\n",
    "    mixup=AUG_MIXUP,\n",
    "    \n",
    "    # Output settings\n",
    "    project='refined_training',\n",
    "    name=f'refine_{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    exist_ok=True,\n",
    "    \n",
    "    # Validation and saving\n",
    "    val=True,\n",
    "    save=True,\n",
    "    save_period=10,\n",
    "    \n",
    "    # System\n",
    "    device=device,\n",
    "    amp=True,  # Enable automatic mixed precision for faster training\n",
    "    seed=42,\n",
    "    deterministic=True,\n",
    "    \n",
    "    # Resume from pretrained\n",
    "    resume=False,  # Don't resume - we're fine-tuning\n",
    ")\n",
    "\n",
    "print(f'\\n{\"=\"*80}')\n",
    "print(f'STARTING REFINED TRAINING')\n",
    "print(f'{\"=\"*80}\\n')\n",
    "print(f'Start time: {datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "print(f'Device: {device}\\n')\n",
    "\n",
    "# Execute training\n",
    "results = model.train(**train_params)\n",
    "\n",
    "print(f'\\n{\"=\"*80}')\n",
    "print(f'TRAINING COMPLETED')\n",
    "print(f'{\"=\"*80}\\n')\n",
    "print(f'End time: {datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "print(f'Results directory: {results.save_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dbebfb",
   "metadata": {},
   "source": [
    "## Export Best Model to Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507bd7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Find best model from training run\n",
    "best_model = Path(results.save_dir) / 'weights' / 'best.pt'\n",
    "last_model = Path(results.save_dir) / 'weights' / 'last.pt'\n",
    "\n",
    "# Export to Drive\n",
    "export_dir = f'{DRIVE_ROOT}/refined_models'\n",
    "os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "if best_model.exists():\n",
    "    export_best = f'{export_dir}/refined_best_{timestamp}.pt'\n",
    "    shutil.copy2(best_model, export_best)\n",
    "    print(f'Best model exported to: {export_best}')\n",
    "\n",
    "if last_model.exists():\n",
    "    export_last = f'{export_dir}/refined_last_{timestamp}.pt'\n",
    "    shutil.copy2(last_model, export_last)\n",
    "    print(f'Last model exported to: {export_last}')\n",
    "\n",
    "# Copy results CSV\n",
    "results_csv = Path(results.save_dir) / 'results.csv'\n",
    "if results_csv.exists():\n",
    "    export_results = f'{export_dir}/refined_results_{timestamp}.csv'\n",
    "    shutil.copy2(results_csv, export_results)\n",
    "    print(f'Results CSV exported to: {export_results}')\n",
    "\n",
    "print(f'\\nAll files exported to Google Drive')\n",
    "print(f'  Location: {export_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebcce99",
   "metadata": {},
   "source": [
    "## Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208e10e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load results\n",
    "results_csv_path = Path(results.save_dir) / 'results.csv'\n",
    "\n",
    "if results_csv_path.exists():\n",
    "    df = pd.read_csv(results_csv_path)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    \n",
    "    print('Refined Training Results Summary')\n",
    "    print('=' * 80)\n",
    "    print(f'Total epochs: {len(df)}')\n",
    "    print()\n",
    "    \n",
    "    # Final metrics\n",
    "    print('Final Epoch Metrics:')\n",
    "    print('-' * 80)\n",
    "    print(f'Classification Loss (val): {df[\"val/cls_loss\"].iloc[-1]:.4f}')\n",
    "    print(f'Segmentation mAP@50-95:    {df[\"metrics/mAP50-95(M)\"].iloc[-1]:.4f}')\n",
    "    print(f'Segmentation Precision:     {df[\"metrics/precision(M)\"].iloc[-1]:.4f}')\n",
    "    print(f'Segmentation Recall:        {df[\"metrics/recall(M)\"].iloc[-1]:.4f}')\n",
    "    print()\n",
    "    \n",
    "    # Best metrics\n",
    "    best_cls_loss_idx = df['val/cls_loss'].idxmin()\n",
    "    best_map_idx = df['metrics/mAP50-95(M)'].idxmax()\n",
    "    \n",
    "    print('Best Performance:')\n",
    "    print('-' * 80)\n",
    "    print(f'Best Classification Loss:   {df[\"val/cls_loss\"].iloc[best_cls_loss_idx]:.4f} (epoch {df[\"epoch\"].iloc[best_cls_loss_idx]:.0f})')\n",
    "    print(f'Best Segmentation mAP:      {df[\"metrics/mAP50-95(M)\"].iloc[best_map_idx]:.4f} (epoch {df[\"epoch\"].iloc[best_map_idx]:.0f})')\n",
    "    print()\n",
    "    \n",
    "    # Improvement over baseline\n",
    "    baseline_cls_loss = 0.4321  # From epoch 68 of original training\n",
    "    baseline_map = 0.4799\n",
    "    \n",
    "    final_cls_loss = df['val/cls_loss'].iloc[-1]\n",
    "    final_map = df['metrics/mAP50-95(M)'].iloc[-1]\n",
    "    \n",
    "    cls_improvement = ((baseline_cls_loss - final_cls_loss) / baseline_cls_loss) * 100\n",
    "    map_improvement = ((final_map - baseline_map) / baseline_map) * 100\n",
    "    \n",
    "    print('Improvement Over Baseline (Epoch 68):')\n",
    "    print('-' * 80)\n",
    "    print(f'Classification Loss: {baseline_cls_loss:.4f} → {final_cls_loss:.4f} ({cls_improvement:+.2f}%)')\n",
    "    print(f'Segmentation mAP:    {baseline_map:.4f} → {final_map:.4f} ({map_improvement:+.2f}%)')\n",
    "    print()\n",
    "    \n",
    "    # Plot training curves\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('Refined Training Performance', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot 1: Classification Loss\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(df['epoch'], df['train/cls_loss'], label='Train', linewidth=2, alpha=0.7)\n",
    "    ax.plot(df['epoch'], df['val/cls_loss'], label='Validation', linewidth=2)\n",
    "    ax.axhline(y=baseline_cls_loss, color='red', linestyle='--', label=f'Baseline ({baseline_cls_loss:.4f})', alpha=0.5)\n",
    "    ax.axvline(x=30, color='gray', linestyle=':', alpha=0.5, label='Phase 2')\n",
    "    ax.axvline(x=60, color='gray', linestyle=':', alpha=0.5, label='Phase 3')\n",
    "    ax.set_xlabel('Epoch', fontweight='bold')\n",
    "    ax.set_ylabel('Loss', fontweight='bold')\n",
    "    ax.set_title('Classification Loss (Lower is Better)', fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    \n",
    "    # Plot 2: Segmentation mAP\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(df['epoch'], df['metrics/mAP50-95(M)'], linewidth=2, color='green')\n",
    "    ax.axhline(y=baseline_map, color='red', linestyle='--', label=f'Baseline ({baseline_map:.4f})', alpha=0.5)\n",
    "    ax.axvline(x=30, color='gray', linestyle=':', alpha=0.5)\n",
    "    ax.axvline(x=60, color='gray', linestyle=':', alpha=0.5)\n",
    "    ax.set_xlabel('Epoch', fontweight='bold')\n",
    "    ax.set_ylabel('mAP@50-95', fontweight='bold')\n",
    "    ax.set_title('Segmentation Quality (Higher is Better)', fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    ax.set_ylim([0.4, 1.0])\n",
    "    \n",
    "    # Plot 3: Precision & Recall\n",
    "    ax = axes[1, 0]\n",
    "    ax.plot(df['epoch'], df['metrics/precision(M)'], label='Precision', linewidth=2)\n",
    "    ax.plot(df['epoch'], df['metrics/recall(M)'], label='Recall', linewidth=2)\n",
    "    ax.axvline(x=30, color='gray', linestyle=':', alpha=0.5)\n",
    "    ax.axvline(x=60, color='gray', linestyle=':', alpha=0.5)\n",
    "    ax.set_xlabel('Epoch', fontweight='bold')\n",
    "    ax.set_ylabel('Score', fontweight='bold')\n",
    "    ax.set_title('Precision & Recall', fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    ax.set_ylim([0.7, 1.0])\n",
    "    \n",
    "    # Plot 4: Learning Rate\n",
    "    ax = axes[1, 1]\n",
    "    ax.plot(df['epoch'], df['lr/pg0'], linewidth=2, color='purple')\n",
    "    ax.axvline(x=30, color='gray', linestyle=':', alpha=0.5, label='Phase transitions')\n",
    "    ax.axvline(x=60, color='gray', linestyle=':', alpha=0.5)\n",
    "    ax.set_xlabel('Epoch', fontweight='bold')\n",
    "    ax.set_ylabel('Learning Rate', fontweight='bold')\n",
    "    ax.set_title('Learning Rate Schedule', fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    plot_path = Path(results.save_dir) / 'refined_training_analysis.png'\n",
    "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "    print(f'Training curves saved to: {plot_path}')\n",
    "    \n",
    "    # Also save to Drive\n",
    "    drive_plot_path = f'{export_dir}/refined_training_analysis_{timestamp}.png'\n",
    "    shutil.copy2(plot_path, drive_plot_path)\n",
    "    print(f'Plots exported to Drive: {drive_plot_path}')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print('Results CSV not found. Training may not have completed.')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
