{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giosanchez0208/CSC173-DeepCV-Sanchez/blob/main/refine_segmentation_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a7e14ac",
      "metadata": {
        "id": "3a7e14ac"
      },
      "source": [
        "## Setup: Google Colab Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "02620f41",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02620f41",
        "outputId": "f9664faa-448d-49b3-c78d-72863d82e601"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU available: Tesla T4\n",
            "Memory: 15.83 GB\n",
            "\n",
            "PyTorch version: 2.9.0+cu126\n",
            "CUDA version: 12.6\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Verify CUDA is available\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "    print(f'GPU available: {torch.cuda.get_device_name(0)}')\n",
        "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')\n",
        "else:\n",
        "    device = 'cpu'\n",
        "    print('No GPU available, using CPU (training will be slow)')\n",
        "\n",
        "print(f'\\nPyTorch version: {torch.__version__}')\n",
        "print(f'CUDA version: {torch.version.cuda}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1b038eae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b038eae",
        "outputId": "575f253b-46fd-468a-f689-450ba3ca2ab8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All packages installed successfully\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install -q ultralytics opencv-python-headless pillow pyyaml numpy scipy matplotlib pandas\n",
        "\n",
        "print('All packages installed successfully')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0b48dbb1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b48dbb1",
        "outputId": "87f31052-0c08-48cd-a334-bf27e57bca5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Drive mounted and working directory set\n",
            "Data config: /content/drive/MyDrive/csc173_dataset/dataset/data.yaml\n",
            "Pretrained model: /content/drive/MyDrive/csc173_dataset/models/custom_ocr_last.pt\n",
            "Working directory: /content/refined_training\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive (if using dataset from Drive)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set paths - MODIFY THESE TO YOUR ACTUAL PATHS\n",
        "DRIVE_ROOT = '/content/drive/MyDrive/csc173_dataset'\n",
        "DATA_YAML_PATH = f'{DRIVE_ROOT}/dataset/data.yaml'\n",
        "PRETRAINED_MODEL = f'{DRIVE_ROOT}/models/custom_ocr_last.pt'  # Your existing checkpoint\n",
        "\n",
        "# Create local working directory\n",
        "WORK_DIR = '/content/refined_training'\n",
        "os.makedirs(WORK_DIR, exist_ok=True)\n",
        "os.chdir(WORK_DIR)\n",
        "\n",
        "print(f'Drive mounted and working directory set')\n",
        "print(f'Data config: {DATA_YAML_PATH}')\n",
        "print(f'Pretrained model: {PRETRAINED_MODEL}')\n",
        "print(f'Working directory: {WORK_DIR}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6201797b",
      "metadata": {
        "id": "6201797b"
      },
      "source": [
        "## Copy Dataset to Local Storage (Fixes Slow I/O)\n",
        "\n",
        "Google Drive mounting is extremely slow for datasets with many small files. We'll copy the dataset to Colab's local SSD once per session for fast training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b2cd3d33",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2cd3d33",
        "outputId": "35de4580-5cc8-4873-dec1-6924a83d480d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset from zip to local SSD...\n",
            "================================================================================\n",
            "Source: /content/drive/MyDrive/csc173_dataset/dataset.zip\n",
            "Destination: /content/local_dataset\n",
            "\n",
            "Extracting zip file (this is MUCH faster than copying individual files)...\n",
            "\n",
            "Dataset extracted in 2.6 minutes\n",
            "\n",
            "Fixing data.yaml paths for Colab...\n",
            "  Updated data.yaml with correct paths\n",
            "  Base path: /content/local_dataset\n",
            "\n",
            "Updated paths:\n",
            "  Dataset: /content/local_dataset\n",
            "  data.yaml: /content/local_dataset/data.yaml\n",
            "  Model: /content/drive/MyDrive/csc173_dataset/models/custom_ocr_last.pt\n",
            "\n",
            "Training will now use LOCAL SSD\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "import time\n",
        "import zipfile\n",
        "\n",
        "print('Extracting dataset from zip to local SSD...')\n",
        "print('=' * 80)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Local paths on Colab's SSD - FAST!)\n",
        "LOCAL_DATASET_ROOT = '/content/local_dataset'\n",
        "LOCAL_DATA_YAML = f'{LOCAL_DATASET_ROOT}/data.yaml'\n",
        "\n",
        "# Path to the zip file on Google Drive\n",
        "DATASET_ZIP = f'{DRIVE_ROOT}/dataset.zip'\n",
        "\n",
        "# Check if already extracted\n",
        "if Path(LOCAL_DATASET_ROOT).exists() and Path(LOCAL_DATA_YAML).exists():\n",
        "    print('Dataset already extracted to local storage')\n",
        "else:\n",
        "    # Extract dataset from zip file\n",
        "    print(f'Source: {DATASET_ZIP}')\n",
        "    print(f'Destination: {LOCAL_DATASET_ROOT}')\n",
        "    print()\n",
        "\n",
        "    if not Path(DATASET_ZIP).exists():\n",
        "        print(f'   ERROR: dataset.zip not found at {DATASET_ZIP}')\n",
        "        print(f'   Please ensure dataset.zip is uploaded to {DRIVE_ROOT}/')\n",
        "        raise FileNotFoundError(f'dataset.zip not found at {DATASET_ZIP}')\n",
        "\n",
        "    print('Extracting zip file (this is MUCH faster than copying individual files)...')\n",
        "\n",
        "    # Create parent directory\n",
        "    Path(LOCAL_DATASET_ROOT).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Extract the zip file\n",
        "    with zipfile.ZipFile(DATASET_ZIP, 'r') as zip_ref:\n",
        "        zip_ref.extractall('/content')\n",
        "\n",
        "    # Check if extraction created a nested 'dataset' folder\n",
        "    if Path('/content/dataset').exists():\n",
        "        # Move contents from /content/dataset to LOCAL_DATASET_ROOT\n",
        "        shutil.move('/content/dataset', LOCAL_DATASET_ROOT)\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f'\\nDataset extracted in {elapsed/60:.1f} minutes')\n",
        "\n",
        "    # Fix data.yaml paths (it may contain Windows paths from local machine)\n",
        "    print('\\nFixing data.yaml paths for Colab...')\n",
        "    import yaml\n",
        "\n",
        "    if Path(LOCAL_DATA_YAML).exists():\n",
        "        with open(LOCAL_DATA_YAML, 'r') as f:\n",
        "            data_config = yaml.safe_load(f)\n",
        "\n",
        "        # Update paths to use local dataset root\n",
        "        data_config['path'] = LOCAL_DATASET_ROOT\n",
        "        data_config['train'] = 'train/images'\n",
        "        data_config['val'] = 'val/images'\n",
        "        data_config['test'] = 'test/images'\n",
        "\n",
        "        # Write back\n",
        "        with open(LOCAL_DATA_YAML, 'w') as f:\n",
        "            yaml.dump(data_config, f, default_flow_style=False)\n",
        "\n",
        "        print(f'  Updated data.yaml with correct paths')\n",
        "        print(f'  Base path: {LOCAL_DATASET_ROOT}')\n",
        "    else:\n",
        "        print(f'  Warning: data.yaml not found at {LOCAL_DATA_YAML}')\n",
        "\n",
        "# Update paths to use LOCAL storage instead of Drive\n",
        "DATA_YAML_PATH = LOCAL_DATA_YAML\n",
        "\n",
        "print()\n",
        "print('Updated paths:')\n",
        "print(f'  Dataset: {LOCAL_DATASET_ROOT}')\n",
        "print(f'  data.yaml: {DATA_YAML_PATH}')\n",
        "print(f'  Model: {PRETRAINED_MODEL}')\n",
        "print()\n",
        "print('Training will now use LOCAL SSD')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cb3737f",
      "metadata": {
        "id": "9cb3737f"
      },
      "source": [
        "## Core Components (Reused from Original Training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7000dfec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7000dfec",
        "outputId": "53959067-aad9-4634-d058-3b850742d3e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes: 36\n",
            "Characters: ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\n",
            "Similarity matrix initialized: torch.Size([36, 36])\n"
          ]
        }
      ],
      "source": [
        "# Character set and similarity matrix (from original)\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "CHARS = [chr(i) for i in range(65, 91)] + [str(i) for i in range(10)]\n",
        "NUM_CLASSES = len(CHARS)\n",
        "CHAR_TO_IDX = {c: i for i, c in enumerate(CHARS)}\n",
        "IDX_TO_CHAR = {i: c for i, c in enumerate(CHARS)}\n",
        "\n",
        "print(f'Number of classes: {NUM_CLASSES}')\n",
        "print(f'Characters: {\"\".join(CHARS)}')\n",
        "\n",
        "SIMILAR_GROUPS = [\n",
        "    ['O', '0'], # I want to refine Q and 0/O differentiation, so it was removed from the group\n",
        "    ['I', '1'], # I want to refine L and 1/I differentiation, so it was removed from the group\n",
        "    ['S', '5'],\n",
        "    ['Z', '2'],\n",
        "    ['B', '8'],\n",
        "    ['D', '0'],\n",
        "    ['G', 'C'],\n",
        "    ['U', 'V'],\n",
        "    ['P', 'R'],\n",
        "]\n",
        "\n",
        "def create_similarity_matrix(num_classes=NUM_CLASSES, groups=SIMILAR_GROUPS, base_sim=0.6):\n",
        "    S = np.zeros((num_classes, num_classes), dtype=np.float32)\n",
        "    np.fill_diagonal(S, 1.0)\n",
        "    for group in groups:\n",
        "        idxs = [CHAR_TO_IDX[c] for c in group if c in CHAR_TO_IDX]\n",
        "        for i in idxs:\n",
        "            for j in idxs:\n",
        "                if i != j:\n",
        "                    S[i, j] = base_sim\n",
        "    return torch.tensor(S, dtype=torch.float32)\n",
        "\n",
        "similarity_matrix = create_similarity_matrix()\n",
        "print(f'Similarity matrix initialized: {similarity_matrix.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0ebec263",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ebec263",
        "outputId": "49bafe7d-223c-46eb-abf2-1edee04dffcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Refined similarity-aware loss defined\n"
          ]
        }
      ],
      "source": [
        "# Enhanced Similarity-Aware Loss with Adaptive Weighting\n",
        "class RefinedSimilarityAwareTopKLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Enhanced loss for fine-tuning with:\n",
        "    - Higher penalty for similar character confusion\n",
        "    - Adaptive temperature based on training phase\n",
        "    - Confidence-based weighting\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=NUM_CLASSES, similarity_matrix=None,\n",
        "                 k=3, initial_temperature=0.5, base_weight=0.5, topk_weight=0.5,\n",
        "                 epochs=40):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.k = k\n",
        "        self.initial_temperature = initial_temperature\n",
        "        self.base_weight = base_weight\n",
        "        self.topk_weight = topk_weight\n",
        "        self.epochs = epochs\n",
        "        self.current_epoch = 0\n",
        "\n",
        "        if similarity_matrix is not None:\n",
        "            self.register_buffer('similarity_matrix', similarity_matrix)\n",
        "        else:\n",
        "            self.register_buffer('similarity_matrix', create_similarity_matrix())\n",
        "\n",
        "    def update_epoch(self, epoch):\n",
        "        \"\"\"Update current epoch for temperature annealing.\"\"\"\n",
        "        self.current_epoch = epoch\n",
        "\n",
        "    def get_temperature(self):\n",
        "        \"\"\"Anneal temperature more aggressively for fine-tuning.\"\"\"\n",
        "        progress = self.current_epoch / max(self.epochs, 1)\n",
        "        # Start at 0.5, go to 0.3 (sharper predictions)\n",
        "        return max(0.3, self.initial_temperature - progress * 0.2)\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        B = logits.size(0)\n",
        "        device = logits.device\n",
        "\n",
        "        temperature = self.get_temperature()\n",
        "\n",
        "        # Standard cross-entropy\n",
        "        ce_loss = F.cross_entropy(logits, targets, reduction='none')\n",
        "\n",
        "        # Softmax with temperature\n",
        "        probs = F.softmax(logits / temperature, dim=1)\n",
        "        topk_probs, topk_indices = torch.topk(probs, min(self.k, self.num_classes), dim=1)\n",
        "\n",
        "        # Similarity-aware penalty\n",
        "        sim_loss = torch.zeros(B, device=device)\n",
        "        confidence_scores = []\n",
        "\n",
        "        for i in range(B):\n",
        "            t = targets[i].item()\n",
        "            if t < 0 or t >= self.num_classes:\n",
        "                continue\n",
        "\n",
        "            sims = self.similarity_matrix[t][topk_indices[i]]\n",
        "\n",
        "            # Higher penalty for similar character confusion\n",
        "            # If model confuses O with 0 (high similarity), penalty is lower\n",
        "            # If model confuses O with X (low similarity), penalty is higher\n",
        "            penalties = (1.0 - sims) * 1.5  # Amplify penalty\n",
        "            weighted_penalties = topk_probs[i] * penalties\n",
        "            sim_loss[i] = weighted_penalties.sum()\n",
        "\n",
        "            confidence_scores.append(topk_probs[i][0].item())\n",
        "\n",
        "        if len(confidence_scores) == 0:\n",
        "            return ce_loss.mean()\n",
        "\n",
        "        # Adaptive weighting based on confidence\n",
        "        confidence = torch.tensor(confidence_scores, device=device)\n",
        "\n",
        "        # When confident: rely more on CE (trust the model)\n",
        "        # When uncertain: rely more on similarity (guide the model)\n",
        "        adaptive_base = self.base_weight + (1 - confidence) * 0.2\n",
        "        adaptive_topk = self.topk_weight + confidence * 0.2\n",
        "\n",
        "        # Normalize\n",
        "        total_weight = adaptive_base + adaptive_topk\n",
        "        adaptive_base = adaptive_base / total_weight\n",
        "        adaptive_topk = adaptive_topk / total_weight\n",
        "\n",
        "        total_loss = adaptive_base * ce_loss + adaptive_topk * sim_loss\n",
        "        return total_loss.mean()\n",
        "\n",
        "print('Refined similarity-aware loss defined')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4557780b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4557780b",
        "outputId": "1113956f-f51f-4d1b-f762-1bba1fb43a93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OCR metrics module loaded\n"
          ]
        }
      ],
      "source": [
        "# OCR Metrics (reused from original)\n",
        "class OCRMetrics:\n",
        "    \"\"\"Compute OCR-specific validation metrics.\"\"\"\n",
        "    def __init__(self, similarity_matrix=None):\n",
        "        self.similarity_matrix = similarity_matrix if similarity_matrix is not None else create_similarity_matrix()\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.total_chars = 0\n",
        "        self.correct_chars = 0\n",
        "        self.top2_correct = 0\n",
        "        self.top3_correct = 0\n",
        "        self.similarity_score = 0.0\n",
        "\n",
        "    def update(self, predictions, targets, top_k_preds=None):\n",
        "        predictions = predictions.cpu().numpy()\n",
        "        targets = targets.cpu().numpy()\n",
        "\n",
        "        self.total_chars += len(targets)\n",
        "        self.correct_chars += (predictions == targets).sum()\n",
        "\n",
        "        # Similarity-aware accuracy\n",
        "        for pred, target in zip(predictions, targets):\n",
        "            if 0 <= target < len(self.similarity_matrix) and 0 <= pred < len(self.similarity_matrix):\n",
        "                sim = self.similarity_matrix[target][pred].item()\n",
        "                self.similarity_score += sim\n",
        "\n",
        "        # Top-k accuracy\n",
        "        if top_k_preds is not None:\n",
        "            top_k_preds = top_k_preds.cpu().numpy()\n",
        "            for i, target in enumerate(targets):\n",
        "                if top_k_preds.shape[1] >= 2 and target in top_k_preds[i, :2]:\n",
        "                    self.top2_correct += 1\n",
        "                if top_k_preds.shape[1] >= 3 and target in top_k_preds[i, :3]:\n",
        "                    self.top3_correct += 1\n",
        "\n",
        "    def compute(self):\n",
        "        if self.total_chars == 0:\n",
        "            return {}\n",
        "\n",
        "        return {\n",
        "            'CER': 1.0 - (self.correct_chars / self.total_chars),\n",
        "            'char_accuracy': self.correct_chars / self.total_chars,\n",
        "            'top2_accuracy': self.top2_correct / self.total_chars,\n",
        "            'top3_accuracy': self.top3_correct / self.total_chars,\n",
        "            'similarity_aware_accuracy': self.similarity_score / self.total_chars,\n",
        "        }\n",
        "\n",
        "print('OCR metrics module loaded')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0e25dad",
      "metadata": {
        "id": "c0e25dad"
      },
      "source": [
        "## Refined Training Strategy\n",
        "\n",
        "### Phase 1: Classifier Head Fine-Tuning (Epochs 1-12)\n",
        "- Freeze backbone and segmentation head\n",
        "- Focus exclusively on improving character classification\n",
        "- Use cyclic learning rate to escape plateau\n",
        "\n",
        "### Phase 2: Progressive Unfreezing (Epochs 13-24)\n",
        "- Gradually unfreeze deeper layers\n",
        "- Lower learning rate for stable refinement\n",
        "- Continue with similarity-aware loss\n",
        "\n",
        "### Phase 3: Full Fine-Tuning (Epochs 25-40)\n",
        "- All layers unfrozen\n",
        "- Very low learning rate for final polish\n",
        "- Focus on reducing classification loss below 0.35"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e4287274",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4287274",
        "outputId": "dc4776c2-9b41-42f6-8327-cbae8f6a475a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Refined segmentation trainer defined with checkpoint support\n"
          ]
        }
      ],
      "source": [
        "# Custom Trainer for Refined Training\n",
        "from ultralytics.models.yolo.segment import SegmentationTrainer\n",
        "from ultralytics import YOLO\n",
        "\n",
        "class RefinedSegmentationTrainer(SegmentationTrainer):\n",
        "    \"\"\"\n",
        "    Refined trainer with:\n",
        "    - Progressive layer unfreezing\n",
        "    - Enhanced loss function\n",
        "    - OCR-specific metrics tracking\n",
        "    - Cyclic learning rate support\n",
        "    - Checkpoint saving after each epoch\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg=None, overrides=None, _callbacks=None):\n",
        "        super().__init__(cfg, overrides, _callbacks)\n",
        "\n",
        "        # Get total epochs from config\n",
        "        total_epochs = self.args.epochs if hasattr(self.args, 'epochs') else 40\n",
        "\n",
        "        # Initialize refined loss\n",
        "        self.character_loss_fn = RefinedSimilarityAwareTopKLoss(\n",
        "            num_classes=NUM_CLASSES,\n",
        "            similarity_matrix=similarity_matrix,\n",
        "            k=3,\n",
        "            initial_temperature=0.5,\n",
        "            base_weight=0.5,\n",
        "            topk_weight=0.5,\n",
        "            epochs=total_epochs\n",
        "        ).to(device)\n",
        "\n",
        "        # OCR metrics\n",
        "        self.ocr_metrics = OCRMetrics(similarity_matrix=similarity_matrix)\n",
        "\n",
        "        # Training phase tracking\n",
        "        self.phase = 1\n",
        "        self.freeze_applied = False\n",
        "\n",
        "    def _setup_train(self, world_size):\n",
        "        \"\"\"Override to apply layer freezing for Phase 1.\"\"\"\n",
        "        super()._setup_train(world_size)\n",
        "\n",
        "        if not self.freeze_applied and self.epoch < 12:\n",
        "            print(f'\\n=== PHASE 1: Classifier Head Fine-Tuning (Epochs 1-12) ===')\n",
        "            print('Freezing backbone and segmentation layers...')\n",
        "\n",
        "            # Freeze all layers except classification head\n",
        "            for name, param in self.model.named_parameters():\n",
        "                # Keep classification layers trainable\n",
        "                if 'cls' in name.lower() or 'cv3' in name.lower():\n",
        "                    param.requires_grad = True\n",
        "                else:\n",
        "                    param.requires_grad = False\n",
        "\n",
        "            trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "            total = sum(p.numel() for p in self.model.parameters())\n",
        "            print(f'Trainable parameters: {trainable:,} / {total:,} ({100*trainable/total:.1f}%)')\n",
        "            self.freeze_applied = True\n",
        "\n",
        "    def on_train_epoch_start(self):\n",
        "        \"\"\"Handle phase transitions and progressive unfreezing.\"\"\"\n",
        "        super().on_train_epoch_start()\n",
        "\n",
        "        # Update temperature in loss\n",
        "        self.character_loss_fn.update_epoch(self.epoch)\n",
        "\n",
        "        # Phase 2: Progressive unfreezing (epochs 12-24)\n",
        "        if self.epoch == 12:\n",
        "            self.phase = 2\n",
        "            print(f'\\n=== PHASE 2: Progressive Unfreezing (Epochs 13-24) ===')\n",
        "            print('Unfreezing segmentation head...')\n",
        "\n",
        "            for name, param in self.model.named_parameters():\n",
        "                if 'seg' in name.lower() or 'mask' in name.lower():\n",
        "                    param.requires_grad = True\n",
        "\n",
        "            trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "            total = sum(p.numel() for p in self.model.parameters())\n",
        "            print(f'Trainable parameters: {trainable:,} / {total:,} ({100*trainable/total:.1f}%)')\n",
        "\n",
        "        # Phase 3: Full fine-tuning (epochs 24+)\n",
        "        elif self.epoch == 24:\n",
        "            self.phase = 3\n",
        "            print(f'\\n=== PHASE 3: Full Fine-Tuning (Epochs 25-40) ===')\n",
        "            print('Unfreezing all layers...')\n",
        "\n",
        "            for param in self.model.parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "            trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "            total = sum(p.numel() for p in self.model.parameters())\n",
        "            print(f'Trainable parameters: {trainable:,} / {total:,} ({100*trainable/total:.1f}%)')\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        \"\"\"Save checkpoint after each epoch.\"\"\"\n",
        "        super().on_train_epoch_end()\n",
        "\n",
        "        # Extract metrics from validator\n",
        "        if hasattr(self, 'metrics') and self.metrics is not None:\n",
        "            metrics_dict = {}\n",
        "\n",
        "            # Try to get metrics from the metrics object\n",
        "            if hasattr(self.metrics, 'results_dict'):\n",
        "                metrics_dict = self.metrics.results_dict\n",
        "\n",
        "            # Also get from validator\n",
        "            if hasattr(self, 'validator') and hasattr(self.validator, 'metrics'):\n",
        "                val_metrics = self.validator.metrics\n",
        "                if hasattr(val_metrics, 'results_dict'):\n",
        "                    metrics_dict.update(val_metrics.results_dict)\n",
        "\n",
        "            # Get loss values\n",
        "            if hasattr(self, 'loss_items'):\n",
        "                loss_items = self.loss_items\n",
        "                if loss_items is not None and len(loss_items) > 0:\n",
        "                    metrics_dict['train/cls_loss'] = float(loss_items[0]) if len(loss_items) > 0 else 0.0\n",
        "                    metrics_dict['train/seg_loss'] = float(loss_items[1]) if len(loss_items) > 1 else 0.0\n",
        "\n",
        "            # Get learning rate\n",
        "            if hasattr(self, 'optimizer'):\n",
        "                metrics_dict['lr/pg0'] = self.optimizer.param_groups[0]['lr']\n",
        "\n",
        "            # Save checkpoint with smart model saving\n",
        "            last_model = Path(self.save_dir) / 'weights' / 'last.pt'\n",
        "            best_model = Path(self.save_dir) / 'weights' / 'best.pt'\n",
        "            save_checkpoint(self.epoch, metrics_dict, str(last_model), str(best_model), force_save_interval=10)\n",
        "\n",
        "    def on_val_start(self):\n",
        "        super().on_val_start()\n",
        "        self.ocr_metrics.reset()\n",
        "\n",
        "    def on_val_end(self):\n",
        "        super().on_val_end()\n",
        "\n",
        "        # Log OCR metrics\n",
        "        ocr_results = self.ocr_metrics.compute()\n",
        "        if ocr_results:\n",
        "            print(f'\\n[Epoch {self.epoch}] OCR Metrics:')\n",
        "            for key, value in ocr_results.items():\n",
        "                print(f'  {key}: {value:.4f}')\n",
        "\n",
        "    def compute_loss(self, preds, batch):\n",
        "        \"\"\"Compute loss with refined similarity-aware classification.\"\"\"\n",
        "        # Get base YOLO losses\n",
        "        base_loss = super().compute_loss(preds, batch)\n",
        "\n",
        "        # Add custom similarity-aware character classification loss\n",
        "        if len(preds) > 3:\n",
        "            cls_logits = preds[3]\n",
        "            cls_targets = batch['cls'].long()\n",
        "\n",
        "            if cls_logits is not None and cls_targets is not None:\n",
        "                cls_logits_flat = cls_logits.view(-1, NUM_CLASSES)\n",
        "                cls_targets_flat = cls_targets.view(-1)\n",
        "\n",
        "                valid_mask = cls_targets_flat >= 0\n",
        "                if valid_mask.sum() > 0:\n",
        "                    # Compute refined similarity-aware loss\n",
        "                    char_loss = self.character_loss_fn(\n",
        "                        cls_logits_flat[valid_mask],\n",
        "                        cls_targets_flat[valid_mask]\n",
        "                    )\n",
        "\n",
        "                    # Update OCR metrics\n",
        "                    with torch.no_grad():\n",
        "                        preds_cls = cls_logits_flat[valid_mask].argmax(dim=1)\n",
        "                        top_k_preds = torch.topk(cls_logits_flat[valid_mask], k=3, dim=1)[1]\n",
        "                        self.ocr_metrics.update(\n",
        "                            preds_cls,\n",
        "                            cls_targets_flat[valid_mask],\n",
        "                            top_k_preds\n",
        "                        )\n",
        "\n",
        "                    # Phase-dependent weighting\n",
        "                    if self.phase == 1:\n",
        "                        # Phase 1: Heavy emphasis on classification\n",
        "                        cls_weight = 0.7\n",
        "                    elif self.phase == 2:\n",
        "                        # Phase 2: Balanced\n",
        "                        cls_weight = 0.5\n",
        "                    else:\n",
        "                        # Phase 3: Standard weighting\n",
        "                        cls_weight = 0.3\n",
        "\n",
        "                    total_loss = (1 - cls_weight) * base_loss + cls_weight * char_loss\n",
        "                    return total_loss\n",
        "\n",
        "        return base_loss\n",
        "\n",
        "print('Refined segmentation trainer defined with checkpoint support')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b00e62a",
      "metadata": {
        "id": "0b00e62a"
      },
      "source": [
        "## Load Pretrained Model and Configure Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "b4b8d469",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4b8d469",
        "outputId": "56070d45-b231-4df7-849e-19b2db74b0d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /content/drive/MyDrive/csc173_dataset/models/custom_ocr_last.pt\n",
            "\n",
            "Model loaded for FRESH training\n",
            "  Starting from pretrained checkpoint\n",
            "  Total target epochs: 40\n"
          ]
        }
      ],
      "source": [
        "# Load model (either from checkpoint or pretrained)\n",
        "\n",
        "# Check if checkpoint management cell was run\n",
        "if 'RESUME_FROM' not in locals() and 'RESUME_FROM' not in globals():\n",
        "    print('    ERROR: Please run the \"Checkpoint Management\" cell first!')\n",
        "    print('   (The cell that defines RESUME_FROM and checkpoint tracking)')\n",
        "    raise RuntimeError('Checkpoint management cell must be run before loading model')\n",
        "\n",
        "print(f'Loading model from: {RESUME_FROM}\\n')\n",
        "\n",
        "model = YOLO(RESUME_FROM)\n",
        "model.trainer = RefinedSegmentationTrainer\n",
        "\n",
        "if RESUME_TRAINING:\n",
        "    print(f'Model loaded for RESUMING training')\n",
        "    print(f'  Will continue from epoch {checkpoint_info[\"last_epoch\"] + 1}')\n",
        "else:\n",
        "    print(f'Model loaded for FRESH training')\n",
        "    print(f'  Starting from pretrained checkpoint')\n",
        "\n",
        "print(f'  Total target epochs: {REFINE_EPOCHS}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77ae0242",
      "metadata": {
        "id": "77ae0242"
      },
      "source": [
        "## Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "bada4767",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bada4767",
        "outputId": "6ae1e19a-c089-4a9b-f023-2b3545534e69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Refined Training Configuration:\n",
            "  Epochs: 40\n",
            "  Batch size: 16\n",
            "  Learning rate: 0.005 â†’ 0.0001\n",
            "  Augmentations: Enhanced HSV + Erasing + Geometric\n",
            "\n",
            "Training Strategy:\n",
            "  Phase 1 (1-12): Classifier head only\n",
            "  Phase 2 (13-24): + Segmentation head\n",
            "  Phase 3 (25-40): All layers\n"
          ]
        }
      ],
      "source": [
        "# Refined training hyperparameters\n",
        "REFINE_EPOCHS = 40\n",
        "BATCH_SIZE = 16\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# Cyclic learning rate for Phase 1 (escaping plateau)\n",
        "# Start higher to shake the model out of local minimum\n",
        "LR0 = 0.005  # Higher than previous 0.001\n",
        "LRF = 0.0001  # End lower for fine control\n",
        "\n",
        "# Optimizer settings\n",
        "MOMENTUM = 0.937\n",
        "WEIGHT_DECAY = 5e-4\n",
        "WARMUP_EPOCHS = 3.0\n",
        "\n",
        "# Augmentations - more aggressive for character robustness\n",
        "AUG_HSV_H = 0.02  # Increased hue variation\n",
        "AUG_HSV_S = 0.8   # Increased saturation variation\n",
        "AUG_HSV_V = 0.5   # Increased brightness variation\n",
        "AUG_ERASING = 0.5  # Increased random erasing\n",
        "AUG_DEGREES = 5.0  # Small rotation for character variation\n",
        "AUG_SHEAR = 2.0    # Perspective variation\n",
        "\n",
        "# Disabled augmentations (not useful for OCR)\n",
        "AUG_FLIPLR = 0.0\n",
        "AUG_MOSAIC = 0.0\n",
        "AUG_MIXUP = 0.0\n",
        "\n",
        "print('Refined Training Configuration:')\n",
        "print(f'  Epochs: {REFINE_EPOCHS}')\n",
        "print(f'  Batch size: {BATCH_SIZE}')\n",
        "print(f'  Learning rate: {LR0} â†’ {LRF}')\n",
        "print(f'  Augmentations: Enhanced HSV + Erasing + Geometric')\n",
        "print(f'\\nTraining Strategy:')\n",
        "print(f'  Phase 1 (1-12): Classifier head only')\n",
        "print(f'  Phase 2 (13-24): + Segmentation head')\n",
        "print(f'  Phase 3 (25-40): All layers')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "529f90cf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "529f90cf",
        "outputId": "f3bbebc8-c2b5-4e15-c63a-d19bd569dd23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing checkpoint CSV: /content/drive/MyDrive/csc173_dataset/refined_checkpoints/training_progress.csv\n",
            "\n",
            "=== STARTING FRESH TRAINING ===\n",
            "No previous checkpoint found\n",
            "\n",
            "Checkpoint directory: /content/drive/MyDrive/csc173_dataset/refined_checkpoints\n",
            "Progress CSV: /content/drive/MyDrive/csc173_dataset/refined_checkpoints/training_progress.csv\n",
            "\n",
            " Performance Note:\n",
            "   CSV is saved every epoch (fast)\n",
            "   Model is saved to Drive every 10 epochs (to avoid slow Drive I/O)\n",
            "   Local model checkpoints are saved every 5 epochs by YOLO\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "CHECKPOINT_DIR = f'{DRIVE_ROOT}/refined_checkpoints'\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "CHECKPOINT_CSV = f'{CHECKPOINT_DIR}/training_progress.csv'\n",
        "CHECKPOINT_MODEL = f'{CHECKPOINT_DIR}/checkpoint_latest.pt'\n",
        "\n",
        "def load_checkpoint_info():\n",
        "    \"\"\"Load checkpoint information if it exists.\"\"\"\n",
        "    if os.path.exists(CHECKPOINT_CSV):\n",
        "        df = pd.read_csv(CHECKPOINT_CSV)\n",
        "        if len(df) > 0:\n",
        "            last_row = df.iloc[-1]\n",
        "            return {\n",
        "                'exists': True,\n",
        "                'last_epoch': int(last_row['epoch']),\n",
        "                'total_epochs_trained': len(df),\n",
        "                'best_cls_loss': df['val/cls_loss'].min() if 'val/cls_loss' in df.columns else float('inf'),\n",
        "                'best_map': df['metrics/mAP50-95(M)'].max() if 'metrics/mAP50-95(M)' in df.columns else 0.0,\n",
        "                'checkpoint_path': CHECKPOINT_MODEL if os.path.exists(CHECKPOINT_MODEL) else None\n",
        "            }\n",
        "    return {'exists': False, 'best_cls_loss': float('inf'), 'best_map': 0.0}\n",
        "\n",
        "def initialize_checkpoint_csv():\n",
        "    \"\"\"Initialize the checkpoint CSV with headers.\"\"\"\n",
        "    if not os.path.exists(CHECKPOINT_CSV):\n",
        "        # Create empty CSV with expected columns\n",
        "        columns = ['epoch', 'train/cls_loss', 'val/cls_loss', 'train/seg_loss', 'val/seg_loss',\n",
        "                   'metrics/precision(M)', 'metrics/recall(M)', 'metrics/mAP50(M)',\n",
        "                   'metrics/mAP50-95(M)', 'lr/pg0', 'timestamp']\n",
        "        pd.DataFrame(columns=columns).to_csv(CHECKPOINT_CSV, index=False)\n",
        "        print(f'Created new checkpoint CSV: {CHECKPOINT_CSV}')\n",
        "        return {'exists': False, 'best_cls_loss': float('inf'), 'best_map': 0.0}\n",
        "    else:\n",
        "        print(f'Found existing checkpoint CSV: {CHECKPOINT_CSV}')\n",
        "        return load_checkpoint_info()\n",
        "\n",
        "def save_checkpoint(epoch, metrics, model_path, best_model_path=None, force_save_interval=10):\n",
        "    \"\"\"Save checkpoint after each epoch with smart model saving.\n",
        "\n",
        "    Args:\n",
        "        epoch: Current epoch number\n",
        "        metrics: Dictionary of metrics to save\n",
        "        model_path: Path to the last.pt model file\n",
        "        best_model_path: Path to the best.pt model file (if available)\n",
        "        force_save_interval: Force save model every N epochs as backup (default: 10)\n",
        "    \"\"\"\n",
        "    import datetime\n",
        "    import threading\n",
        "\n",
        "    # ALWAYS save metrics CSV (fast operation) - DO THIS FIRST\n",
        "    df = pd.read_csv(CHECKPOINT_CSV) if os.path.exists(CHECKPOINT_CSV) else pd.DataFrame()\n",
        "\n",
        "    new_row = {\n",
        "        'epoch': epoch,\n",
        "        'timestamp': datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        **metrics\n",
        "    }\n",
        "\n",
        "    df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
        "    df.to_csv(CHECKPOINT_CSV, index=False)\n",
        "    print(f'CSV saved (Epoch {epoch})')\n",
        "\n",
        "    # Determine if this is the best model\n",
        "    is_best = False\n",
        "    current_cls_loss = metrics.get('val/cls_loss', float('inf'))\n",
        "    current_map = metrics.get('metrics/mAP50-95(M)', 0.0)\n",
        "\n",
        "    # Check if this is the best model based on validation loss\n",
        "    if len(df) > 1:\n",
        "        prev_best_loss = df['val/cls_loss'].iloc[:-1].min() if 'val/cls_loss' in df.columns else float('inf')\n",
        "        if current_cls_loss < prev_best_loss:\n",
        "            is_best = True\n",
        "            print(f'  ðŸ† NEW BEST MODEL! val/cls_loss: {current_cls_loss:.4f} < {prev_best_loss:.4f}')\n",
        "    else:\n",
        "        is_best = True  # First epoch\n",
        "\n",
        "    # Save model to Drive if:\n",
        "    # 1. It's the best model, OR\n",
        "    # 2. It's a backup interval (every N epochs), OR\n",
        "    # 3. It's the first or last epoch\n",
        "    should_save = is_best or (epoch % force_save_interval == 0) or (epoch == 1)\n",
        "\n",
        "    if should_save:\n",
        "        # Prefer best.pt if available, otherwise use last.pt\n",
        "        model_to_save = best_model_path if (best_model_path and os.path.exists(best_model_path)) else model_path\n",
        "\n",
        "        if os.path.exists(model_to_save):\n",
        "            import shutil\n",
        "\n",
        "            # Save asynchronously in background thread to not block training\n",
        "            print(f'\\nSmart Checkpoint System:')\n",
        "            print(f'   CSV saved to Drive EVERY epoch (fast, ~1 second)')\n",
        "            print(f'   Best model saved to Drive IMMEDIATELY when improved (async upload)')\n",
        "            print(f'   Backup model saved every 10 epochs (in case best model corrupts)')\n",
        "\n",
        "\n",
        "            print(f'\\n   If you disconnect, you can resume from the last completed epoch in the CSV')\n",
        "            print(f'   All saves are asynchronous - training continues without blocking!')\n",
        "\n",
        "            def async_save():\n",
        "                try:\n",
        "                    reason = 'best model' if is_best else f'backup at epoch {epoch}'\n",
        "                    shutil.copy2(model_to_save, CHECKPOINT_MODEL)\n",
        "                    print(f'  Model saved to Drive ({reason})')\n",
        "                except Exception as e:\n",
        "                    print(f'  Warning: Failed to save model to Drive: {e}')\n",
        "\n",
        "            # Start async save\n",
        "            save_thread = threading.Thread(target=async_save, daemon=True)\n",
        "            save_thread.start()\n",
        "\n",
        "            if is_best:\n",
        "                print(f'  Uploading best model to Drive (async)...')\n",
        "            else:\n",
        "                print(f'  Backup save to Drive (async)...')\n",
        "        else:\n",
        "            print(f'  Model file not found at {model_to_save}')\n",
        "    else:\n",
        "        print(f'  (Model will be backed up to Drive at epoch {((epoch // force_save_interval) + 1) * force_save_interval})')\n",
        "\n",
        "# Check for existing checkpoint\n",
        "checkpoint_info = initialize_checkpoint_csv()\n",
        "\n",
        "if checkpoint_info and checkpoint_info['exists']:\n",
        "    print(f'\\n=== RESUMABLE TRAINING DETECTED ===')\n",
        "    print(f'Previous training found:')\n",
        "    print(f'  Last completed epoch: {checkpoint_info[\"last_epoch\"]}')\n",
        "    print(f'  Total epochs trained: {checkpoint_info[\"total_epochs_trained\"]}')\n",
        "    print(f'  Best classification loss: {checkpoint_info[\"best_cls_loss\"]:.4f}')\n",
        "    print(f'  Best segmentation mAP: {checkpoint_info[\"best_map\"]:.4f}')\n",
        "\n",
        "    if checkpoint_info['checkpoint_path']:\n",
        "        print(f'  Checkpoint model: {checkpoint_info[\"checkpoint_path\"]}')\n",
        "        RESUME_FROM = checkpoint_info['checkpoint_path']\n",
        "        RESUME_TRAINING = True\n",
        "    else:\n",
        "        print(f'  Checkpoint model not found, will start fresh')\n",
        "        RESUME_FROM = PRETRAINED_MODEL\n",
        "        RESUME_TRAINING = False\n",
        "else:\n",
        "    print(f'\\n=== STARTING FRESH TRAINING ===')\n",
        "    print(f'No previous checkpoint found')\n",
        "    RESUME_FROM = PRETRAINED_MODEL\n",
        "    RESUME_TRAINING = False\n",
        "\n",
        "print(f'\\nCheckpoint directory: {CHECKPOINT_DIR}')\n",
        "print(f'Progress CSV: {CHECKPOINT_CSV}')\n",
        "print(f'\\n Performance Note:')\n",
        "print(f'   CSV is saved every epoch (fast)')\n",
        "print(f'   Model is saved to Drive every 10 epochs (to avoid slow Drive I/O)')\n",
        "print(f'   Local model checkpoints are saved every 5 epochs by YOLO')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7eeb804e",
      "metadata": {
        "id": "7eeb804e"
      },
      "source": [
        "## Execute Refined Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cc4bac1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cc4bac1",
        "outputId": "29d9f717-8f2d-42de-fa28-44a6ff8d8bb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STARTING FRESH REFINED TRAINING\n",
            "================================================================================\n",
            "\n",
            "Start time: 2025-12-22 13:01:37\n",
            "Device: cuda\n",
            "Run name: refine_20251222_130137\n",
            "Checkpoint tracking: /content/drive/MyDrive/csc173_dataset/refined_checkpoints/training_progress.csv\n",
            "\n",
            "Ultralytics 8.3.241 ðŸš€ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/local_dataset/data.yaml, degrees=5.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=40, erasing=0.5, exist_ok=True, fliplr=0.0, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.02, hsv_s=0.8, hsv_v=0.5, imgsz=224, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.005, lrf=0.0001, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=/content/drive/MyDrive/csc173_dataset/models/custom_ocr_last.pt, momentum=0.937, mosaic=0.0, multi_scale=False, name=refine_20251222_130137, nbs=64, nms=False, opset=None, optimize=False, optimizer=SGD, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=refined_training, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/refined_training/refined_training/refine_20251222_130137, save_frames=False, save_json=False, save_period=5, save_txt=False, scale=0.5, seed=42, shear=2.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=segment, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% â”â”â”â”â”â”â”â”â”â”â”â” 755.1KB 105.0MB/s 0.0s\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
            "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
            " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
            " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
            " 23        [16, 19, 22]  1    690460  ultralytics.nn.modules.head.Segment          [36, 32, 64, [64, 128, 256]]  \n",
            "YOLO11n-seg summary: 203 layers, 2,849,628 parameters, 2,849,612 gradients, 9.8 GFLOPs\n",
            "\n",
            "Transferred 561/561 items from pretrained weights\n",
            "Freezing layer 'model.23.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 5.4MB 275.5MB/s 0.0s\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 24.8Â±18.5 MB/s, size: 125.6 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/local_dataset/train/labels... 15960 images, 1 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 15960/15960 477.2it/s 33.4s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/local_dataset/train/labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 54.8Â±35.5 MB/s, size: 308.9 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/local_dataset/val/labels... 2000 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 2000/2000 323.2it/s 6.2s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/local_dataset/val/labels.cache\n",
            "Plotting labels to /content/refined_training/refined_training/refine_20251222_130137/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.005, momentum=0.937) with parameter groups 90 weight(decay=0.0), 101 weight(decay=0.0005), 100 bias(decay=0.0)\n",
            "Image sizes 224 train, 224 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/refined_training/refined_training/refine_20251222_130137\u001b[0m\n",
            "Starting training for 40 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/40     0.449G     0.7604      1.267      0.521     0.8033         61        224: 100% â”â”â”â”â”â”â”â”â”â”â”â” 998/998 3.4it/s 4:58\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 2.0it/s 32.3s\n",
            "                   all       2000      15832      0.945      0.858      0.934      0.752      0.872      0.782      0.808      0.322\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/40      0.52G     0.7528      1.226     0.5135     0.8024         61        224: 100% â”â”â”â”â”â”â”â”â”â”â”â” 998/998 3.7it/s 4:27\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 2.3it/s 27.4s\n",
            "                   all       2000      15832       0.95      0.854      0.933      0.753      0.877      0.781       0.81      0.321\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/40      0.52G     0.7564      1.224     0.5241     0.8036         63        224: 100% â”â”â”â”â”â”â”â”â”â”â”â” 998/998 3.8it/s 4:23\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 2.4it/s 26.5s\n",
            "                   all       2000      15832      0.947      0.849       0.93      0.747      0.877      0.784      0.815      0.335\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/40      0.52G      0.759      1.226     0.5285     0.8043         61        224: 100% â”â”â”â”â”â”â”â”â”â”â”â” 998/998 3.8it/s 4:21\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 2.4it/s 26.2s\n",
            "                   all       2000      15832      0.946       0.85      0.929      0.743      0.874      0.779      0.811      0.324\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/40      0.52G     0.7568      1.222     0.5289     0.8031         62        224: 100% â”â”â”â”â”â”â”â”â”â”â”â” 998/998 3.8it/s 4:21\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 2.4it/s 26.2s\n",
            "                   all       2000      15832      0.942      0.846      0.928      0.745      0.872      0.769      0.805      0.319\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/40      0.52G     0.7567      1.215      0.527     0.8042         62        224: 100% â”â”â”â”â”â”â”â”â”â”â”â” 998/998 3.8it/s 4:21\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 2.3it/s 27.3s\n",
            "                   all       2000      15832      0.945      0.854      0.931      0.749      0.869      0.781      0.806       0.33\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/40      0.52G     0.7561      1.215     0.5242     0.8035         63        224: 100% â”â”â”â”â”â”â”â”â”â”â”â” 998/998 3.8it/s 4:24\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 2.3it/s 27.3s\n",
            "                   all       2000      15832      0.936      0.856      0.931      0.749      0.878      0.782      0.816      0.333\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/40      0.52G     0.7515      1.204     0.5213     0.8031         60        224: 100% â”â”â”â”â”â”â”â”â”â”â”â” 998/998 3.9it/s 4:19\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 2.4it/s 26.6s\n",
            "                   all       2000      15832      0.944      0.852       0.93      0.746      0.873       0.77      0.802      0.317\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/40     0.529G       0.75      1.197     0.5189     0.8025         59        224: 100% â”â”â”â”â”â”â”â”â”â”â”â” 998/998 3.9it/s 4:17\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 2.3it/s 27.0s\n",
            "                   all       2000      15832      0.942      0.853       0.93      0.749      0.872      0.786      0.815      0.338\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/40     0.529G     0.7495      1.195     0.5208     0.8025         62        224: 100% â”â”â”â”â”â”â”â”â”â”â”â” 998/998 3.8it/s 4:26\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 2.3it/s 27.6s\n",
            "                   all       2000      15832      0.949      0.847      0.931       0.75      0.871      0.775      0.804       0.32\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      11/40     0.531G      0.747      1.189     0.5159     0.8023         62        224: 100% â”â”â”â”â”â”â”â”â”â”â”â” 998/998 3.6it/s 4:34\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 2.3it/s 27.4s\n",
            "                   all       2000      15832      0.948      0.853      0.932      0.751      0.872      0.783      0.812      0.322\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      12/40     0.531G     0.7422      1.191     0.5134     0.8022         58        224: 100% â”â”â”â”â”â”â”â”â”â”â”â” 998/998 3.7it/s 4:31\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 2.3it/s 27.9s\n",
            "                   all       2000      15832      0.945      0.857      0.932      0.751       0.88      0.784      0.816       0.33\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      13/40     0.531G     0.7427      1.187     0.5124     0.8021         62        224: 100% â”â”â”â”â”â”â”â”â”â”â”â” 998/998 3.7it/s 4:31\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 2.3it/s 27.1s\n",
            "                   all       2000      15832      0.948      0.856      0.933      0.753      0.879      0.789       0.82      0.336\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      14/40     0.531G     0.7389      1.184     0.5083     0.8025         59        224: 100% â”â”â”â”â”â”â”â”â”â”â”â” 998/998 3.7it/s 4:27\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 2.3it/s 27.4s\n",
            "                   all       2000      15832      0.949      0.852      0.933      0.753       0.88      0.781      0.814       0.33\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      15/40     0.531G     0.7384      1.182     0.5083     0.8019         63        224: 100% â”â”â”â”â”â”â”â”â”â”â”â” 998/998 3.7it/s 4:31\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 2.2it/s 28.1s\n",
            "                   all       2000      15832      0.955      0.854      0.934      0.755      0.884      0.785      0.817      0.332\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      16/40     0.531G     0.7354      1.174     0.5063     0.8015         59        224: 100% â”â”â”â”â”â”â”â”â”â”â”â” 998/998 3.8it/s 4:26\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 2.3it/s 27.1s\n",
            "                   all       2000      15832      0.955      0.852      0.935      0.757      0.887      0.783      0.819      0.333\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      17/40     0.531G     0.7351      1.177     0.5048      0.801         61        224: 100% â”â”â”â”â”â”â”â”â”â”â”â” 998/998 3.7it/s 4:27\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 2.4it/s 25.9s\n",
            "                   all       2000      15832      0.949      0.861      0.935      0.756      0.881      0.793      0.821      0.337\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      18/40     0.531G     0.7348      1.178     0.5047     0.8011         63        224: 100% â”â”â”â”â”â”â”â”â”â”â”â” 998/998 3.9it/s 4:14\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 63/63 2.4it/s 26.3s\n",
            "                   all       2000      15832      0.953      0.858      0.935      0.758      0.874      0.783      0.809      0.322\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      19/40     0.531G     0.7352       1.17     0.4991      0.801        122        224: 47% â”â”â”â”â”â•¸â”€â”€â”€â”€â”€â”€ 474/998 3.4it/s 2:00<2:35"
          ]
        }
      ],
      "source": [
        "import datetime\n",
        "\n",
        "# Determine training name based on checkpoint status\n",
        "if RESUME_TRAINING:\n",
        "    run_name = f'refine_resumed_{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n",
        "else:\n",
        "    run_name = f'refine_{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n",
        "\n",
        "# Training parameters\n",
        "train_params = dict(\n",
        "    data=DATA_YAML_PATH,\n",
        "    epochs=REFINE_EPOCHS,\n",
        "    batch=BATCH_SIZE,\n",
        "    imgsz=IMG_SIZE,\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer='SGD',\n",
        "    lr0=LR0,\n",
        "    lrf=LRF,\n",
        "    momentum=MOMENTUM,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "\n",
        "    # Warmup\n",
        "    warmup_epochs=WARMUP_EPOCHS,\n",
        "    warmup_momentum=0.8,\n",
        "    warmup_bias_lr=0.1,\n",
        "\n",
        "    # Augmentations\n",
        "    hsv_h=AUG_HSV_H,\n",
        "    hsv_s=AUG_HSV_S,\n",
        "    hsv_v=AUG_HSV_V,\n",
        "    erasing=AUG_ERASING,\n",
        "    degrees=AUG_DEGREES,\n",
        "    shear=AUG_SHEAR,\n",
        "    fliplr=AUG_FLIPLR,\n",
        "    mosaic=AUG_MOSAIC,\n",
        "    mixup=AUG_MIXUP,\n",
        "\n",
        "    # Output settings\n",
        "    project='refined_training',\n",
        "    name=run_name,\n",
        "    exist_ok=True,\n",
        "\n",
        "    # Validation and saving\n",
        "    val=True,\n",
        "    save=True,\n",
        "    save_period=5,  # Save every 5 epochs\n",
        "\n",
        "    # System\n",
        "    device=device,\n",
        "    amp=True,  # Enable automatic mixed precision for faster training\n",
        "    seed=42,\n",
        "    deterministic=True,\n",
        "\n",
        "    # Resume handling\n",
        "    resume=RESUME_TRAINING,  # Resume if checkpoint exists\n",
        ")\n",
        "\n",
        "print(f'\\n{\"=\"*80}')\n",
        "if RESUME_TRAINING:\n",
        "    print(f'RESUMING REFINED TRAINING FROM EPOCH {checkpoint_info[\"last_epoch\"] + 1}')\n",
        "else:\n",
        "    print(f'STARTING FRESH REFINED TRAINING')\n",
        "print(f'{\"=\"*80}\\n')\n",
        "print(f'Start time: {datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
        "print(f'Device: {device}')\n",
        "print(f'Run name: {run_name}')\n",
        "print(f'Checkpoint tracking: {CHECKPOINT_CSV}\\n')\n",
        "\n",
        "# Execute training\n",
        "results = model.train(**train_params)\n",
        "\n",
        "print(f'\\n{\"=\"*80}')\n",
        "print(f'TRAINING COMPLETED')\n",
        "print(f'{\"=\"*80}\\n')\n",
        "print(f'End time: {datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
        "print(f'Results directory: {results.save_dir}')\n",
        "print(f'Checkpoint CSV: {CHECKPOINT_CSV}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9dbebfb",
      "metadata": {
        "id": "b9dbebfb"
      },
      "source": [
        "## Export Best Model to Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "507bd7ec",
      "metadata": {
        "id": "507bd7ec"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Find best model from training run\n",
        "best_model = Path(results.save_dir) / 'weights' / 'best.pt'\n",
        "last_model = Path(results.save_dir) / 'weights' / 'last.pt'\n",
        "\n",
        "# Export to Drive\n",
        "export_dir = f'{DRIVE_ROOT}/refined_models'\n",
        "os.makedirs(export_dir, exist_ok=True)\n",
        "\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "if best_model.exists():\n",
        "    export_best = f'{export_dir}/refined_best_{timestamp}.pt'\n",
        "    shutil.copy2(best_model, export_best)\n",
        "    print(f'Best model exported to: {export_best}')\n",
        "\n",
        "if last_model.exists():\n",
        "    export_last = f'{export_dir}/refined_last_{timestamp}.pt'\n",
        "    shutil.copy2(last_model, export_last)\n",
        "    print(f'Last model exported to: {export_last}')\n",
        "\n",
        "# Copy results CSV\n",
        "results_csv = Path(results.save_dir) / 'results.csv'\n",
        "if results_csv.exists():\n",
        "    export_results = f'{export_dir}/refined_results_{timestamp}.csv'\n",
        "    shutil.copy2(results_csv, export_results)\n",
        "    print(f'Results CSV exported to: {export_results}')\n",
        "\n",
        "print(f'\\nAll files exported to Google Drive')\n",
        "print(f'  Location: {export_dir}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ebcce99",
      "metadata": {
        "id": "6ebcce99"
      },
      "source": [
        "## Performance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "208e10e5",
      "metadata": {
        "id": "208e10e5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load results\n",
        "results_csv_path = Path(results.save_dir) / 'results.csv'\n",
        "\n",
        "if results_csv_path.exists():\n",
        "    df = pd.read_csv(results_csv_path)\n",
        "    df.columns = df.columns.str.strip()\n",
        "\n",
        "    print('Refined Training Results Summary')\n",
        "    print('=' * 80)\n",
        "    print(f'Total epochs: {len(df)}')\n",
        "    print()\n",
        "\n",
        "    # Final metrics\n",
        "    print('Final Epoch Metrics:')\n",
        "    print('-' * 80)\n",
        "    print(f'Classification Loss (val): {df[\"val/cls_loss\"].iloc[-1]:.4f}')\n",
        "    print(f'Segmentation mAP@50-95:    {df[\"metrics/mAP50-95(M)\"].iloc[-1]:.4f}')\n",
        "    print(f'Segmentation Precision:     {df[\"metrics/precision(M)\"].iloc[-1]:.4f}')\n",
        "    print(f'Segmentation Recall:        {df[\"metrics/recall(M)\"].iloc[-1]:.4f}')\n",
        "    print()\n",
        "\n",
        "    # Best metrics\n",
        "    best_cls_loss_idx = df['val/cls_loss'].idxmin()\n",
        "    best_map_idx = df['metrics/mAP50-95(M)'].idxmax()\n",
        "\n",
        "    print('Best Performance:')\n",
        "    print('-' * 80)\n",
        "    print(f'Best Classification Loss:   {df[\"val/cls_loss\"].iloc[best_cls_loss_idx]:.4f} (epoch {df[\"epoch\"].iloc[best_cls_loss_idx]:.0f})')\n",
        "    print(f'Best Segmentation mAP:      {df[\"metrics/mAP50-95(M)\"].iloc[best_map_idx]:.4f} (epoch {df[\"epoch\"].iloc[best_map_idx]:.0f})')\n",
        "    print()\n",
        "\n",
        "    # Improvement over baseline\n",
        "    baseline_cls_loss = 0.4321  # From epoch 68 of original training\n",
        "    baseline_map = 0.4799\n",
        "\n",
        "    final_cls_loss = df['val/cls_loss'].iloc[-1]\n",
        "    final_map = df['metrics/mAP50-95(M)'].iloc[-1]\n",
        "\n",
        "    cls_improvement = ((baseline_cls_loss - final_cls_loss) / baseline_cls_loss) * 100\n",
        "    map_improvement = ((final_map - baseline_map) / baseline_map) * 100\n",
        "\n",
        "    print('Improvement Over Baseline (Epoch 68):')\n",
        "    print('-' * 80)\n",
        "    print(f'Classification Loss: {baseline_cls_loss:.4f} â†’ {final_cls_loss:.4f} ({cls_improvement:+.2f}%)')\n",
        "    print(f'Segmentation mAP:    {baseline_map:.4f} â†’ {final_map:.4f} ({map_improvement:+.2f}%)')\n",
        "    print()\n",
        "\n",
        "    # Plot training curves\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    fig.suptitle('Refined Training Performance', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Plot 1: Classification Loss\n",
        "    ax = axes[0, 0]\n",
        "    ax.plot(df['epoch'], df['train/cls_loss'], label='Train', linewidth=2, alpha=0.7)\n",
        "    ax.plot(df['epoch'], df['val/cls_loss'], label='Validation', linewidth=2)\n",
        "    ax.axhline(y=baseline_cls_loss, color='red', linestyle='--', label=f'Baseline ({baseline_cls_loss:.4f})', alpha=0.5)\n",
        "    ax.axvline(x=30, color='gray', linestyle=':', alpha=0.5, label='Phase 2')\n",
        "    ax.axvline(x=60, color='gray', linestyle=':', alpha=0.5, label='Phase 3')\n",
        "    ax.set_xlabel('Epoch', fontweight='bold')\n",
        "    ax.set_ylabel('Loss', fontweight='bold')\n",
        "    ax.set_title('Classification Loss (Lower is Better)', fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.legend()\n",
        "\n",
        "    # Plot 2: Segmentation mAP\n",
        "    ax = axes[0, 1]\n",
        "    ax.plot(df['epoch'], df['metrics/mAP50-95(M)'], linewidth=2, color='green')\n",
        "    ax.axhline(y=baseline_map, color='red', linestyle='--', label=f'Baseline ({baseline_map:.4f})', alpha=0.5)\n",
        "    ax.axvline(x=30, color='gray', linestyle=':', alpha=0.5)\n",
        "    ax.axvline(x=60, color='gray', linestyle=':', alpha=0.5)\n",
        "    ax.set_xlabel('Epoch', fontweight='bold')\n",
        "    ax.set_ylabel('mAP@50-95', fontweight='bold')\n",
        "    ax.set_title('Segmentation Quality (Higher is Better)', fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.legend()\n",
        "    ax.set_ylim([0.4, 1.0])\n",
        "\n",
        "    # Plot 3: Precision & Recall\n",
        "    ax = axes[1, 0]\n",
        "    ax.plot(df['epoch'], df['metrics/precision(M)'], label='Precision', linewidth=2)\n",
        "    ax.plot(df['epoch'], df['metrics/recall(M)'], label='Recall', linewidth=2)\n",
        "    ax.axvline(x=30, color='gray', linestyle=':', alpha=0.5)\n",
        "    ax.axvline(x=60, color='gray', linestyle=':', alpha=0.5)\n",
        "    ax.set_xlabel('Epoch', fontweight='bold')\n",
        "    ax.set_ylabel('Score', fontweight='bold')\n",
        "    ax.set_title('Precision & Recall', fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.legend()\n",
        "    ax.set_ylim([0.7, 1.0])\n",
        "\n",
        "    # Plot 4: Learning Rate\n",
        "    ax = axes[1, 1]\n",
        "    ax.plot(df['epoch'], df['lr/pg0'], linewidth=2, color='purple')\n",
        "    ax.axvline(x=30, color='gray', linestyle=':', alpha=0.5, label='Phase transitions')\n",
        "    ax.axvline(x=60, color='gray', linestyle=':', alpha=0.5)\n",
        "    ax.set_xlabel('Epoch', fontweight='bold')\n",
        "    ax.set_ylabel('Learning Rate', fontweight='bold')\n",
        "    ax.set_title('Learning Rate Schedule', fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.legend()\n",
        "    ax.set_yscale('log')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save plot\n",
        "    plot_path = Path(results.save_dir) / 'refined_training_analysis.png'\n",
        "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
        "    print(f'Training curves saved to: {plot_path}')\n",
        "\n",
        "    # Also save to Drive\n",
        "    drive_plot_path = f'{export_dir}/refined_training_analysis_{timestamp}.png'\n",
        "    shutil.copy2(plot_path, drive_plot_path)\n",
        "    print(f'Plots exported to Drive: {drive_plot_path}')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print('Results CSV not found. Training may not have completed.')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}