{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a7e14ac",
   "metadata": {},
   "source": [
    "## Setup: Google Colab Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02620f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using CPU (training will be slow)\n",
      "\n",
      "PyTorch version: 2.9.1+cpu\n",
      "CUDA version: None\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Verify CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print(f'GPU available: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('No GPU available, using CPU (training will be slow)')\n",
    "\n",
    "print(f'\\nPyTorch version: {torch.__version__}')\n",
    "print(f'CUDA version: {torch.version.cuda}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b038eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All packages installed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orch (c:\\Users\\lifei\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001E057591550>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/opencv-python-headless/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001E057592190>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/opencv-python-headless/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001E057592DD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/opencv-python-headless/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001E057593A10>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/opencv-python-headless/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001E05759C5D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/opencv-python-headless/\n",
      "ERROR: Could not find a version that satisfies the requirement opencv-python-headless (from versions: none)\n",
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: No matching distribution found for opencv-python-headless\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q ultralytics opencv-python-headless pillow pyyaml numpy scipy matplotlib pandas\n",
    "\n",
    "print('All packages installed successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b48dbb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Mount Google Drive (if using dataset from Drive)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[32m      3\u001b[39m drive.mount(\u001b[33m'\u001b[39m\u001b[33m/content/drive\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Set paths - MODIFY THESE TO YOUR ACTUAL PATHS\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive (if using dataset from Drive)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set paths - MODIFY THESE TO YOUR ACTUAL PATHS\n",
    "DRIVE_ROOT = '/content/drive/MyDrive/csc173_dataset'\n",
    "DATA_YAML_PATH = f'{DRIVE_ROOT}/dataset/data.yaml'\n",
    "PRETRAINED_MODEL = f'{DRIVE_ROOT}/models/custom_ocr_last.pt'  # Your existing checkpoint\n",
    "\n",
    "# Create local working directory\n",
    "WORK_DIR = '/content/refined_training'\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "os.chdir(WORK_DIR)\n",
    "\n",
    "print(f'Drive mounted and working directory set')\n",
    "print(f'Data config: {DATA_YAML_PATH}')\n",
    "print(f'Pretrained model: {PRETRAINED_MODEL}')\n",
    "print(f'Working directory: {WORK_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6201797b",
   "metadata": {},
   "source": [
    "## Copy Dataset to Local Storage (Fixes Slow I/O)\n",
    "\n",
    "Google Drive mounting is extremely slow for datasets with many small files. We'll copy the dataset to Colab's local SSD once per session for fast training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cd3d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "print('Extracting dataset from zip to local SSD...')\n",
    "print('This is a ONE-TIME operation per Colab session (much faster than copying many files)')\n",
    "print('=' * 80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Local paths (on Colab's SSD - FAST!)\n",
    "LOCAL_DATASET_ROOT = '/content/local_dataset'\n",
    "LOCAL_DATA_YAML = f'{LOCAL_DATASET_ROOT}/data.yaml'\n",
    "\n",
    "# Path to the zip file on Google Drive\n",
    "DATASET_ZIP = f'{DRIVE_ROOT}/dataset.zip'\n",
    "\n",
    "# Check if already extracted\n",
    "if Path(LOCAL_DATASET_ROOT).exists() and Path(LOCAL_DATA_YAML).exists():\n",
    "    print('Dataset already extracted to local storage')\n",
    "else:\n",
    "    # Extract dataset from zip file\n",
    "    print(f'Source: {DATASET_ZIP}')\n",
    "    print(f'Destination: {LOCAL_DATASET_ROOT}')\n",
    "    print()\n",
    "    \n",
    "    if not Path(DATASET_ZIP).exists():\n",
    "        print(f'   ERROR: dataset.zip not found at {DATASET_ZIP}')\n",
    "        print(f'   Please ensure dataset.zip is uploaded to {DRIVE_ROOT}/')\n",
    "        raise FileNotFoundError(f'dataset.zip not found at {DATASET_ZIP}')\n",
    "    \n",
    "    print('Extracting zip file (this is MUCH faster than copying individual files)...')\n",
    "    \n",
    "    # Create parent directory\n",
    "    Path(LOCAL_DATASET_ROOT).parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Extract the zip file\n",
    "    with zipfile.ZipFile(DATASET_ZIP, 'r') as zip_ref:\n",
    "        zip_ref.extractall('/content')\n",
    "    \n",
    "    # Check if extraction created a nested 'dataset' folder\n",
    "    if Path('/content/dataset').exists():\n",
    "        # Move contents from /content/dataset to LOCAL_DATASET_ROOT\n",
    "        shutil.move('/content/dataset', LOCAL_DATASET_ROOT)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f'\\nDataset extracted in {elapsed/60:.1f} minutes')\n",
    "    \n",
    "    # Fix data.yaml paths (it may contain Windows paths from local machine)\n",
    "    print('\\nFixing data.yaml paths for Colab...')\n",
    "    import yaml\n",
    "    \n",
    "    if Path(LOCAL_DATA_YAML).exists():\n",
    "        with open(LOCAL_DATA_YAML, 'r') as f:\n",
    "            data_config = yaml.safe_load(f)\n",
    "        \n",
    "        # Update paths to use local dataset root\n",
    "        data_config['path'] = LOCAL_DATASET_ROOT\n",
    "        data_config['train'] = 'train/images'\n",
    "        data_config['val'] = 'val/images'\n",
    "        data_config['test'] = 'test/images'\n",
    "        \n",
    "        # Write back\n",
    "        with open(LOCAL_DATA_YAML, 'w') as f:\n",
    "            yaml.dump(data_config, f, default_flow_style=False)\n",
    "        \n",
    "        print(f'  Updated data.yaml with correct paths')\n",
    "        print(f'  Base path: {LOCAL_DATASET_ROOT}')\n",
    "    else:\n",
    "        print(f'  Warning: data.yaml not found at {LOCAL_DATA_YAML}')\n",
    "\n",
    "# Update paths to use LOCAL storage instead of Drive\n",
    "DATA_YAML_PATH = LOCAL_DATA_YAML\n",
    "\n",
    "print()\n",
    "print('Updated paths:')\n",
    "print(f'  Dataset: {LOCAL_DATASET_ROOT}')\n",
    "print(f'  data.yaml: {DATA_YAML_PATH}')\n",
    "print(f'  Model: {PRETRAINED_MODEL}')\n",
    "print()\n",
    "print('Training will now use LOCAL SSD')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb3737f",
   "metadata": {},
   "source": [
    "## Core Components (Reused from Original Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7000dfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 36\n",
      "Characters: ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\n",
      "Similarity matrix initialized: torch.Size([36, 36])\n"
     ]
    }
   ],
   "source": [
    "# Character set and similarity matrix (from original)\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "CHARS = [chr(i) for i in range(65, 91)] + [str(i) for i in range(10)]\n",
    "NUM_CLASSES = len(CHARS)\n",
    "CHAR_TO_IDX = {c: i for i, c in enumerate(CHARS)}\n",
    "IDX_TO_CHAR = {i: c for i, c in enumerate(CHARS)}\n",
    "\n",
    "print(f'Number of classes: {NUM_CLASSES}')\n",
    "print(f'Characters: {\"\".join(CHARS)}')\n",
    "\n",
    "SIMILAR_GROUPS = [\n",
    "    ['O', '0'], # I want to refine Q and 0/O differentiation, so it was removed from the group\n",
    "    ['I', '1'], # I want to refine L and 1/I differentiation, so it was removed from the group\n",
    "    ['S', '5'],\n",
    "    ['Z', '2'],\n",
    "    ['B', '8'],\n",
    "    ['D', '0'],\n",
    "    ['G', 'C'],\n",
    "    ['U', 'V'],\n",
    "    ['P', 'R'],\n",
    "]\n",
    "\n",
    "def create_similarity_matrix(num_classes=NUM_CLASSES, groups=SIMILAR_GROUPS, base_sim=0.6):\n",
    "    S = np.zeros((num_classes, num_classes), dtype=np.float32)\n",
    "    np.fill_diagonal(S, 1.0)\n",
    "    for group in groups:\n",
    "        idxs = [CHAR_TO_IDX[c] for c in group if c in CHAR_TO_IDX]\n",
    "        for i in idxs:\n",
    "            for j in idxs:\n",
    "                if i != j:\n",
    "                    S[i, j] = base_sim\n",
    "    return torch.tensor(S, dtype=torch.float32)\n",
    "\n",
    "similarity_matrix = create_similarity_matrix()\n",
    "print(f'Similarity matrix initialized: {similarity_matrix.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ebec263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refined similarity-aware loss defined\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Similarity-Aware Loss with Adaptive Weighting\n",
    "class RefinedSimilarityAwareTopKLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Enhanced loss for fine-tuning with:\n",
    "    - Higher penalty for similar character confusion\n",
    "    - Adaptive temperature based on training phase\n",
    "    - Confidence-based weighting\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=NUM_CLASSES, similarity_matrix=None,\n",
    "                 k=3, initial_temperature=0.5, base_weight=0.5, topk_weight=0.5,\n",
    "                 epochs=40):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.k = k\n",
    "        self.initial_temperature = initial_temperature\n",
    "        self.base_weight = base_weight\n",
    "        self.topk_weight = topk_weight\n",
    "        self.epochs = epochs\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "        if similarity_matrix is not None:\n",
    "            self.register_buffer('similarity_matrix', similarity_matrix)\n",
    "        else:\n",
    "            self.register_buffer('similarity_matrix', create_similarity_matrix())\n",
    "\n",
    "    def update_epoch(self, epoch):\n",
    "        \"\"\"Update current epoch for temperature annealing.\"\"\"\n",
    "        self.current_epoch = epoch\n",
    "    \n",
    "    def get_temperature(self):\n",
    "        \"\"\"Anneal temperature more aggressively for fine-tuning.\"\"\"\n",
    "        progress = self.current_epoch / max(self.epochs, 1)\n",
    "        # Start at 0.5, go to 0.3 (sharper predictions)\n",
    "        return max(0.3, self.initial_temperature - progress * 0.2)\n",
    "    \n",
    "    def forward(self, logits, targets):\n",
    "        B = logits.size(0)\n",
    "        device = logits.device\n",
    "        \n",
    "        temperature = self.get_temperature()\n",
    "        \n",
    "        # Standard cross-entropy\n",
    "        ce_loss = F.cross_entropy(logits, targets, reduction='none')\n",
    "        \n",
    "        # Softmax with temperature\n",
    "        probs = F.softmax(logits / temperature, dim=1)\n",
    "        topk_probs, topk_indices = torch.topk(probs, min(self.k, self.num_classes), dim=1)\n",
    "        \n",
    "        # Similarity-aware penalty\n",
    "        sim_loss = torch.zeros(B, device=device)\n",
    "        confidence_scores = []\n",
    "        \n",
    "        for i in range(B):\n",
    "            t = targets[i].item()\n",
    "            if t < 0 or t >= self.num_classes:\n",
    "                continue\n",
    "                \n",
    "            sims = self.similarity_matrix[t][topk_indices[i]]\n",
    "            \n",
    "            # Higher penalty for similar character confusion\n",
    "            # If model confuses O with 0 (high similarity), penalty is lower\n",
    "            # If model confuses O with X (low similarity), penalty is higher\n",
    "            penalties = (1.0 - sims) * 1.5  # Amplify penalty\n",
    "            weighted_penalties = topk_probs[i] * penalties\n",
    "            sim_loss[i] = weighted_penalties.sum()\n",
    "            \n",
    "            confidence_scores.append(topk_probs[i][0].item())\n",
    "        \n",
    "        if len(confidence_scores) == 0:\n",
    "            return ce_loss.mean()\n",
    "        \n",
    "        # Adaptive weighting based on confidence\n",
    "        confidence = torch.tensor(confidence_scores, device=device)\n",
    "        \n",
    "        # When confident: rely more on CE (trust the model)\n",
    "        # When uncertain: rely more on similarity (guide the model)\n",
    "        adaptive_base = self.base_weight + (1 - confidence) * 0.2\n",
    "        adaptive_topk = self.topk_weight + confidence * 0.2\n",
    "        \n",
    "        # Normalize\n",
    "        total_weight = adaptive_base + adaptive_topk\n",
    "        adaptive_base = adaptive_base / total_weight\n",
    "        adaptive_topk = adaptive_topk / total_weight\n",
    "        \n",
    "        total_loss = adaptive_base * ce_loss + adaptive_topk * sim_loss\n",
    "        return total_loss.mean()\n",
    "\n",
    "print('Refined similarity-aware loss defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4557780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OCR Metrics (reused from original)\n",
    "class OCRMetrics:\n",
    "    \"\"\"Compute OCR-specific validation metrics.\"\"\"\n",
    "    def __init__(self, similarity_matrix=None):\n",
    "        self.similarity_matrix = similarity_matrix if similarity_matrix is not None else create_similarity_matrix()\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.total_chars = 0\n",
    "        self.correct_chars = 0\n",
    "        self.top2_correct = 0\n",
    "        self.top3_correct = 0\n",
    "        self.similarity_score = 0.0\n",
    "    \n",
    "    def update(self, predictions, targets, top_k_preds=None):\n",
    "        predictions = predictions.cpu().numpy()\n",
    "        targets = targets.cpu().numpy()\n",
    "        \n",
    "        self.total_chars += len(targets)\n",
    "        self.correct_chars += (predictions == targets).sum()\n",
    "        \n",
    "        # Similarity-aware accuracy\n",
    "        for pred, target in zip(predictions, targets):\n",
    "            if 0 <= target < len(self.similarity_matrix) and 0 <= pred < len(self.similarity_matrix):\n",
    "                sim = self.similarity_matrix[target][pred].item()\n",
    "                self.similarity_score += sim\n",
    "        \n",
    "        # Top-k accuracy\n",
    "        if top_k_preds is not None:\n",
    "            top_k_preds = top_k_preds.cpu().numpy()\n",
    "            for i, target in enumerate(targets):\n",
    "                if top_k_preds.shape[1] >= 2 and target in top_k_preds[i, :2]:\n",
    "                    self.top2_correct += 1\n",
    "                if top_k_preds.shape[1] >= 3 and target in top_k_preds[i, :3]:\n",
    "                    self.top3_correct += 1\n",
    "    \n",
    "    def compute(self):\n",
    "        if self.total_chars == 0:\n",
    "            return {}\n",
    "        \n",
    "        return {\n",
    "            'CER': 1.0 - (self.correct_chars / self.total_chars),\n",
    "            'char_accuracy': self.correct_chars / self.total_chars,\n",
    "            'top2_accuracy': self.top2_correct / self.total_chars,\n",
    "            'top3_accuracy': self.top3_correct / self.total_chars,\n",
    "            'similarity_aware_accuracy': self.similarity_score / self.total_chars,\n",
    "        }\n",
    "\n",
    "print('OCR metrics module loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e25dad",
   "metadata": {},
   "source": [
    "## Refined Training Strategy\n",
    "\n",
    "### Phase 1: Classifier Head Fine-Tuning (Epochs 1-12)\n",
    "- Freeze backbone and segmentation head\n",
    "- Focus exclusively on improving character classification\n",
    "- Use cyclic learning rate to escape plateau\n",
    "\n",
    "### Phase 2: Progressive Unfreezing (Epochs 13-24)\n",
    "- Gradually unfreeze deeper layers\n",
    "- Lower learning rate for stable refinement\n",
    "- Continue with similarity-aware loss\n",
    "\n",
    "### Phase 3: Full Fine-Tuning (Epochs 25-40)\n",
    "- All layers unfrozen\n",
    "- Very low learning rate for final polish\n",
    "- Focus on reducing classification loss below 0.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4287274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Trainer for Refined Training\n",
    "from ultralytics.models.yolo.segment import SegmentationTrainer\n",
    "from ultralytics import YOLO\n",
    "\n",
    "class RefinedSegmentationTrainer(SegmentationTrainer):\n",
    "    \"\"\"\n",
    "    Refined trainer with:\n",
    "    - Progressive layer unfreezing\n",
    "    - Enhanced loss function\n",
    "    - OCR-specific metrics tracking\n",
    "    - Cyclic learning rate support\n",
    "    - Checkpoint saving after each epoch\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg=None, overrides=None, _callbacks=None):\n",
    "        super().__init__(cfg, overrides, _callbacks)\n",
    "        \n",
    "        # Get total epochs from config\n",
    "        total_epochs = self.args.epochs if hasattr(self.args, 'epochs') else 40\n",
    "        \n",
    "        # Initialize refined loss\n",
    "        self.character_loss_fn = RefinedSimilarityAwareTopKLoss(\n",
    "            num_classes=NUM_CLASSES,\n",
    "            similarity_matrix=similarity_matrix,\n",
    "            k=3,\n",
    "            initial_temperature=0.5,\n",
    "            base_weight=0.5,\n",
    "            topk_weight=0.5,\n",
    "            epochs=total_epochs\n",
    "        ).to(device)\n",
    "        \n",
    "        # OCR metrics\n",
    "        self.ocr_metrics = OCRMetrics(similarity_matrix=similarity_matrix)\n",
    "        \n",
    "        # Training phase tracking\n",
    "        self.phase = 1\n",
    "        self.freeze_applied = False\n",
    "    \n",
    "    def _setup_train(self, world_size):\n",
    "        \"\"\"Override to apply layer freezing for Phase 1.\"\"\"\n",
    "        super()._setup_train(world_size)\n",
    "        \n",
    "        if not self.freeze_applied and self.epoch < 12:\n",
    "            print(f'\\n=== PHASE 1: Classifier Head Fine-Tuning (Epochs 1-12) ===')\n",
    "            print('Freezing backbone and segmentation layers...')\n",
    "            \n",
    "            # Freeze all layers except classification head\n",
    "            for name, param in self.model.named_parameters():\n",
    "                # Keep classification layers trainable\n",
    "                if 'cls' in name.lower() or 'cv3' in name.lower():\n",
    "                    param.requires_grad = True\n",
    "                else:\n",
    "                    param.requires_grad = False\n",
    "            \n",
    "            trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "            total = sum(p.numel() for p in self.model.parameters())\n",
    "            print(f'Trainable parameters: {trainable:,} / {total:,} ({100*trainable/total:.1f}%)')\n",
    "            self.freeze_applied = True\n",
    "    \n",
    "    def on_train_epoch_start(self):\n",
    "        \"\"\"Handle phase transitions and progressive unfreezing.\"\"\"\n",
    "        super().on_train_epoch_start()\n",
    "        \n",
    "        # Update temperature in loss\n",
    "        self.character_loss_fn.update_epoch(self.epoch)\n",
    "        \n",
    "        # Phase 2: Progressive unfreezing (epochs 12-24)\n",
    "        if self.epoch == 12:\n",
    "            self.phase = 2\n",
    "            print(f'\\n=== PHASE 2: Progressive Unfreezing (Epochs 13-24) ===')\n",
    "            print('Unfreezing segmentation head...')\n",
    "            \n",
    "            for name, param in self.model.named_parameters():\n",
    "                if 'seg' in name.lower() or 'mask' in name.lower():\n",
    "                    param.requires_grad = True\n",
    "            \n",
    "            trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "            total = sum(p.numel() for p in self.model.parameters())\n",
    "            print(f'Trainable parameters: {trainable:,} / {total:,} ({100*trainable/total:.1f}%)')\n",
    "        \n",
    "        # Phase 3: Full fine-tuning (epochs 24+)\n",
    "        elif self.epoch == 24:\n",
    "            self.phase = 3\n",
    "            print(f'\\n=== PHASE 3: Full Fine-Tuning (Epochs 25-40) ===')\n",
    "            print('Unfreezing all layers...')\n",
    "            \n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = True\n",
    "            \n",
    "            trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "            total = sum(p.numel() for p in self.model.parameters())\n",
    "            print(f'Trainable parameters: {trainable:,} / {total:,} ({100*trainable/total:.1f}%)')\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        \"\"\"Save checkpoint after each epoch.\"\"\"\n",
    "        super().on_train_epoch_end()\n",
    "        \n",
    "        # Extract metrics from validator\n",
    "        if hasattr(self, 'metrics') and self.metrics is not None:\n",
    "            metrics_dict = {}\n",
    "            \n",
    "            # Try to get metrics from the metrics object\n",
    "            if hasattr(self.metrics, 'results_dict'):\n",
    "                metrics_dict = self.metrics.results_dict\n",
    "            \n",
    "            # Also get from validator\n",
    "            if hasattr(self, 'validator') and hasattr(self.validator, 'metrics'):\n",
    "                val_metrics = self.validator.metrics\n",
    "                if hasattr(val_metrics, 'results_dict'):\n",
    "                    metrics_dict.update(val_metrics.results_dict)\n",
    "            \n",
    "            # Get loss values\n",
    "            if hasattr(self, 'loss_items'):\n",
    "                loss_items = self.loss_items\n",
    "                if loss_items is not None and len(loss_items) > 0:\n",
    "                    metrics_dict['train/cls_loss'] = float(loss_items[0]) if len(loss_items) > 0 else 0.0\n",
    "                    metrics_dict['train/seg_loss'] = float(loss_items[1]) if len(loss_items) > 1 else 0.0\n",
    "            \n",
    "            # Get learning rate\n",
    "            if hasattr(self, 'optimizer'):\n",
    "                metrics_dict['lr/pg0'] = self.optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            # Save checkpoint with smart model saving\n",
    "            last_model = Path(self.save_dir) / 'weights' / 'last.pt'\n",
    "            best_model = Path(self.save_dir) / 'weights' / 'best.pt'\n",
    "            save_checkpoint(self.epoch, metrics_dict, str(last_model), str(best_model), force_save_interval=10)\n",
    "    \n",
    "    def on_val_start(self):\n",
    "        super().on_val_start()\n",
    "        self.ocr_metrics.reset()\n",
    "    \n",
    "    def on_val_end(self):\n",
    "        super().on_val_end()\n",
    "        \n",
    "        # Log OCR metrics\n",
    "        ocr_results = self.ocr_metrics.compute()\n",
    "        if ocr_results:\n",
    "            print(f'\\n[Epoch {self.epoch}] OCR Metrics:')\n",
    "            for key, value in ocr_results.items():\n",
    "                print(f'  {key}: {value:.4f}')\n",
    "    \n",
    "    def compute_loss(self, preds, batch):\n",
    "        \"\"\"Compute loss with refined similarity-aware classification.\"\"\"\n",
    "        # Get base YOLO losses\n",
    "        base_loss = super().compute_loss(preds, batch)\n",
    "        \n",
    "        # Add custom similarity-aware character classification loss\n",
    "        if len(preds) > 3:\n",
    "            cls_logits = preds[3]\n",
    "            cls_targets = batch['cls'].long()\n",
    "            \n",
    "            if cls_logits is not None and cls_targets is not None:\n",
    "                cls_logits_flat = cls_logits.view(-1, NUM_CLASSES)\n",
    "                cls_targets_flat = cls_targets.view(-1)\n",
    "                \n",
    "                valid_mask = cls_targets_flat >= 0\n",
    "                if valid_mask.sum() > 0:\n",
    "                    # Compute refined similarity-aware loss\n",
    "                    char_loss = self.character_loss_fn(\n",
    "                        cls_logits_flat[valid_mask],\n",
    "                        cls_targets_flat[valid_mask]\n",
    "                    )\n",
    "                    \n",
    "                    # Update OCR metrics\n",
    "                    with torch.no_grad():\n",
    "                        preds_cls = cls_logits_flat[valid_mask].argmax(dim=1)\n",
    "                        top_k_preds = torch.topk(cls_logits_flat[valid_mask], k=3, dim=1)[1]\n",
    "                        self.ocr_metrics.update(\n",
    "                            preds_cls,\n",
    "                            cls_targets_flat[valid_mask],\n",
    "                            top_k_preds\n",
    "                        )\n",
    "                    \n",
    "                    # Phase-dependent weighting\n",
    "                    if self.phase == 1:\n",
    "                        # Phase 1: Heavy emphasis on classification\n",
    "                        cls_weight = 0.7\n",
    "                    elif self.phase == 2:\n",
    "                        # Phase 2: Balanced\n",
    "                        cls_weight = 0.5\n",
    "                    else:\n",
    "                        # Phase 3: Standard weighting\n",
    "                        cls_weight = 0.3\n",
    "                    \n",
    "                    total_loss = (1 - cls_weight) * base_loss + cls_weight * char_loss\n",
    "                    return total_loss\n",
    "        \n",
    "        return base_loss\n",
    "\n",
    "print('Refined segmentation trainer defined with checkpoint support')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b00e62a",
   "metadata": {},
   "source": [
    "## Load Pretrained Model and Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b8d469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model (either from checkpoint or pretrained)\n",
    "\n",
    "# Check if checkpoint management cell was run\n",
    "if 'RESUME_FROM' not in locals() and 'RESUME_FROM' not in globals():\n",
    "    print('    ERROR: Please run the \"Checkpoint Management\" cell first!')\n",
    "    print('   (The cell that defines RESUME_FROM and checkpoint tracking)')\n",
    "    raise RuntimeError('Checkpoint management cell must be run before loading model')\n",
    "\n",
    "print(f'Loading model from: {RESUME_FROM}\\n')\n",
    "\n",
    "model = YOLO(RESUME_FROM)\n",
    "model.trainer = RefinedSegmentationTrainer\n",
    "\n",
    "if RESUME_TRAINING:\n",
    "    print(f'Model loaded for RESUMING training')\n",
    "    print(f'  Will continue from epoch {checkpoint_info[\"last_epoch\"] + 1}')\n",
    "else:\n",
    "    print(f'Model loaded for FRESH training')\n",
    "    print(f'  Starting from pretrained checkpoint')\n",
    "\n",
    "print(f'  Total target epochs: {REFINE_EPOCHS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ae0242",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bada4767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refined training hyperparameters\n",
    "REFINE_EPOCHS = 40\n",
    "BATCH_SIZE = 16\n",
    "IMG_SIZE = 224\n",
    "\n",
    "# Cyclic learning rate for Phase 1 (escaping plateau)\n",
    "# Start higher to shake the model out of local minimum\n",
    "LR0 = 0.005  # Higher than previous 0.001\n",
    "LRF = 0.0001  # End lower for fine control\n",
    "\n",
    "# Optimizer settings\n",
    "MOMENTUM = 0.937\n",
    "WEIGHT_DECAY = 5e-4\n",
    "WARMUP_EPOCHS = 3.0\n",
    "\n",
    "# Augmentations - more aggressive for character robustness\n",
    "AUG_HSV_H = 0.02  # Increased hue variation\n",
    "AUG_HSV_S = 0.8   # Increased saturation variation\n",
    "AUG_HSV_V = 0.5   # Increased brightness variation\n",
    "AUG_ERASING = 0.5  # Increased random erasing\n",
    "AUG_DEGREES = 5.0  # Small rotation for character variation\n",
    "AUG_SHEAR = 2.0    # Perspective variation\n",
    "\n",
    "# Disabled augmentations (not useful for OCR)\n",
    "AUG_FLIPLR = 0.0\n",
    "AUG_MOSAIC = 0.0\n",
    "AUG_MIXUP = 0.0\n",
    "\n",
    "print('Refined Training Configuration:')\n",
    "print(f'  Epochs: {REFINE_EPOCHS}')\n",
    "print(f'  Batch size: {BATCH_SIZE}')\n",
    "print(f'  Learning rate: {LR0} â†’ {LRF}')\n",
    "print(f'  Augmentations: Enhanced HSV + Erasing + Geometric')\n",
    "print(f'\\nTraining Strategy:')\n",
    "print(f'  Phase 1 (1-12): Classifier head only')\n",
    "print(f'  Phase 2 (13-24): + Segmentation head')\n",
    "print(f'  Phase 3 (25-40): All layers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529f90cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "CHECKPOINT_DIR = f'{DRIVE_ROOT}/refined_checkpoints'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_CSV = f'{CHECKPOINT_DIR}/training_progress.csv'\n",
    "CHECKPOINT_MODEL = f'{CHECKPOINT_DIR}/checkpoint_latest.pt'\n",
    "\n",
    "def load_checkpoint_info():\n",
    "    \"\"\"Load checkpoint information if it exists.\"\"\"\n",
    "    if os.path.exists(CHECKPOINT_CSV):\n",
    "        df = pd.read_csv(CHECKPOINT_CSV)\n",
    "        if len(df) > 0:\n",
    "            last_row = df.iloc[-1]\n",
    "            return {\n",
    "                'exists': True,\n",
    "                'last_epoch': int(last_row['epoch']),\n",
    "                'total_epochs_trained': len(df),\n",
    "                'best_cls_loss': df['val/cls_loss'].min() if 'val/cls_loss' in df.columns else float('inf'),\n",
    "                'best_map': df['metrics/mAP50-95(M)'].max() if 'metrics/mAP50-95(M)' in df.columns else 0.0,\n",
    "                'checkpoint_path': CHECKPOINT_MODEL if os.path.exists(CHECKPOINT_MODEL) else None\n",
    "            }\n",
    "    return {'exists': False, 'best_cls_loss': float('inf'), 'best_map': 0.0}\n",
    "\n",
    "def initialize_checkpoint_csv():\n",
    "    \"\"\"Initialize the checkpoint CSV with headers.\"\"\"\n",
    "    if not os.path.exists(CHECKPOINT_CSV):\n",
    "        # Create empty CSV with expected columns\n",
    "        columns = ['epoch', 'train/cls_loss', 'val/cls_loss', 'train/seg_loss', 'val/seg_loss',\n",
    "                   'metrics/precision(M)', 'metrics/recall(M)', 'metrics/mAP50(M)', \n",
    "                   'metrics/mAP50-95(M)', 'lr/pg0', 'timestamp']\n",
    "        pd.DataFrame(columns=columns).to_csv(CHECKPOINT_CSV, index=False)\n",
    "        print(f'Created new checkpoint CSV: {CHECKPOINT_CSV}')\n",
    "        return {'exists': False, 'best_cls_loss': float('inf'), 'best_map': 0.0}\n",
    "    else:\n",
    "        print(f'Found existing checkpoint CSV: {CHECKPOINT_CSV}')\n",
    "        return load_checkpoint_info()\n",
    "\n",
    "def save_checkpoint(epoch, metrics, model_path, best_model_path=None, force_save_interval=10):\n",
    "    \"\"\"Save checkpoint after each epoch with smart model saving.\n",
    "    \n",
    "    Args:\n",
    "        epoch: Current epoch number\n",
    "        metrics: Dictionary of metrics to save\n",
    "        model_path: Path to the last.pt model file\n",
    "        best_model_path: Path to the best.pt model file (if available)\n",
    "        force_save_interval: Force save model every N epochs as backup (default: 10)\n",
    "    \"\"\"\n",
    "    import datetime\n",
    "    import threading\n",
    "    \n",
    "    # ALWAYS save metrics CSV (fast operation) - DO THIS FIRST\n",
    "    df = pd.read_csv(CHECKPOINT_CSV) if os.path.exists(CHECKPOINT_CSV) else pd.DataFrame()\n",
    "    \n",
    "    new_row = {\n",
    "        'epoch': epoch,\n",
    "        'timestamp': datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        **metrics\n",
    "    }\n",
    "    \n",
    "    df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    df.to_csv(CHECKPOINT_CSV, index=False)\n",
    "    print(f'CSV saved (Epoch {epoch})')\n",
    "    \n",
    "    # Determine if this is the best model\n",
    "    is_best = False\n",
    "    current_cls_loss = metrics.get('val/cls_loss', float('inf'))\n",
    "    current_map = metrics.get('metrics/mAP50-95(M)', 0.0)\n",
    "    \n",
    "    # Check if this is the best model based on validation loss\n",
    "    if len(df) > 1:\n",
    "        prev_best_loss = df['val/cls_loss'].iloc[:-1].min() if 'val/cls_loss' in df.columns else float('inf')\n",
    "        if current_cls_loss < prev_best_loss:\n",
    "            is_best = True\n",
    "            print(f'  ðŸ† NEW BEST MODEL! val/cls_loss: {current_cls_loss:.4f} < {prev_best_loss:.4f}')\n",
    "    else:\n",
    "        is_best = True  # First epoch\n",
    "    \n",
    "    # Save model to Drive if:\n",
    "    # 1. It's the best model, OR\n",
    "    # 2. It's a backup interval (every N epochs), OR  \n",
    "    # 3. It's the first or last epoch\n",
    "    should_save = is_best or (epoch % force_save_interval == 0) or (epoch == 1)\n",
    "    \n",
    "    if should_save:\n",
    "        # Prefer best.pt if available, otherwise use last.pt\n",
    "        model_to_save = best_model_path if (best_model_path and os.path.exists(best_model_path)) else model_path\n",
    "        \n",
    "        if os.path.exists(model_to_save):\n",
    "            import shutil\n",
    "            \n",
    "            # Save asynchronously in background thread to not block training\n",
    "print(f'\\nSmart Checkpoint System:')\n",
    "print(f'   CSV saved to Drive EVERY epoch (fast, ~1 second)')\n",
    "print(f'   Best model saved to Drive IMMEDIATELY when improved (async upload)')\n",
    "print(f'   Backup model saved every 10 epochs (in case best model corrupts)')\n",
    "\n",
    "\n",
    "print(f'\\n   If you disconnect, you can resume from the last completed epoch in the CSV')print(f'   All saves are asynchronous - training continues without blocking!')                    print(f'  Model saved to Drive ({reason})')\n",
    "                except Exception as e:\n",
    "                    print(f'  âš  Warning: Failed to save model to Drive: {e}')\n",
    "            \n",
    "            # Start async save\n",
    "            save_thread = threading.Thread(target=async_save, daemon=True)\n",
    "            save_thread.start()\n",
    "            \n",
    "            if is_best:\n",
    "                print(f'  Uploading best model to Drive (async)...')\n",
    "            else:\n",
    "                print(f'  Backup save to Drive (async)...')\n",
    "        else:\n",
    "            print(f'  Model file not found at {model_to_save}')\n",
    "    else:\n",
    "        print(f'  (Model will be backed up to Drive at epoch {((epoch // force_save_interval) + 1) * force_save_interval})')\n",
    "\n",
    "# Check for existing checkpoint\n",
    "checkpoint_info = initialize_checkpoint_csv()\n",
    "\n",
    "if checkpoint_info and checkpoint_info['exists']:\n",
    "    print(f'\\n=== RESUMABLE TRAINING DETECTED ===')\n",
    "    print(f'Previous training found:')\n",
    "    print(f'  Last completed epoch: {checkpoint_info[\"last_epoch\"]}')\n",
    "    print(f'  Total epochs trained: {checkpoint_info[\"total_epochs_trained\"]}')\n",
    "    print(f'  Best classification loss: {checkpoint_info[\"best_cls_loss\"]:.4f}')\n",
    "    print(f'  Best segmentation mAP: {checkpoint_info[\"best_map\"]:.4f}')\n",
    "    \n",
    "    if checkpoint_info['checkpoint_path']:\n",
    "        print(f'  Checkpoint model: {checkpoint_info[\"checkpoint_path\"]}')\n",
    "        RESUME_FROM = checkpoint_info['checkpoint_path']\n",
    "        RESUME_TRAINING = True\n",
    "    else:\n",
    "        print(f'  Checkpoint model not found, will start fresh')\n",
    "        RESUME_FROM = PRETRAINED_MODEL\n",
    "        RESUME_TRAINING = False\n",
    "else:\n",
    "    print(f'\\n=== STARTING FRESH TRAINING ===')\n",
    "    print(f'No previous checkpoint found')\n",
    "    RESUME_FROM = PRETRAINED_MODEL\n",
    "    RESUME_TRAINING = False\n",
    "\n",
    "print(f'\\nCheckpoint directory: {CHECKPOINT_DIR}')\n",
    "print(f'Progress CSV: {CHECKPOINT_CSV}')\n",
    "print(f'\\n Performance Note:')\n",
    "print(f'   CSV is saved every epoch (fast)')\n",
    "print(f'   Model is saved to Drive every 10 epochs (to avoid slow Drive I/O)')\n",
    "print(f'   Local model checkpoints are saved every 5 epochs by YOLO')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976b37c9",
   "metadata": {},
   "source": [
    "## Checkpoint Management\n",
    "\n",
    "This cell manages training checkpoints so you can resume training across Colab sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeb804e",
   "metadata": {},
   "source": [
    "## Execute Refined Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc4bac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Determine training name based on checkpoint status\n",
    "if RESUME_TRAINING:\n",
    "    run_name = f'refine_resumed_{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "else:\n",
    "    run_name = f'refine_{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "\n",
    "# Training parameters\n",
    "train_params = dict(\n",
    "    data=DATA_YAML_PATH,\n",
    "    epochs=REFINE_EPOCHS,\n",
    "    batch=BATCH_SIZE,\n",
    "    imgsz=IMG_SIZE,\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer='SGD',\n",
    "    lr0=LR0,\n",
    "    lrf=LRF,\n",
    "    momentum=MOMENTUM,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    \n",
    "    # Warmup\n",
    "    warmup_epochs=WARMUP_EPOCHS,\n",
    "    warmup_momentum=0.8,\n",
    "    warmup_bias_lr=0.1,\n",
    "    \n",
    "    # Augmentations\n",
    "    hsv_h=AUG_HSV_H,\n",
    "    hsv_s=AUG_HSV_S,\n",
    "    hsv_v=AUG_HSV_V,\n",
    "    erasing=AUG_ERASING,\n",
    "    degrees=AUG_DEGREES,\n",
    "    shear=AUG_SHEAR,\n",
    "    fliplr=AUG_FLIPLR,\n",
    "    mosaic=AUG_MOSAIC,\n",
    "    mixup=AUG_MIXUP,\n",
    "    \n",
    "    # Output settings\n",
    "    project='refined_training',\n",
    "    name=run_name,\n",
    "    exist_ok=True,\n",
    "    \n",
    "    # Validation and saving\n",
    "    val=True,\n",
    "    save=True,\n",
    "    save_period=5,  # Save every 5 epochs\n",
    "    \n",
    "    # System\n",
    "    device=device,\n",
    "    amp=True,  # Enable automatic mixed precision for faster training\n",
    "    seed=42,\n",
    "    deterministic=True,\n",
    "    \n",
    "    # Resume handling\n",
    "    resume=RESUME_TRAINING,  # Resume if checkpoint exists\n",
    ")\n",
    "\n",
    "print(f'\\n{\"=\"*80}')\n",
    "if RESUME_TRAINING:\n",
    "    print(f'RESUMING REFINED TRAINING FROM EPOCH {checkpoint_info[\"last_epoch\"] + 1}')\n",
    "else:\n",
    "    print(f'STARTING FRESH REFINED TRAINING')\n",
    "print(f'{\"=\"*80}\\n')\n",
    "print(f'Start time: {datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "print(f'Device: {device}')\n",
    "print(f'Run name: {run_name}')\n",
    "print(f'Checkpoint tracking: {CHECKPOINT_CSV}\\n')\n",
    "\n",
    "# Execute training\n",
    "results = model.train(**train_params)\n",
    "\n",
    "print(f'\\n{\"=\"*80}')\n",
    "print(f'TRAINING COMPLETED')\n",
    "print(f'{\"=\"*80}\\n')\n",
    "print(f'End time: {datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "print(f'Results directory: {results.save_dir}')\n",
    "print(f'Checkpoint CSV: {CHECKPOINT_CSV}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dbebfb",
   "metadata": {},
   "source": [
    "## Export Best Model to Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507bd7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Find best model from training run\n",
    "best_model = Path(results.save_dir) / 'weights' / 'best.pt'\n",
    "last_model = Path(results.save_dir) / 'weights' / 'last.pt'\n",
    "\n",
    "# Export to Drive\n",
    "export_dir = f'{DRIVE_ROOT}/refined_models'\n",
    "os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "if best_model.exists():\n",
    "    export_best = f'{export_dir}/refined_best_{timestamp}.pt'\n",
    "    shutil.copy2(best_model, export_best)\n",
    "    print(f'Best model exported to: {export_best}')\n",
    "\n",
    "if last_model.exists():\n",
    "    export_last = f'{export_dir}/refined_last_{timestamp}.pt'\n",
    "    shutil.copy2(last_model, export_last)\n",
    "    print(f'Last model exported to: {export_last}')\n",
    "\n",
    "# Copy results CSV\n",
    "results_csv = Path(results.save_dir) / 'results.csv'\n",
    "if results_csv.exists():\n",
    "    export_results = f'{export_dir}/refined_results_{timestamp}.csv'\n",
    "    shutil.copy2(results_csv, export_results)\n",
    "    print(f'Results CSV exported to: {export_results}')\n",
    "\n",
    "print(f'\\nAll files exported to Google Drive')\n",
    "print(f'  Location: {export_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebcce99",
   "metadata": {},
   "source": [
    "## Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208e10e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load results\n",
    "results_csv_path = Path(results.save_dir) / 'results.csv'\n",
    "\n",
    "if results_csv_path.exists():\n",
    "    df = pd.read_csv(results_csv_path)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    \n",
    "    print('Refined Training Results Summary')\n",
    "    print('=' * 80)\n",
    "    print(f'Total epochs: {len(df)}')\n",
    "    print()\n",
    "    \n",
    "    # Final metrics\n",
    "    print('Final Epoch Metrics:')\n",
    "    print('-' * 80)\n",
    "    print(f'Classification Loss (val): {df[\"val/cls_loss\"].iloc[-1]:.4f}')\n",
    "    print(f'Segmentation mAP@50-95:    {df[\"metrics/mAP50-95(M)\"].iloc[-1]:.4f}')\n",
    "    print(f'Segmentation Precision:     {df[\"metrics/precision(M)\"].iloc[-1]:.4f}')\n",
    "    print(f'Segmentation Recall:        {df[\"metrics/recall(M)\"].iloc[-1]:.4f}')\n",
    "    print()\n",
    "    \n",
    "    # Best metrics\n",
    "    best_cls_loss_idx = df['val/cls_loss'].idxmin()\n",
    "    best_map_idx = df['metrics/mAP50-95(M)'].idxmax()\n",
    "    \n",
    "    print('Best Performance:')\n",
    "    print('-' * 80)\n",
    "    print(f'Best Classification Loss:   {df[\"val/cls_loss\"].iloc[best_cls_loss_idx]:.4f} (epoch {df[\"epoch\"].iloc[best_cls_loss_idx]:.0f})')\n",
    "    print(f'Best Segmentation mAP:      {df[\"metrics/mAP50-95(M)\"].iloc[best_map_idx]:.4f} (epoch {df[\"epoch\"].iloc[best_map_idx]:.0f})')\n",
    "    print()\n",
    "    \n",
    "    # Improvement over baseline\n",
    "    baseline_cls_loss = 0.4321  # From epoch 68 of original training\n",
    "    baseline_map = 0.4799\n",
    "    \n",
    "    final_cls_loss = df['val/cls_loss'].iloc[-1]\n",
    "    final_map = df['metrics/mAP50-95(M)'].iloc[-1]\n",
    "    \n",
    "    cls_improvement = ((baseline_cls_loss - final_cls_loss) / baseline_cls_loss) * 100\n",
    "    map_improvement = ((final_map - baseline_map) / baseline_map) * 100\n",
    "    \n",
    "    print('Improvement Over Baseline (Epoch 68):')\n",
    "    print('-' * 80)\n",
    "    print(f'Classification Loss: {baseline_cls_loss:.4f} â†’ {final_cls_loss:.4f} ({cls_improvement:+.2f}%)')\n",
    "    print(f'Segmentation mAP:    {baseline_map:.4f} â†’ {final_map:.4f} ({map_improvement:+.2f}%)')\n",
    "    print()\n",
    "    \n",
    "    # Plot training curves\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('Refined Training Performance', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot 1: Classification Loss\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(df['epoch'], df['train/cls_loss'], label='Train', linewidth=2, alpha=0.7)\n",
    "    ax.plot(df['epoch'], df['val/cls_loss'], label='Validation', linewidth=2)\n",
    "    ax.axhline(y=baseline_cls_loss, color='red', linestyle='--', label=f'Baseline ({baseline_cls_loss:.4f})', alpha=0.5)\n",
    "    ax.axvline(x=30, color='gray', linestyle=':', alpha=0.5, label='Phase 2')\n",
    "    ax.axvline(x=60, color='gray', linestyle=':', alpha=0.5, label='Phase 3')\n",
    "    ax.set_xlabel('Epoch', fontweight='bold')\n",
    "    ax.set_ylabel('Loss', fontweight='bold')\n",
    "    ax.set_title('Classification Loss (Lower is Better)', fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    \n",
    "    # Plot 2: Segmentation mAP\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(df['epoch'], df['metrics/mAP50-95(M)'], linewidth=2, color='green')\n",
    "    ax.axhline(y=baseline_map, color='red', linestyle='--', label=f'Baseline ({baseline_map:.4f})', alpha=0.5)\n",
    "    ax.axvline(x=30, color='gray', linestyle=':', alpha=0.5)\n",
    "    ax.axvline(x=60, color='gray', linestyle=':', alpha=0.5)\n",
    "    ax.set_xlabel('Epoch', fontweight='bold')\n",
    "    ax.set_ylabel('mAP@50-95', fontweight='bold')\n",
    "    ax.set_title('Segmentation Quality (Higher is Better)', fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    ax.set_ylim([0.4, 1.0])\n",
    "    \n",
    "    # Plot 3: Precision & Recall\n",
    "    ax = axes[1, 0]\n",
    "    ax.plot(df['epoch'], df['metrics/precision(M)'], label='Precision', linewidth=2)\n",
    "    ax.plot(df['epoch'], df['metrics/recall(M)'], label='Recall', linewidth=2)\n",
    "    ax.axvline(x=30, color='gray', linestyle=':', alpha=0.5)\n",
    "    ax.axvline(x=60, color='gray', linestyle=':', alpha=0.5)\n",
    "    ax.set_xlabel('Epoch', fontweight='bold')\n",
    "    ax.set_ylabel('Score', fontweight='bold')\n",
    "    ax.set_title('Precision & Recall', fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    ax.set_ylim([0.7, 1.0])\n",
    "    \n",
    "    # Plot 4: Learning Rate\n",
    "    ax = axes[1, 1]\n",
    "    ax.plot(df['epoch'], df['lr/pg0'], linewidth=2, color='purple')\n",
    "    ax.axvline(x=30, color='gray', linestyle=':', alpha=0.5, label='Phase transitions')\n",
    "    ax.axvline(x=60, color='gray', linestyle=':', alpha=0.5)\n",
    "    ax.set_xlabel('Epoch', fontweight='bold')\n",
    "    ax.set_ylabel('Learning Rate', fontweight='bold')\n",
    "    ax.set_title('Learning Rate Schedule', fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    plot_path = Path(results.save_dir) / 'refined_training_analysis.png'\n",
    "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "    print(f'Training curves saved to: {plot_path}')\n",
    "    \n",
    "    # Also save to Drive\n",
    "    drive_plot_path = f'{export_dir}/refined_training_analysis_{timestamp}.png'\n",
    "    shutil.copy2(plot_path, drive_plot_path)\n",
    "    print(f'Plots exported to Drive: {drive_plot_path}')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print('Results CSV not found. Training may not have completed.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
