{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "847daa14",
   "metadata": {},
   "source": [
    "# License Plate OCR Model Testing & Validation\n",
    "\n",
    "We are benchmarking four distinct approaches here: EasyOCR, PyTesseract, your Custom OCR, and the Refined Custom OCR.\n",
    "\n",
    "The core of this evaluation is the Intersection over Union (IoU) calculation. This metric tells us how much our predicted bounding box overlaps with the ground truth. If the overlap is over our threshold (standard 0.5), it's a hit (True Positive). Anything else is either a miss (False Negative) or a ghost (False Positive). We're also calculating Mean Average Precision (mAP) across multiple thresholds to see which model has the most \"stamina\" when we tighten the requirements for accuracy.\n",
    "\n",
    "Dataset: CatEye-ALPR-v3-3 (pre-cropped license plates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f8e6e2",
   "metadata": {},
   "source": [
    "# Benchmarking Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741e0bdf",
   "metadata": {},
   "source": [
    "### Merge the train, test, and valid folders into one folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fa99fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "src_root = 'CatEye-ALPR-v3-3'\n",
    "dst_root = 'CatEye-ALPR-v3-3-Merged'\n",
    "\n",
    "subfolders = ['images', 'labels']\n",
    "splits = ['train', 'test', 'valid']\n",
    "\n",
    "# Create merged directories\n",
    "for subfolder in subfolders:\n",
    "    os.makedirs(os.path.join(dst_root, subfolder), exist_ok=True)\n",
    "\n",
    "# Merge files from each split into the merged folder\n",
    "for split in splits:\n",
    "    for subfolder in subfolders:\n",
    "        src_dir = os.path.join(src_root, split, subfolder)\n",
    "        dst_dir = os.path.join(dst_root, subfolder)\n",
    "        if os.path.exists(src_dir):\n",
    "            for fname in os.listdir(src_dir):\n",
    "                src_file = os.path.join(src_dir, fname)\n",
    "                dst_file = os.path.join(dst_dir, fname)\n",
    "                if not os.path.exists(dst_file):\n",
    "                    shutil.copy2(src_file, dst_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab43cbd",
   "metadata": {},
   "source": [
    "This folder will be used for benchmarking the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cdd3ac",
   "metadata": {},
   "source": [
    "## Model Initialization and Configuration\n",
    "\n",
    "In this section, we set up our hardware acceleration and load our pre-trained weights. We are using a standard ImageNet normalization for our custom models to ensure the input distribution matches what they saw during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28b0156e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CPU. Note: This module is much faster with a GPU.\n",
      "Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import ultralytics\n",
    "import easyocr\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load custom models\n",
    "# Setting weights_only=False bypasses the security check\n",
    "custom_model = torch.load('models/custom_ocr.pt', map_location=device, weights_only=False)\n",
    "refined_model = torch.load('models/custom_ocr_refined.pt', map_location=device, weights_only=False)\n",
    "\n",
    "# Initialize EasyOCR\n",
    "reader = easyocr.Reader(['en'], gpu=torch.cuda.is_available())\n",
    "\n",
    "# Preprocessing pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075b211c",
   "metadata": {},
   "source": [
    "## Inference Wrappers\n",
    "\n",
    "These functions standardize the output from different libraries. No matter the engine, we want a dictionary containing bounding boxes, confidence scores, and the clock time it took to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2251d74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_easyocr_predictions(image_path):\n",
    "    \"\"\"\n",
    "    Wraps EasyOCR detection logic.\n",
    "    Inputs: image_path (str)\n",
    "    Returns: dict(boxes: np.ndarray, scores: np.ndarray, inference_time: float)\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    results = reader.readtext(image_path)\n",
    "    inference_time = time.time() - start\n",
    "    \n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    for bbox, text, conf in results:\n",
    "        x_coords = [p[0] for p in bbox]\n",
    "        y_coords = [p[1] for p in bbox]\n",
    "        x1, y1 = min(x_coords), min(y_coords)\n",
    "        x2, y2 = max(x_coords), max(y_coords)\n",
    "        boxes.append([x1, y1, x2, y2])\n",
    "        confidences.append(conf)\n",
    "    \n",
    "    return {\n",
    "        'boxes': np.array(boxes) if boxes else np.array([]).reshape(0, 4),\n",
    "        'scores': np.array(confidences) if confidences else np.array([]),\n",
    "        'inference_time': inference_time\n",
    "    }\n",
    "\n",
    "def get_pytesseract_predictions(image_path):\n",
    "    \"\"\"\n",
    "    Wraps Tesseract OCR engine.\n",
    "    Inputs: image_path (str)\n",
    "    Returns: dict(boxes: np.ndarray, scores: np.ndarray, inference_time: float)\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    img = Image.open(image_path)\n",
    "    data = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT)\n",
    "    inference_time = time.time() - start\n",
    "    \n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    for i in range(len(data['text'])):\n",
    "        if int(data['conf'][i]) > 0:\n",
    "            x, y, w, h = data['left'][i], data['top'][i], data['width'][i], data['height'][i]\n",
    "            boxes.append([x, y, x+w, y+h])\n",
    "            confidences.append(data['conf'][i] / 100.0)\n",
    "    \n",
    "    return {\n",
    "        'boxes': np.array(boxes) if boxes else np.array([]).reshape(0, 4),\n",
    "        'scores': np.array(confidences) if confidences else np.array([]),\n",
    "        'inference_time': inference_time\n",
    "    }\n",
    "\n",
    "def get_custom_model_predictions(model, image_path, conf_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Inference for custom PyTorch detection models.\n",
    "    Inputs: model (nn.Module), image_path (str), conf_threshold (float)\n",
    "    Returns: dict(boxes: np.ndarray, scores: np.ndarray, inference_time: float)\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "    \n",
    "    inference_time = time.time() - start\n",
    "    \n",
    "    if isinstance(output, dict):\n",
    "        boxes = output['boxes'][0].cpu().numpy()\n",
    "        scores = output['scores'][0].cpu().numpy()\n",
    "    else:\n",
    "        boxes = output[0]['boxes'].cpu().numpy()\n",
    "        scores = output[0]['scores'].cpu().numpy()\n",
    "    \n",
    "    mask = scores >= conf_threshold\n",
    "    boxes = boxes[mask]\n",
    "    scores = scores[mask]\n",
    "    \n",
    "    return {\n",
    "        'boxes': boxes,\n",
    "        'scores': scores,\n",
    "        'inference_time': inference_time\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729fd810",
   "metadata": {},
   "source": [
    "## Metric Calculation Logic\n",
    "This is the scorecard. We are calculating IoU for overlap, standard Precision/Recall, and Mean Average Precision (mAP) to ensure our models aren't just getting lucky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93579cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculates Intersection over Union between two boxes.\n",
    "    Inputs: box1 (list/np.array), box2 (list/np.array)\n",
    "    Returns: float (IoU score)\n",
    "    \"\"\"\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union = area1 + area2 - intersection\n",
    "    \n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "def calculate_metrics(pred_boxes, pred_scores, gt_boxes, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Computes PR metrics for a single image.\n",
    "    Inputs: pred_boxes (np.array), pred_scores (np.array), gt_boxes (np.array), iou_threshold (float)\n",
    "    Returns: dict(precision: float, recall: float, tp: int, fp: int, fn: int)\n",
    "    \"\"\"\n",
    "    if len(pred_boxes) == 0:\n",
    "        return {'precision': 0.0, 'recall': 0.0 if len(gt_boxes) > 0 else 1.0, 'tp': 0, 'fp': 0, 'fn': len(gt_boxes)}\n",
    "    \n",
    "    if len(gt_boxes) == 0:\n",
    "        return {'precision': 0.0, 'recall': 1.0, 'tp': 0, 'fp': len(pred_boxes), 'fn': 0}\n",
    "    \n",
    "    matched_gt = set()\n",
    "    tp, fp = 0, 0\n",
    "    sorted_indices = np.argsort(pred_scores)[::-1]\n",
    "    \n",
    "    for idx in sorted_indices:\n",
    "        pred_box = pred_boxes[idx]\n",
    "        max_iou, max_gt_idx = 0, -1\n",
    "        \n",
    "        for gt_idx, gt_box in enumerate(gt_boxes):\n",
    "            if gt_idx in matched_gt:\n",
    "                continue\n",
    "            iou = calculate_iou(pred_box, gt_box)\n",
    "            if iou > max_iou:\n",
    "                max_iou, max_gt_idx = iou, gt_idx\n",
    "        \n",
    "        if max_iou >= iou_threshold:\n",
    "            tp += 1\n",
    "            matched_gt.add(max_gt_idx)\n",
    "        else:\n",
    "            fp += 1\n",
    "    \n",
    "    fn = len(gt_boxes) - len(matched_gt)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    \n",
    "    return {'precision': precision, 'recall': recall, 'tp': tp, 'fp': fp, 'fn': fn}\n",
    "\n",
    "def calculate_map(pred_boxes, pred_scores, gt_boxes, iou_thresholds=[0.5, 0.75]):\n",
    "    \"\"\"\n",
    "    Computes mAP across specified IoU thresholds.\n",
    "    Inputs: pred_boxes (np.array), pred_scores (np.array), gt_boxes (np.array), iou_thresholds (list)\n",
    "    Returns: float (mAP score)\n",
    "    \"\"\"\n",
    "    aps = []\n",
    "    for iou_thresh in iou_thresholds:\n",
    "        if len(pred_boxes) == 0 or len(gt_boxes) == 0:\n",
    "            aps.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        sorted_indices = np.argsort(pred_scores)[::-1]\n",
    "        sorted_boxes = pred_boxes[sorted_indices]\n",
    "        matched_gt = [False] * len(gt_boxes)\n",
    "        tp, fp = np.zeros(len(sorted_boxes)), np.zeros(len(sorted_boxes))\n",
    "        \n",
    "        for pred_idx, pred_box in enumerate(sorted_boxes):\n",
    "            max_iou, max_gt_idx = 0, -1\n",
    "            for gt_idx, gt_box in enumerate(gt_boxes):\n",
    "                iou = calculate_iou(pred_box, gt_box)\n",
    "                if iou > max_iou:\n",
    "                    max_iou, max_gt_idx = iou, gt_idx\n",
    "            \n",
    "            if max_iou >= iou_thresh and not matched_gt[max_gt_idx]:\n",
    "                tp[pred_idx], matched_gt[max_gt_idx] = 1, True\n",
    "            else:\n",
    "                fp[pred_idx] = 1\n",
    "        \n",
    "        tp_cumsum, fp_cumsum = np.cumsum(tp), np.cumsum(fp)\n",
    "        recalls = tp_cumsum / len(gt_boxes)\n",
    "        precisions = tp_cumsum / (tp_cumsum + fp_cumsum)\n",
    "        recalls = np.concatenate(([0], recalls, [1]))\n",
    "        precisions = np.concatenate(([0], precisions, [0]))\n",
    "        \n",
    "        for i in range(len(precisions) - 1, 0, -1):\n",
    "            precisions[i - 1] = max(precisions[i - 1], precisions[i])\n",
    "        \n",
    "        indices = np.where(recalls[1:] != recalls[:-1])[0]\n",
    "        ap = np.sum((recalls[indices + 1] - recalls[indices]) * precisions[indices + 1])\n",
    "        aps.append(ap)\n",
    "    \n",
    "    return np.mean(aps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f38c782",
   "metadata": {},
   "source": [
    "## Benchmarking Execution\n",
    "We'll iterate through our test dataset and gather the stats for every model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4872f012",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[32m     64\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m img, target\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# Initialize the full dataset from your merged folder\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m full_dataset = \u001b[43mLicensePlateDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mCatEye-ALPR-v3-3-Merged\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# Create the test_dataset object (using Subset to match your indices logic)\u001b[39;00m\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# Here we use the full range as the \"test\" split for benchmarking\u001b[39;00m\n\u001b[32m     71\u001b[39m indices = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(full_dataset)))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mLicensePlateDataset.__init__\u001b[39m\u001b[34m(self, root_dir, transform)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mself\u001b[39m.imgs = [os.path.join(\u001b[38;5;28mself\u001b[39m.image_dir, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(os.listdir(\u001b[38;5;28mself\u001b[39m.image_dir)) \n\u001b[32m     21\u001b[39m              \u001b[38;5;28;01mif\u001b[39;00m f.lower().endswith((\u001b[33m'\u001b[39m\u001b[33m.png\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m.jpg\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m.jpeg\u001b[39m\u001b[33m'\u001b[39m))]\n\u001b[32m     23\u001b[39m \u001b[38;5;28mself\u001b[39m.targets = []\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mLicensePlateDataset._load_targets\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[33;03mParses YOLO format labels into absolute pixel coordinates.\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m img_path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.imgs:\n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m# Get image dimensions for coordinate conversion\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     img = \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m     h, w, _ = img.shape\n\u001b[32m     35\u001b[39m     label_path = os.path.join(\u001b[38;5;28mself\u001b[39m.label_dir, os.path.basename(img_path).rsplit(\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m] + \u001b[33m'\u001b[39m\u001b[33m.txt\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lifei\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\utils\\patches.py:34\u001b[39m, in \u001b[36mimread\u001b[39m\u001b[34m(filename, flags)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mimread\u001b[39m(filename: \u001b[38;5;28mstr\u001b[39m, flags: \u001b[38;5;28mint\u001b[39m = cv2.IMREAD_COLOR) -> np.ndarray | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     21\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Read an image from a file with multilanguage filename support.\u001b[39;00m\n\u001b[32m     22\u001b[39m \n\u001b[32m     23\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m \u001b[33;03m        >>> img = imread(\"path/to/image.jpg\", cv2.IMREAD_GRAYSCALE)\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     file_bytes = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfromfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m filename.endswith((\u001b[33m\"\u001b[39m\u001b[33m.tiff\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m.tif\u001b[39m\u001b[33m\"\u001b[39m)):\n\u001b[32m     36\u001b[39m         success, frames = cv2.imdecodemulti(file_bytes, cv2.IMREAD_UNCHANGED)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class LicensePlateDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for License Plate Detection benchmarking.\n",
    "    Inputs: \n",
    "        root_dir (str): Path to merged dataset folder\n",
    "        transform (callable): Preprocessing transforms\n",
    "    Returns: \n",
    "        image (Tensor), target (dict with 'boxes' key)\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_dir = os.path.join(root_dir, 'images')\n",
    "        self.label_dir = os.path.join(root_dir, 'labels')\n",
    "        \n",
    "        self.imgs = [os.path.join(self.image_dir, f) for f in sorted(os.listdir(self.image_dir)) \n",
    "                     if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "        self.targets = []\n",
    "        self._load_targets()\n",
    "\n",
    "    def _load_targets(self):\n",
    "        \"\"\"\n",
    "        Parses YOLO format labels. Optimized to use image headers for speed.\n",
    "        \"\"\"\n",
    "        for img_path in tqdm(self.imgs, desc=\"Preparing Dataset Metadata\"):\n",
    "            # Fast header-only read for dimensions\n",
    "            with Image.open(img_path) as img:\n",
    "                w, h = img.size\n",
    "            \n",
    "            label_path = os.path.join(self.label_dir, os.path.basename(img_path).rsplit('.', 1)[0] + '.txt')\n",
    "            boxes = []\n",
    "            \n",
    "            if os.path.exists(label_path):\n",
    "                with open(label_path, 'r') as f:\n",
    "                    for line in f:\n",
    "                        # YOLO: class, x_center, y_center, width, height\n",
    "                        _, x_c, y_c, bw, bh = map(float, line.split())\n",
    "                        \n",
    "                        # Scale to absolute pixels\n",
    "                        x1 = (x_c - bw / 2) * w\n",
    "                        y1 = (y_c - bh / 2) * h\n",
    "                        x2 = (x_c + bw / 2) * w\n",
    "                        y2 = (y_c + bh / 2) * h\n",
    "                        boxes.append([x1, y1, x2, y2])\n",
    "            \n",
    "            self.targets.append({'boxes': torch.tensor(boxes)})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.imgs[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        target = self.targets[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        return img, target\n",
    "\n",
    "# Execute the preparation\n",
    "full_dataset = LicensePlateDataset(root_dir='CatEye-ALPR-v3-3-Merged', transform=transform)\n",
    "indices = list(range(len(full_dataset)))\n",
    "test_dataset = Subset(full_dataset, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fea86915",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Preparation of test data\u001b[39;00m\n\u001b[32m     32\u001b[39m test_data = []\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mtest_dataset\u001b[49m)):\n\u001b[32m     34\u001b[39m     img_path = test_dataset.dataset.imgs[test_dataset.indices[idx]]\n\u001b[32m     35\u001b[39m     gt_boxes = test_dataset.dataset.targets[test_dataset.indices[idx]][\u001b[33m'\u001b[39m\u001b[33mboxes\u001b[39m\u001b[33m'\u001b[39m].numpy()\n",
      "\u001b[31mNameError\u001b[39m: name 'test_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "def benchmark_model(model_name, predict_fn, test_data, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Runs the full benchmarking loop for a model.\n",
    "    Inputs: model_name (str), predict_fn (callable), test_data (list), iou_threshold (float)\n",
    "    Returns: dict (aggregated metrics)\n",
    "    \"\"\"\n",
    "    all_metrics, all_times, all_precisions, all_recalls, all_maps = [], [], [], [], []\n",
    "    \n",
    "    for image_path, gt_boxes in test_data:\n",
    "        predictions = predict_fn(image_path)\n",
    "        metrics = calculate_metrics(predictions['boxes'], predictions['scores'], gt_boxes, iou_threshold)\n",
    "        map_score = calculate_map(predictions['boxes'], predictions['scores'], gt_boxes)\n",
    "        \n",
    "        all_metrics.append(metrics)\n",
    "        all_times.append(predictions['inference_time'])\n",
    "        all_precisions.append(metrics['precision'])\n",
    "        all_recalls.append(metrics['recall'])\n",
    "        all_maps.append(map_score)\n",
    "    \n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'avg_precision': np.mean(all_precisions),\n",
    "        'avg_recall': np.mean(all_recalls),\n",
    "        'avg_map': np.mean(all_maps),\n",
    "        'avg_inference_time': np.mean(all_times),\n",
    "        'total_tp': sum(m['tp'] for m in all_metrics),\n",
    "        'total_fp': sum(m['fp'] for m in all_metrics),\n",
    "        'total_fn': sum(m['fn'] for m in all_metrics)\n",
    "    }\n",
    "\n",
    "# Preparation of test data\n",
    "test_data = []\n",
    "for idx in range(len(test_dataset)):\n",
    "    img_path = test_dataset.dataset.imgs[test_dataset.indices[idx]]\n",
    "    gt_boxes = test_dataset.dataset.targets[test_dataset.indices[idx]]['boxes'].numpy()\n",
    "    test_data.append((img_path, gt_boxes))\n",
    "\n",
    "# Execute benchmarks\n",
    "results = []\n",
    "results.append(benchmark_model('EasyOCR', get_easyocr_predictions, test_data))\n",
    "results.append(benchmark_model('Pytesseract', get_pytesseract_predictions, test_data))\n",
    "results.append(benchmark_model('Custom OCR', lambda x: get_custom_model_predictions(custom_model, x), test_data))\n",
    "results.append(benchmark_model('Custom OCR Refined', lambda x: get_custom_model_predictions(refined_model, x), test_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
