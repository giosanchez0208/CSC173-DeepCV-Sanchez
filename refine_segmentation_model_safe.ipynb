{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giosanchez0208/CSC173-DeepCV-Sanchez/blob/main/refine_segmentation_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a7e14ac",
      "metadata": {
        "id": "3a7e14ac"
      },
      "source": [
        "## Setup: Google Colab Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02620f41",
      "metadata": {
        "id": "02620f41"
      },
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Verify CUDA is available\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "    print(f'GPU available: {torch.cuda.get_device_name(0)}')\n",
        "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')\n",
        "else:\n",
        "    device = 'cpu'\n",
        "    print('No GPU available, using CPU (training will be slow)')\n",
        "\n",
        "print(f'\\nPyTorch version: {torch.__version__}')\n",
        "print(f'CUDA version: {torch.version.cuda}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b038eae",
      "metadata": {
        "id": "1b038eae"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q ultralytics opencv-python-headless pillow pyyaml numpy scipy matplotlib pandas gdown\n",
        "\n",
        "print('All packages installed successfully')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cb3737f",
      "metadata": {
        "id": "9cb3737f"
      },
      "source": [
        "## Dataset and Model Download (Reproducible Setup)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b48dbb1",
      "metadata": {
        "id": "0b48dbb1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gdown\n",
        "import zipfile\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "# Public Google Drive folder for the dataset\n",
        "PUBLIC_DRIVE_FOLDER = 'https://drive.google.com/drive/folders/1rYbZXSwnd0DQ49VP_04C8OYkhU6b4p-t'\n",
        "\n",
        "# File IDs (you'll need to extract these from the public folder)\n",
        "# You can get these by right-clicking on the file in Google Drive and getting the shareable link\n",
        "# The file ID is the long string between /d/ and /view\n",
        "DATASET_ZIP_ID = 'YOUR_DATASET_ZIP_FILE_ID'  # Replace with actual file ID\n",
        "PRETRAINED_MODEL_ID = 'YOUR_PRETRAINED_MODEL_FILE_ID'  # Replace with actual file ID\n",
        "\n",
        "# Local paths\n",
        "LOCAL_ROOT = '/content/ocr_project'\n",
        "DATASET_ZIP_PATH = f'{LOCAL_ROOT}/dataset.zip'\n",
        "LOCAL_DATASET_PATH = f'{LOCAL_ROOT}/dataset'\n",
        "LOCAL_MODELS_PATH = f'{LOCAL_ROOT}/models'\n",
        "LOCAL_CHECKPOINTS_PATH = f'{LOCAL_ROOT}/checkpoints'\n",
        "\n",
        "# Create directories\n",
        "for path in [LOCAL_ROOT, LOCAL_MODELS_PATH, LOCAL_CHECKPOINTS_PATH]:\n",
        "    Path(path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print('Directory structure created:')\n",
        "print(f'  Root: {LOCAL_ROOT}')\n",
        "print(f'  Dataset: {LOCAL_DATASET_PATH}')\n",
        "print(f'  Models: {LOCAL_MODELS_PATH}')\n",
        "print(f'  Checkpoints: {LOCAL_CHECKPOINTS_PATH}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2cd3d33",
      "metadata": {
        "id": "b2cd3d33"
      },
      "outputs": [],
      "source": [
        "def download_file_from_drive(file_id, output_path, retries=3):\n",
        "    \"\"\"Download a file from Google Drive using gdown.\"\"\"\n",
        "    url = f'https://drive.google.com/uc?id={file_id}'\n",
        "    \n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            print(f'Download attempt {attempt + 1}/{retries} for {output_path}')\n",
        "            gdown.download(url, output_path, quiet=False)\n",
        "            \n",
        "            if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "                print(f'Successfully downloaded: {output_path}')\n",
        "                print(f'File size: {os.path.getsize(output_path) / (1024**2):.2f} MB')\n",
        "                return True\n",
        "            else:\n",
        "                print(f'Download failed: file is empty or does not exist')\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f'Download attempt {attempt + 1} failed: {e}')\n",
        "            \n",
        "    return False\n",
        "\n",
        "# Download dataset if not already present\n",
        "if not os.path.exists(DATASET_ZIP_PATH):\n",
        "    print('Downloading dataset...')\n",
        "    # IMPORTANT: Replace with actual file ID from your public folder\n",
        "    # You need to get the actual file ID for dataset.zip\n",
        "    success = download_file_from_drive(DATASET_ZIP_ID, DATASET_ZIP_PATH)\n",
        "    if not success:\n",
        "        print('ERROR: Failed to download dataset. Please check the file ID.')\n",
        "        print(f'Public folder: {PUBLIC_DRIVE_FOLDER}')\n",
        "        print('Please update DATASET_ZIP_ID with the actual file ID from the dataset.zip file')\n",
        "        raise FileNotFoundError('Dataset download failed')\n",
        "else:\n",
        "    print(f'Dataset already exists at: {DATASET_ZIP_PATH}')\n",
        "\n",
        "# Download pretrained model if not already present\n",
        "PRETRAINED_MODEL_PATH = f'{LOCAL_MODELS_PATH}/custom_ocr_last.pt'\n",
        "if not os.path.exists(PRETRAINED_MODEL_PATH):\n",
        "    print('\\nDownloading pretrained model...')\n",
        "    # IMPORTANT: Replace with actual file ID from your public folder\n",
        "    # You need to get the actual file ID for custom_ocr_last.pt\n",
        "    success = download_file_from_drive(PRETRAINED_MODEL_ID, PRETRAINED_MODEL_PATH)\n",
        "    if not success:\n",
        "        print('WARNING: Failed to download pretrained model. Training will start from scratch.')\n",
        "        PRETRAINED_MODEL_PATH = None\n",
        "else:\n",
        "    print(f'Pretrained model already exists at: {PRETRAINED_MODEL_PATH}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7000dfec",
      "metadata": {
        "id": "7000dfec"
      },
      "outputs": [],
      "source": [
        "# Extract dataset if not already extracted\n",
        "if not os.path.exists(LOCAL_DATASET_PATH) or not os.path.exists(f'{LOCAL_DATASET_PATH}/data.yaml'):\n",
        "    print('\\nExtracting dataset...')\n",
        "    \n",
        "    if not os.path.exists(DATASET_ZIP_PATH):\n",
        "        print(f'ERROR: Dataset zip not found at {DATASET_ZIP_PATH}')\n",
        "        raise FileNotFoundError('Dataset zip file not found')\n",
        "    \n",
        "    try:\n",
        "        with zipfile.ZipFile(DATASET_ZIP_PATH, 'r') as zip_ref:\n",
        "            zip_ref.extractall(LOCAL_ROOT)\n",
        "        \n",
        "        # Check if extraction created a nested 'dataset' folder\n",
        "        extracted_paths = list(Path(LOCAL_ROOT).glob('**/data.yaml'))\n",
        "        \n",
        "        if extracted_paths:\n",
        "            # Found data.yaml somewhere\n",
        "            data_yaml_path = str(extracted_paths[0])\n",
        "            dataset_parent = str(extracted_paths[0].parent)\n",
        "            \n",
        "            # If it's not in the expected location, move it\n",
        "            if dataset_parent != LOCAL_DATASET_PATH:\n",
        "                print(f'Moving dataset from {dataset_parent} to {LOCAL_DATASET_PATH}')\n",
        "                if os.path.exists(LOCAL_DATASET_PATH):\n",
        "                    shutil.rmtree(LOCAL_DATASET_PATH)\n",
        "                shutil.move(dataset_parent, LOCAL_DATASET_PATH)\n",
        "        \n",
        "        print(f'Dataset extracted to: {LOCAL_DATASET_PATH}')\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f'ERROR: Failed to extract dataset: {e}')\n",
        "        raise\n",
        "else:\n",
        "    print(f'Dataset already extracted at: {LOCAL_DATASET_PATH}')\n",
        "\n",
        "# Verify data.yaml exists\n",
        "DATA_YAML_PATH = f'{LOCAL_DATASET_PATH}/data.yaml'\n",
        "if not os.path.exists(DATA_YAML_PATH):\n",
        "    print(f'ERROR: data.yaml not found at {DATA_YAML_PATH}')\n",
        "    print('Looking for data.yaml in extracted files...')\n",
        "    \n",
        "    # Search for data.yaml\n",
        "    for root, dirs, files in os.walk(LOCAL_ROOT):\n",
        "        if 'data.yaml' in files:\n",
        "            DATA_YAML_PATH = os.path.join(root, 'data.yaml')\n",
        "            print(f'Found data.yaml at: {DATA_YAML_PATH}')\n",
        "            break\n",
        "    \n",
        "    if not os.path.exists(DATA_YAML_PATH):\n",
        "        raise FileNotFoundError(f'data.yaml not found in {LOCAL_ROOT}')\n",
        "\n",
        "print(f'\\nData configuration: {DATA_YAML_PATH}')\n",
        "print(f'Pretrained model: {PRETRAINED_MODEL_PATH if PRETRAINED_MODEL_PATH else \"None (starting from scratch)\"}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ebec263",
      "metadata": {
        "id": "0ebec263"
      },
      "outputs": [],
      "source": [
        "# Sanity check: Verify dataset structure\n",
        "print('\\n=== DATASET SANITY CHECK ===')\n",
        "\n",
        "required_folders = ['train/images', 'train/labels', \n",
        "                    'val/images', 'val/labels',\n",
        "                    'test/images', 'test/labels']\n",
        "\n",
        "all_good = True\n",
        "for folder in required_folders:\n",
        "    folder_path = f'{LOCAL_DATASET_PATH}/{folder}'\n",
        "    if os.path.exists(folder_path):\n",
        "        file_count = len(os.listdir(folder_path))\n",
        "        print(f'✓ {folder}: {file_count} files')\n",
        "    else:\n",
        "        print(f'✗ {folder}: NOT FOUND')\n",
        "        all_good = False\n",
        "\n",
        "# Check data.yaml content\n",
        "try:\n",
        "    with open(DATA_YAML_PATH, 'r') as f:\n",
        "        data_config = yaml.safe_load(f)\n",
        "    \n",
        "    print(f'\\ndata.yaml content:')\n",
        "    print(f'  Classes: {len(data_config.get(\"names\", []))}')\n",
        "    print(f'  Path: {data_config.get(\"path\", \"Not specified\")}')\n",
        "    print(f'  Train: {data_config.get(\"train\", \"Not specified\")}')\n",
        "    print(f'  Val: {data_config.get(\"val\", \"Not specified\")}')\n",
        "    \n",
        "    # Fix paths in data.yaml if they're wrong\n",
        "    if data_config.get('path') != LOCAL_DATASET_PATH:\n",
        "        print(f'\\nFixing data.yaml paths...')\n",
        "        data_config['path'] = LOCAL_DATASET_PATH\n",
        "        data_config['train'] = 'train/images'\n",
        "        data_config['val'] = 'val/images'\n",
        "        data_config['test'] = 'test/images'\n",
        "        \n",
        "        with open(DATA_YAML_PATH, 'w') as f:\n",
        "            yaml.dump(data_config, f, default_flow_style=False)\n",
        "        print(f'  Updated paths in data.yaml')\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f'ERROR reading data.yaml: {e}')\n",
        "    all_good = False\n",
        "\n",
        "if all_good:\n",
        "    print('\\n✓ All dataset checks passed')\n",
        "else:\n",
        "    print('\\n✗ Dataset has issues. Please check the structure.')\n",
        "    raise ValueError('Dataset structure is incorrect')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6201797b",
      "metadata": {
        "id": "6201797b"
      },
      "source": [
        "## Enhanced Checkpoint Management System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4287274",
      "metadata": {
        "id": "e4287274"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "import threading\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "class CheckpointManager:\n",
        "    \"\"\"\n",
        "    Robust checkpoint management system that:\n",
        "    1. Saves CSV progress every epoch\n",
        "    2. Saves model checkpoints to Google Drive every epoch\n",
        "    3. Maintains detailed training history\n",
        "    4. Allows resuming from any point\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, experiment_name, local_checkpoint_dir, drive_checkpoint_dir=None):\n",
        "        \"\"\"\n",
        "        Initialize checkpoint manager.\n",
        "        \n",
        "        Args:\n",
        "            experiment_name: Name for this training run\n",
        "            local_checkpoint_dir: Local directory for fast checkpoint access\n",
        "            drive_checkpoint_dir: Google Drive directory for permanent storage (optional)\n",
        "        \"\"\"\n",
        "        self.experiment_name = experiment_name\n",
        "        self.local_dir = Path(local_checkpoint_dir) / experiment_name\n",
        "        self.drive_dir = Path(drive_checkpoint_dir) / experiment_name if drive_checkpoint_dir else None\n",
        "        \n",
        "        # Create directories\n",
        "        self.local_dir.mkdir(parents=True, exist_ok=True)\n",
        "        if self.drive_dir:\n",
        "            self.drive_dir.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        # File paths\n",
        "        self.csv_path = self.local_dir / 'training_progress.csv'\n",
        "        self.config_path = self.local_dir / 'training_config.json'\n",
        "        self.best_model_local = self.local_dir / 'best_model.pt'\n",
        "        self.last_model_local = self.local_dir / 'last_model.pt'\n",
        "        \n",
        "        if self.drive_dir:\n",
        "            self.best_model_drive = self.drive_dir / 'best_model.pt'\n",
        "            self.last_model_drive = self.drive_dir / 'last_model.pt'\n",
        "            self.csv_drive = self.drive_dir / 'training_progress.csv'\n",
        "        \n",
        "        # Initialize CSV if it doesn't exist\n",
        "        self._initialize_csv()\n",
        "        \n",
        "        print(f'Checkpoint Manager initialized:')\n",
        "        print(f'  Experiment: {experiment_name}')\n",
        "        print(f'  Local directory: {self.local_dir}')\n",
        "        if self.drive_dir:\n",
        "            print(f'  Drive directory: {self.drive_dir}')\n",
        "    \n",
        "    def _initialize_csv(self):\n",
        "        \"\"\"Initialize CSV with required columns.\"\"\"\n",
        "        if not self.csv_path.exists():\n",
        "            columns = [\n",
        "                'epoch', 'timestamp',\n",
        "                'train/cls_loss', 'val/cls_loss',\n",
        "                'train/seg_loss', 'val/seg_loss',\n",
        "                'train/box_loss', 'val/box_loss',\n",
        "                'metrics/precision(M)', 'metrics/recall(M)',\n",
        "                'metrics/mAP50(M)', 'metrics/mAP50-95(M)',\n",
        "                'learning_rate', 'phase',\n",
        "                'ocr_char_accuracy', 'ocr_top2_accuracy', 'ocr_top3_accuracy'\n",
        "            ]\n",
        "            pd.DataFrame(columns=columns).to_csv(self.csv_path, index=False)\n",
        "    \n",
        "    def save_config(self, config):\n",
        "        \"\"\"Save training configuration.\"\"\"\n",
        "        with open(self.config_path, 'w') as f:\n",
        "            json.dump(config, f, indent=2)\n",
        "        \n",
        "        # Also save to drive if available\n",
        "        if self.drive_dir:\n",
        "            drive_config_path = self.drive_dir / 'training_config.json'\n",
        "            with open(drive_config_path, 'w') as f:\n",
        "                json.dump(config, f, indent=2)\n",
        "    \n",
        "    def save_progress(self, epoch, metrics, model_path=None, is_best=False):\n",
        "        \"\"\"\n",
        "        Save training progress for an epoch.\n",
        "        \n",
        "        Args:\n",
        "            epoch: Current epoch number\n",
        "            metrics: Dictionary of metrics\n",
        "            model_path: Path to model file to save\n",
        "            is_best: Whether this is the best model so far\n",
        "        \"\"\"\n",
        "        # Add epoch and timestamp to metrics\n",
        "        metrics['epoch'] = epoch\n",
        "        metrics['timestamp'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        \n",
        "        # Load existing CSV and append new row\n",
        "        df = pd.read_csv(self.csv_path) if self.csv_path.exists() else pd.DataFrame()\n",
        "        new_row = pd.DataFrame([metrics])\n",
        "        df = pd.concat([df, new_row], ignore_index=True)\n",
        "        \n",
        "        # Save to local CSV\n",
        "        df.to_csv(self.csv_path, index=False)\n",
        "        \n",
        "        # Save model if provided\n",
        "        if model_path and Path(model_path).exists():\n",
        "            # Save last model\n",
        "            shutil.copy2(model_path, self.last_model_local)\n",
        "            \n",
        "            # Save best model if applicable\n",
        "            if is_best:\n",
        "                shutil.copy2(model_path, self.best_model_local)\n",
        "                print(f'  [Checkpoint] New best model saved (epoch {epoch})')\n",
        "            \n",
        "            # Async save to Google Drive\n",
        "            if self.drive_dir:\n",
        "                self._async_save_to_drive(model_path, is_best)\n",
        "        \n",
        "        # Async save CSV to Google Drive\n",
        "        if self.drive_dir:\n",
        "            self._async_save_csv_to_drive()\n",
        "        \n",
        "        print(f'[Checkpoint] Progress saved for epoch {epoch}')\n",
        "    \n",
        "    def _async_save_to_drive(self, model_path, is_best):\n",
        "        \"\"\"Asynchronously save model to Google Drive.\"\"\"\n",
        "        def save_task():\n",
        "            try:\n",
        "                # Save last model\n",
        "                shutil.copy2(model_path, self.last_model_drive)\n",
        "                \n",
        "                # Save best model if applicable\n",
        "                if is_best:\n",
        "                    shutil.copy2(model_path, self.best_model_drive)\n",
        "                    \n",
        "                print(f'  [Checkpoint] Model backed up to Drive')\n",
        "            except Exception as e:\n",
        "                print(f'  [Checkpoint] Warning: Failed to save model to Drive: {e}')\n",
        "        \n",
        "        # Start async save\n",
        "        thread = threading.Thread(target=save_task, daemon=True)\n",
        "        thread.start()\n",
        "    \n",
        "    def _async_save_csv_to_drive(self):\n",
        "        \"\"\"Asynchronously save CSV to Google Drive.\"\"\"\n",
        "        def save_task():\n",
        "            try:\n",
        "                shutil.copy2(self.csv_path, self.csv_drive)\n",
        "            except Exception as e:\n",
        "                print(f'  [Checkpoint] Warning: Failed to save CSV to Drive: {e}')\n",
        "        \n",
        "        thread = threading.Thread(target=save_task, daemon=True)\n",
        "        thread.start()\n",
        "    \n",
        "    def get_last_checkpoint(self):\n",
        "        \"\"\"Get information about the last checkpoint.\"\"\"\n",
        "        info = {\n",
        "            'exists': False,\n",
        "            'last_epoch': 0,\n",
        "            'best_model_path': None,\n",
        "            'last_model_path': None\n",
        "        }\n",
        "        \n",
        "        # Check local first\n",
        "        if self.csv_path.exists():\n",
        "            df = pd.read_csv(self.csv_path)\n",
        "            if len(df) > 0:\n",
        "                info['exists'] = True\n",
        "                info['last_epoch'] = int(df['epoch'].iloc[-1])\n",
        "                \n",
        "                # Check for model files\n",
        "                if self.last_model_local.exists():\n",
        "                    info['last_model_path'] = str(self.last_model_local)\n",
        "                if self.best_model_local.exists():\n",
        "                    info['best_model_path'] = str(self.best_model_local)\n",
        "        \n",
        "        return info\n",
        "    \n",
        "    def load_progress(self):\n",
        "        \"\"\"Load training progress from CSV.\"\"\"\n",
        "        if self.csv_path.exists():\n",
        "            return pd.read_csv(self.csv_path)\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Initialize checkpoint manager\n",
        "experiment_name = f'refine_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n",
        "checkpoint_manager = CheckpointManager(\n",
        "    experiment_name=experiment_name,\n",
        "    local_checkpoint_dir=LOCAL_CHECKPOINTS_PATH,\n",
        "    drive_checkpoint_dir=None  # Will be set after mounting Drive\n",
        ")\n",
        "\n",
        "print('\\nCheckpoint system ready. CSV will be saved every epoch.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0e25dad",
      "metadata": {
        "id": "c0e25dad"
      },
      "source": [
        "## Mount Google Drive for Permanent Storage (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4557780b",
      "metadata": {
        "id": "4557780b"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive if available\n",
        "DRIVE_CHECKPOINT_DIR = None\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    \n",
        "    # Set up Drive checkpoint directory\n",
        "    DRIVE_CHECKPOINT_DIR = f'/content/drive/MyDrive/ocr_checkpoints'\n",
        "    os.makedirs(DRIVE_CHECKPOINT_DIR, exist_ok=True)\n",
        "    \n",
        "    # Update checkpoint manager with Drive directory\n",
        "    checkpoint_manager.drive_dir = Path(DRIVE_CHECKPOINT_DIR) / experiment_name\n",
        "    checkpoint_manager.drive_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    checkpoint_manager.best_model_drive = checkpoint_manager.drive_dir / 'best_model.pt'\n",
        "    checkpoint_manager.last_model_drive = checkpoint_manager.drive_dir / 'last_model.pt'\n",
        "    checkpoint_manager.csv_drive = checkpoint_manager.drive_dir / 'training_progress.csv'\n",
        "    \n",
        "    print(f'Google Drive mounted successfully')\n",
        "    print(f'Checkpoints will be saved to: {DRIVE_CHECKPOINT_DIR}')\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f'Note: Google Drive not mounted. Checkpoints will only be saved locally.')\n",
        "    print(f'Error: {e}')\n",
        "\n",
        "# Save training configuration\n",
        "training_config = {\n",
        "    'experiment_name': experiment_name,\n",
        "    'dataset_path': LOCAL_DATASET_PATH,\n",
        "    'pretrained_model': PRETRAINED_MODEL_PATH,\n",
        "    'data_yaml': DATA_YAML_PATH,\n",
        "    'device': device,\n",
        "    'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "}\n",
        "\n",
        "checkpoint_manager.save_config(training_config)\n",
        "print(f'Training configuration saved')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0e25dad-2",
      "metadata": {
        "id": "c0e25dad-2"
      },
      "source": [
        "## Core Components (Reused from Original Training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7000dfec-2",
      "metadata": {
        "id": "7000dfec-2"
      },
      "outputs": [],
      "source": [
        "# Character set and similarity matrix (from original)\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "CHARS = [chr(i) for i in range(65, 91)] + [str(i) for i in range(10)]\n",
        "NUM_CLASSES = len(CHARS)\n",
        "CHAR_TO_IDX = {c: i for i, c in enumerate(CHARS)}\n",
        "IDX_TO_CHAR = {i: c for i, c in enumerate(CHARS)}\n",
        "\n",
        "print(f'Number of classes: {NUM_CLASSES}')\n",
        "print(f'Characters: {\"\".join(CHARS)}')\n",
        "\n",
        "SIMILAR_GROUPS = [\n",
        "    ['O', '0'],\n",
        "    ['I', '1'],\n",
        "    ['S', '5'],\n",
        "    ['Z', '2'],\n",
        "    ['B', '8'],\n",
        "    ['D', '0'],\n",
        "    ['G', 'C'],\n",
        "    ['U', 'V'],\n",
        "    ['P', 'R'],\n",
        "]\n",
        "\n",
        "def create_similarity_matrix(num_classes=NUM_CLASSES, groups=SIMILAR_GROUPS, base_sim=0.6):\n",
        "    S = np.zeros((num_classes, num_classes), dtype=np.float32)\n",
        "    np.fill_diagonal(S, 1.0)\n",
        "    for group in groups:\n",
        "        idxs = [CHAR_TO_IDX[c] for c in group if c in CHAR_TO_IDX]\n",
        "        for i in idxs:\n",
        "            for j in idxs:\n",
        "                if i != j:\n",
        "                    S[i, j] = base_sim\n",
        "    return torch.tensor(S, dtype=torch.float32)\n",
        "\n",
        "similarity_matrix = create_similarity_matrix()\n",
        "print(f'Similarity matrix initialized: {similarity_matrix.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ebec263-2",
      "metadata": {
        "id": "0ebec263-2"
      },
      "outputs": [],
      "source": [
        "# Enhanced Similarity-Aware Loss with Adaptive Weighting\n",
        "class RefinedSimilarityAwareTopKLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Enhanced loss for fine-tuning with:\n",
        "    - Higher penalty for similar character confusion\n",
        "    - Adaptive temperature based on training phase\n",
        "    - Confidence-based weighting\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=NUM_CLASSES, similarity_matrix=None,\n",
        "                 k=3, initial_temperature=0.5, base_weight=0.5, topk_weight=0.5,\n",
        "                 epochs=40):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.k = k\n",
        "        self.initial_temperature = initial_temperature\n",
        "        self.base_weight = base_weight\n",
        "        self.topk_weight = topk_weight\n",
        "        self.epochs = epochs\n",
        "        self.current_epoch = 0\n",
        "\n",
        "        if similarity_matrix is not None:\n",
        "            self.register_buffer('similarity_matrix', similarity_matrix)\n",
        "        else:\n",
        "            self.register_buffer('similarity_matrix', create_similarity_matrix())\n",
        "\n",
        "    def update_epoch(self, epoch):\n",
        "        \"\"\"Update current epoch for temperature annealing.\"\"\"\n",
        "        self.current_epoch = epoch\n",
        "\n",
        "    def get_temperature(self):\n",
        "        \"\"\"Anneal temperature more aggressively for fine-tuning.\"\"\"\n",
        "        progress = self.current_epoch / max(self.epochs, 1)\n",
        "        # Start at 0.5, go to 0.3 (sharper predictions)\n",
        "        return max(0.3, self.initial_temperature - progress * 0.2)\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        B = logits.size(0)\n",
        "        device = logits.device\n",
        "\n",
        "        temperature = self.get_temperature()\n",
        "\n",
        "        # Standard cross-entropy\n",
        "        ce_loss = F.cross_entropy(logits, targets, reduction='none')\n",
        "\n",
        "        # Softmax with temperature\n",
        "        probs = F.softmax(logits / temperature, dim=1)\n",
        "        topk_probs, topk_indices = torch.topk(probs, min(self.k, self.num_classes), dim=1)\n",
        "\n",
        "        # Similarity-aware penalty\n",
        "        sim_loss = torch.zeros(B, device=device)\n",
        "        confidence_scores = []\n",
        "\n",
        "        for i in range(B):\n",
        "            t = targets[i].item()\n",
        "            if t < 0 or t >= self.num_classes:\n",
        "                continue\n",
        "\n",
        "            sims = self.similarity_matrix[t][topk_indices[i]]\n",
        "\n",
        "            # Higher penalty for similar character confusion\n",
        "            penalties = (1.0 - sims) * 1.5  # Amplify penalty\n",
        "            weighted_penalties = topk_probs[i] * penalties\n",
        "            sim_loss[i] = weighted_penalties.sum()\n",
        "\n",
        "            confidence_scores.append(topk_probs[i][0].item())\n",
        "\n",
        "        if len(confidence_scores) == 0:\n",
        "            return ce_loss.mean()\n",
        "\n",
        "        # Adaptive weighting based on confidence\n",
        "        confidence = torch.tensor(confidence_scores, device=device)\n",
        "\n",
        "        # When confident: rely more on CE (trust the model)\n",
        "        # When uncertain: rely more on similarity (guide the model)\n",
        "        adaptive_base = self.base_weight + (1 - confidence) * 0.2\n",
        "        adaptive_topk = self.topk_weight + confidence * 0.2\n",
        "\n",
        "        # Normalize\n",
        "        total_weight = adaptive_base + adaptive_topk\n",
        "        adaptive_base = adaptive_base / total_weight\n",
        "        adaptive_topk = adaptive_topk / total_weight\n",
        "\n",
        "        total_loss = adaptive_base * ce_loss + adaptive_topk * sim_loss\n",
        "        return total_loss.mean()\n",
        "\n",
        "print('Refined similarity-aware loss defined')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4557780b-2",
      "metadata": {
        "id": "4557780b-2"
      },
      "outputs": [],
      "source": [
        "# OCR Metrics (reused from original)\n",
        "class OCRMetrics:\n",
        "    \"\"\"Compute OCR-specific validation metrics.\"\"\"\n",
        "    def __init__(self, similarity_matrix=None):\n",
        "        self.similarity_matrix = similarity_matrix if similarity_matrix is not None else create_similarity_matrix()\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.total_chars = 0\n",
        "        self.correct_chars = 0\n",
        "        self.top2_correct = 0\n",
        "        self.top3_correct = 0\n",
        "        self.similarity_score = 0.0\n",
        "\n",
        "    def update(self, predictions, targets, top_k_preds=None):\n",
        "        predictions = predictions.cpu().numpy()\n",
        "        targets = targets.cpu().numpy()\n",
        "\n",
        "        self.total_chars += len(targets)\n",
        "        self.correct_chars += (predictions == targets).sum()\n",
        "\n",
        "        # Similarity-aware accuracy\n",
        "        for pred, target in zip(predictions, targets):\n",
        "            if 0 <= target < len(self.similarity_matrix) and 0 <= pred < len(self.similarity_matrix):\n",
        "                sim = self.similarity_matrix[target][pred].item()\n",
        "                self.similarity_score += sim\n",
        "\n",
        "        # Top-k accuracy\n",
        "        if top_k_preds is not None:\n",
        "            top_k_preds = top_k_preds.cpu().numpy()\n",
        "            for i, target in enumerate(targets):\n",
        "                if top_k_preds.shape[1] >= 2 and target in top_k_preds[i, :2]:\n",
        "                    self.top2_correct += 1\n",
        "                if top_k_preds.shape[1] >= 3 and target in top_k_preds[i, :3]:\n",
        "                    self.top3_correct += 1\n",
        "\n",
        "    def compute(self):\n",
        "        if self.total_chars == 0:\n",
        "            return {}\n",
        "\n",
        "        return {\n",
        "            'ocr_char_accuracy': self.correct_chars / self.total_chars,\n",
        "            'ocr_top2_accuracy': self.top2_correct / self.total_chars,\n",
        "            'ocr_top3_accuracy': self.top3_correct / self.total_chars,\n",
        "            'ocr_similarity_aware_accuracy': self.similarity_score / self.total_chars,\n",
        "        }\n",
        "\n",
        "print('OCR metrics module loaded')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0e25dad-3",
      "metadata": {
        "id": "c0e25dad-3"
      },
      "source": [
        "## Enhanced Trainer with Checkpoint Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4287274-2",
      "metadata": {
        "id": "e4287274-2"
      },
      "outputs": [],
      "source": [
        "# Custom Trainer for Refined Training\n",
        "from ultralytics.models.yolo.segment import SegmentationTrainer\n",
        "from ultralytics import YOLO\n",
        "\n",
        "class RefinedSegmentationTrainer(SegmentationTrainer):\n",
        "    \"\"\"\n",
        "    Refined trainer with:\n",
        "    - Progressive layer unfreezing\n",
        "    - Enhanced loss function\n",
        "    - OCR-specific metrics tracking\n",
        "    - Integrated checkpoint management\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, cfg=None, overrides=None, _callbacks=None, checkpoint_manager=None):\n",
        "        super().__init__(cfg, overrides, _callbacks)\n",
        "        \n",
        "        self.checkpoint_manager = checkpoint_manager\n",
        "        \n",
        "        # Get total epochs from config\n",
        "        total_epochs = self.args.epochs if hasattr(self.args, 'epochs') else 40\n",
        "\n",
        "        # Initialize refined loss\n",
        "        self.character_loss_fn = RefinedSimilarityAwareTopKLoss(\n",
        "            num_classes=NUM_CLASSES,\n",
        "            similarity_matrix=similarity_matrix,\n",
        "            k=3,\n",
        "            initial_temperature=0.5,\n",
        "            base_weight=0.5,\n",
        "            topk_weight=0.5,\n",
        "            epochs=total_epochs\n",
        "        ).to(device)\n",
        "\n",
        "        # OCR metrics\n",
        "        self.ocr_metrics = OCRMetrics(similarity_matrix=similarity_matrix)\n",
        "\n",
        "        # Training phase tracking\n",
        "        self.phase = 1\n",
        "        self.freeze_applied = False\n",
        "        \n",
        "        # Track best loss for checkpoint saving\n",
        "        self.best_cls_loss = float('inf')\n",
        "        \n",
        "        print(f'Trainer initialized with {total_epochs} total epochs')\n",
        "        \n",
        "    def _setup_train(self, world_size):\n",
        "        \"\"\"Override to apply layer freezing for Phase 1.\"\"\"\n",
        "        super()._setup_train(world_size)\n",
        "\n",
        "        if not self.freeze_applied and self.epoch < 12:\n",
        "            print(f'\\nPHASE 1: Classifier Head Fine-Tuning (Epochs 1-12)')\n",
        "            print('Freezing backbone and segmentation layers...')\n",
        "\n",
        "            # Freeze all layers except classification head\n",
        "            for name, param in self.model.named_parameters():\n",
        "                # Keep classification layers trainable\n",
        "                if 'cls' in name.lower() or 'cv3' in name.lower():\n",
        "                    param.requires_grad = True\n",
        "                else:\n",
        "                    param.requires_grad = False\n",
        "\n",
        "            trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "            total = sum(p.numel() for p in self.model.parameters())\n",
        "            print(f'Trainable parameters: {trainable:,} / {total:,} ({100*trainable/total:.1f}%)')\n",
        "            self.freeze_applied = True\n",
        "\n",
        "    def on_train_epoch_start(self):\n",
        "        \"\"\"Handle phase transitions and progressive unfreezing.\"\"\"\n",
        "        super().on_train_epoch_start()\n",
        "\n",
        "        # Update temperature in loss\n",
        "        self.character_loss_fn.update_epoch(self.epoch)\n",
        "\n",
        "        # Phase 2: Progressive unfreezing (epochs 12-24)\n",
        "        if self.epoch == 12:\n",
        "            self.phase = 2\n",
        "            print(f'\\nPHASE 2: Progressive Unfreezing (Epochs 13-24)')\n",
        "            print('Unfreezing segmentation head...')\n",
        "\n",
        "            for name, param in self.model.named_parameters():\n",
        "                if 'seg' in name.lower() or 'mask' in name.lower():\n",
        "                    param.requires_grad = True\n",
        "\n",
        "            trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "            total = sum(p.numel() for p in self.model.parameters())\n",
        "            print(f'Trainable parameters: {trainable:,} / {total:,} ({100*trainable/total:.1f}%)')\n",
        "\n",
        "        # Phase 3: Full fine-tuning (epochs 24+)\n",
        "        elif self.epoch == 24:\n",
        "            self.phase = 3\n",
        "            print(f'\\nPHASE 3: Full Fine-Tuning (Epochs 25-40)')\n",
        "            print('Unfreezing all layers...')\n",
        "\n",
        "            for param in self.model.parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "            trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "            total = sum(p.numel() for p in self.model.parameters())\n",
        "            print(f'Trainable parameters: {trainable:,} / {total:,} ({100*trainable/total:.1f}%)')\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        \"\"\"Save checkpoint after each epoch.\"\"\"\n",
        "        super().on_train_epoch_end()\n",
        "        \n",
        "        if self.checkpoint_manager:\n",
        "            # Collect metrics\n",
        "            metrics = self._collect_metrics()\n",
        "            \n",
        "            # Determine if this is the best model\n",
        "            current_cls_loss = metrics.get('val/cls_loss', float('inf'))\n",
        "            is_best = current_cls_loss < self.best_cls_loss\n",
        "            \n",
        "            if is_best:\n",
        "                self.best_cls_loss = current_cls_loss\n",
        "            \n",
        "            # Save progress\n",
        "            model_path = Path(self.save_dir) / 'weights' / 'last.pt'\n",
        "            self.checkpoint_manager.save_progress(\n",
        "                epoch=self.epoch,\n",
        "                metrics=metrics,\n",
        "                model_path=str(model_path) if model_path.exists() else None,\n",
        "                is_best=is_best\n",
        "            )\n",
        "\n",
        "    def _collect_metrics(self):\n",
        "        \"\"\"Collect metrics from training.\"\"\"\n",
        "        metrics = {\n",
        "            'phase': self.phase,\n",
        "            'learning_rate': self.optimizer.param_groups[0]['lr'] if hasattr(self, 'optimizer') else 0.0,\n",
        "        }\n",
        "        \n",
        "        # Get loss values\n",
        "        if hasattr(self, 'loss_items') and self.loss_items is not None:\n",
        "            if len(self.loss_items) > 0:\n",
        "                metrics['train/cls_loss'] = float(self.loss_items[0])\n",
        "            if len(self.loss_items) > 1:\n",
        "                metrics['train/seg_loss'] = float(self.loss_items[1])\n",
        "            if len(self.loss_items) > 2:\n",
        "                metrics['train/box_loss'] = float(self.loss_items[2])\n",
        "        \n",
        "        # Get validation metrics from validator\n",
        "        if hasattr(self, 'validator') and hasattr(self.validator, 'metrics'):\n",
        "            val_metrics = self.validator.metrics\n",
        "            if hasattr(val_metrics, 'results_dict'):\n",
        "                val_dict = val_metrics.results_dict\n",
        "                metrics.update({\n",
        "                    'val/cls_loss': val_dict.get('val/cls_loss', 0.0),\n",
        "                    'val/seg_loss': val_dict.get('val/seg_loss', 0.0),\n",
        "                    'val/box_loss': val_dict.get('val/box_loss', 0.0),\n",
        "                    'metrics/precision(M)': val_dict.get('metrics/precision(M)', 0.0),\n",
        "                    'metrics/recall(M)': val_dict.get('metrics/recall(M)', 0.0),\n",
        "                    'metrics/mAP50(M)': val_dict.get('metrics/mAP50(M)', 0.0),\n",
        "                    'metrics/mAP50-95(M)': val_dict.get('metrics/mAP50-95(M)', 0.0),\n",
        "                })\n",
        "        \n",
        "        # Add OCR metrics\n",
        "        ocr_results = self.ocr_metrics.compute()\n",
        "        metrics.update(ocr_results)\n",
        "        \n",
        "        return metrics\n",
        "\n",
        "    def on_val_start(self):\n",
        "        super().on_val_start()\n",
        "        self.ocr_metrics.reset()\n",
        "\n",
        "    def on_val_end(self):\n",
        "        super().on_val_end()\n",
        "\n",
        "        # Log OCR metrics\n",
        "        ocr_results = self.ocr_metrics.compute()\n",
        "        if ocr_results:\n",
        "            print(f'\\n[Epoch {self.epoch}] OCR Metrics:')\n",
        "            for key, value in ocr_results.items():\n",
        "                print(f'  {key}: {value:.4f}')\n",
        "\n",
        "    def compute_loss(self, preds, batch):\n",
        "        \"\"\"Compute loss with refined similarity-aware classification.\"\"\"\n",
        "        # Get base YOLO losses\n",
        "        base_loss = super().compute_loss(preds, batch)\n",
        "\n",
        "        # Add custom similarity-aware character classification loss\n",
        "        if len(preds) > 3:\n",
        "            cls_logits = preds[3]\n",
        "            cls_targets = batch['cls'].long()\n",
        "\n",
        "            if cls_logits is not None and cls_targets is not None:\n",
        "                cls_logits_flat = cls_logits.view(-1, NUM_CLASSES)\n",
        "                cls_targets_flat = cls_targets.view(-1)\n",
        "\n",
        "                valid_mask = cls_targets_flat >= 0\n",
        "                if valid_mask.sum() > 0:\n",
        "                    # Compute refined similarity-aware loss\n",
        "                    char_loss = self.character_loss_fn(\n",
        "                        cls_logits_flat[valid_mask],\n",
        "                        cls_targets_flat[valid_mask]\n",
        "                    )\n",
        "\n",
        "                    # Update OCR metrics\n",
        "                    with torch.no_grad():\n",
        "                        preds_cls = cls_logits_flat[valid_mask].argmax(dim=1)\n",
        "                        top_k_preds = torch.topk(cls_logits_flat[valid_mask], k=3, dim=1)[1]\n",
        "                        self.ocr_metrics.update(\n",
        "                            preds_cls,\n",
        "                            cls_targets_flat[valid_mask],\n",
        "                            top_k_preds\n",
        "                        )\n",
        "\n",
        "                    # Phase-dependent weighting\n",
        "                    if self.phase == 1:\n",
        "                        # Phase 1: Heavy emphasis on classification\n",
        "                        cls_weight = 0.7\n",
        "                    elif self.phase == 2:\n",
        "                        # Phase 2: Balanced\n",
        "                        cls_weight = 0.5\n",
        "                    else:\n",
        "                        # Phase 3: Standard weighting\n",
        "                        cls_weight = 0.3\n",
        "\n",
        "                    total_loss = (1 - cls_weight) * base_loss + cls_weight * char_loss\n",
        "                    return total_loss\n",
        "\n",
        "        return base_loss\n",
        "\n",
        "print('Refined segmentation trainer defined with checkpoint integration')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b00e62a",
      "metadata": {
        "id": "0b00e62a"
      },
      "source": [
        "## Load Pretrained Model and Configure Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4b8d469",
      "metadata": {
        "id": "b4b8d469"
      },
      "outputs": [],
      "source": [
        "# Load model with resume capability\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Check if we should resume from checkpoint\n",
        "checkpoint_info = checkpoint_manager.get_last_checkpoint()\n",
        "\n",
        "if checkpoint_info['exists'] and checkpoint_info['last_model_path']:\n",
        "    print(f'Found previous checkpoint at epoch {checkpoint_info[\"last_epoch\"]}')\n",
        "    print(f'Last model: {checkpoint_info[\"last_model_path\"]}')\n",
        "    \n",
        "    # Ask user if they want to resume\n",
        "    resume_choice = input('Do you want to resume training from checkpoint? (y/n): ').lower().strip()\n",
        "    \n",
        "    if resume_choice == 'y':\n",
        "        model_path = checkpoint_info['last_model_path']\n",
        "        print(f'Resuming from checkpoint: {model_path}')\n",
        "        RESUME_TRAINING = True\n",
        "    else:\n",
        "        model_path = PRETRAINED_MODEL_PATH\n",
        "        print(f'Starting fresh training from: {model_path}')\n",
        "        RESUME_TRAINING = False\n",
        "else:\n",
        "    model_path = PRETRAINED_MODEL_PATH\n",
        "    RESUME_TRAINING = False\n",
        "    print(f'No checkpoint found. Starting fresh training from: {model_path}')\n",
        "\n",
        "# Load the model\n",
        "if model_path and os.path.exists(model_path):\n",
        "    try:\n",
        "        model = YOLO(model_path)\n",
        "        print(f'Model loaded successfully')\n",
        "    except Exception as e:\n",
        "        print(f'ERROR loading model {model_path}: {e}')\n",
        "        print('Loading default YOLO model...')\n",
        "        model = YOLO('yolo11n-seg.pt')\n",
        "else:\n",
        "    print(f'Model path not found: {model_path}')\n",
        "    print('Loading default YOLO model...')\n",
        "    model = YOLO('yolo11n-seg.pt')\n",
        "\n",
        "# Set custom trainer\n",
        "model.trainer = RefinedSegmentationTrainer\n",
        "\n",
        "print('\\nModel ready for training')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77ae0242",
      "metadata": {
        "id": "77ae0242"
      },
      "source": [
        "## Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bada4767",
      "metadata": {
        "id": "bada4767"
      },
      "outputs": [],
      "source": [
        "# Refined training hyperparameters\n",
        "REFINE_EPOCHS = 40\n",
        "BATCH_SIZE = 16\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# Learning rate schedule\n",
        "LR0 = 0.005\n",
        "LRF = 0.0001\n",
        "\n",
        "# Optimizer settings\n",
        "MOMENTUM = 0.937\n",
        "WEIGHT_DECAY = 5e-4\n",
        "WARMUP_EPOCHS = 3.0\n",
        "\n",
        "# Augmentations\n",
        "AUG_HSV_H = 0.02\n",
        "AUG_HSV_S = 0.8\n",
        "AUG_HSV_V = 0.5\n",
        "AUG_ERASING = 0.5\n",
        "AUG_DEGREES = 5.0\n",
        "AUG_SHEAR = 2.0\n",
        "\n",
        "# Disabled augmentations (not useful for OCR)\n",
        "AUG_FLIPLR = 0.0\n",
        "AUG_MOSAIC = 0.0\n",
        "AUG_MIXUP = 0.0\n",
        "\n",
        "print('Refined Training Configuration:')\n",
        "print(f'  Epochs: {REFINE_EPOCHS}')\n",
        "print(f'  Batch size: {BATCH_SIZE}')\n",
        "print(f'  Learning rate: {LR0} -> {LRF}')\n",
        "print(f'  Augmentations: Enhanced HSV + Erasing + Geometric')\n",
        "print(f'\\nTraining Strategy:')\n",
        "print(f'  Phase 1 (1-12): Classifier head only')\n",
        "print(f'  Phase 2 (13-24): + Segmentation head')\n",
        "print(f'  Phase 3 (25-40): All layers')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7eeb804e",
      "metadata": {
        "id": "7eeb804e"
      },
      "source": [
        "## Execute Refined Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cc4bac1",
      "metadata": {
        "id": "8cc4bac1"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "# Training parameters\n",
        "train_params = dict(\n",
        "    data=DATA_YAML_PATH,\n",
        "    epochs=REFINE_EPOCHS,\n",
        "    batch=BATCH_SIZE,\n",
        "    imgsz=IMG_SIZE,\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer='SGD',\n",
        "    lr0=LR0,\n",
        "    lrf=LRF,\n",
        "    momentum=MOMENTUM,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "\n",
        "    # Warmup\n",
        "    warmup_epochs=WARMUP_EPOCHS,\n",
        "    warmup_momentum=0.8,\n",
        "    warmup_bias_lr=0.1,\n",
        "\n",
        "    # Augmentations\n",
        "    hsv_h=AUG_HSV_H,\n",
        "    hsv_s=AUG_HSV_S,\n",
        "    hsv_v=AUG_HSV_V,\n",
        "    erasing=AUG_ERASING,\n",
        "    degrees=AUG_DEGREES,\n",
        "    shear=AUG_SHEAR,\n",
        "    fliplr=AUG_FLIPLR,\n",
        "    mosaic=AUG_MOSAIC,\n",
        "    mixup=AUG_MIXUP,\n",
        "\n",
        "    # Output settings\n",
        "    project='refined_training',\n",
        "    name=experiment_name,\n",
        "    exist_ok=True,\n",
        "\n",
        "    # Validation and saving\n",
        "    val=True,\n",
        "    save=True,\n",
        "    save_period=5,  # Save local copy every 5 epochs\n",
        "\n",
        "    # System\n",
        "    device=device,\n",
        "    amp=True,\n",
        "    seed=42,\n",
        "    deterministic=True,\n",
        "\n",
        "    # Resume handling\n",
        "    resume=RESUME_TRAINING,\n",
        ")\n",
        "\n",
        "# Create a custom trainer instance with checkpoint manager\n",
        "trainer = RefinedSegmentationTrainer(\n",
        "    cfg=train_params,\n",
        "    checkpoint_manager=checkpoint_manager\n",
        ")\n",
        "\n",
        "print(f'\\n{'='*80}')\n",
        "if RESUME_TRAINING:\n",
        "    print(f'RESUMING TRAINING FROM CHECKPOINT')\n",
        "else:\n",
        "    print(f'STARTING FRESH TRAINING')\n",
        "print(f'{'='*80}\\n')\n",
        "print(f'Start time: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}')\n",
        "print(f'Device: {device}')\n",
        "print(f'Experiment: {experiment_name}')\n",
        "print(f'Checkpoint CSV: {checkpoint_manager.csv_path}')\n",
        "if checkpoint_manager.drive_dir:\n",
        "    print(f'Drive backup: {checkpoint_manager.drive_dir}')\n",
        "print()\n",
        "\n",
        "# Execute training\n",
        "try:\n",
        "    results = trainer.train(model)\n",
        "    \n",
        "    print(f'\\n{'='*80}')\n",
        "    print(f'TRAINING COMPLETED SUCCESSFULLY')\n",
        "    print(f'{'='*80}\\n')\n",
        "    print(f'End time: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}')\n",
        "    \n",
        "except KeyboardInterrupt:\n",
        "    print(f'\\nTraining interrupted by user')\n",
        "    print(f'Progress saved to: {checkpoint_manager.csv_path}')\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f'\\nTraining failed with error: {e}')\n",
        "    print(f'Progress saved to: {checkpoint_manager.csv_path}')\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9dbebfb",
      "metadata": {
        "id": "b9dbebfb"
      },
      "source": [
        "## Save Final Models and Export Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "507bd7ec",
      "metadata": {
        "id": "507bd7ec"
      },
      "outputs": [],
      "source": [
        "# Final export of models\n",
        "print('\\n=== FINAL MODEL EXPORT ===')\n",
        "\n",
        "# Get paths from trainer\n",
        "if 'trainer' in locals() and hasattr(trainer, 'save_dir'):\n",
        "    results_dir = trainer.save_dir\n",
        "    \n",
        "    # Find best and last models\n",
        "    best_model_local = Path(results_dir) / 'weights' / 'best.pt'\n",
        "    last_model_local = Path(results_dir) / 'weights' / 'last.pt'\n",
        "    \n",
        "    # Check if models exist\n",
        "    models_found = []\n",
        "    \n",
        "    if best_model_local.exists():\n",
        "        models_found.append(('best', best_model_local))\n",
        "        \n",
        "    if last_model_local.exists():\n",
        "        models_found.append(('last', last_model_local))\n",
        "    \n",
        "    # Also check checkpoint manager\n",
        "    if checkpoint_manager.best_model_local.exists():\n",
        "        models_found.append(('checkpoint_best', checkpoint_manager.best_model_local))\n",
        "    \n",
        "    if checkpoint_manager.last_model_local.exists():\n",
        "        models_found.append(('checkpoint_last', checkpoint_manager.last_model_local))\n",
        "    \n",
        "    # Copy models to final export directory\n",
        "    export_dir = Path(LOCAL_ROOT) / 'final_models' / experiment_name\n",
        "    export_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    for model_name, model_path in models_found:\n",
        "        export_path = export_dir / f'{model_name}.pt'\n",
        "        shutil.copy2(model_path, export_path)\n",
        "        print(f'Exported {model_name} model to: {export_path}')\n",
        "        \n",
        "        # Also copy to Drive if available\n",
        "        if checkpoint_manager.drive_dir:\n",
        "            drive_export_path = checkpoint_manager.drive_dir / 'final_models' / f'{model_name}.pt'\n",
        "            drive_export_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            shutil.copy2(model_path, drive_export_path)\n",
        "            print(f'  Backup to Drive: {drive_export_path}')\n",
        "    \n",
        "    # Export results CSV\n",
        "    results_csv = Path(results_dir) / 'results.csv'\n",
        "    if results_csv.exists():\n",
        "        export_csv = export_dir / 'training_results.csv'\n",
        "        shutil.copy2(results_csv, export_csv)\n",
        "        print(f'\\nExported results CSV to: {export_csv}')\n",
        "    \n",
        "    # Also export our checkpoint CSV\n",
        "    if checkpoint_manager.csv_path.exists():\n",
        "        checkpoint_csv = export_dir / 'checkpoint_progress.csv'\n",
        "        shutil.copy2(checkpoint_manager.csv_path, checkpoint_csv)\n",
        "        print(f'Exported checkpoint CSV to: {checkpoint_csv}')\n",
        "    \n",
        "    print(f'\\nAll models and results exported to: {export_dir}')\n",
        "\n",
        "else:\n",
        "    print('Trainer not found. Using checkpoint manager models.')\n",
        "    \n",
        "    # Export from checkpoint manager\n",
        "    export_dir = Path(LOCAL_ROOT) / 'final_models' / experiment_name\n",
        "    export_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    if checkpoint_manager.best_model_local.exists():\n",
        "        export_path = export_dir / 'best_model.pt'\n",
        "        shutil.copy2(checkpoint_manager.best_model_local, export_path)\n",
        "        print(f'Exported best model to: {export_path}')\n",
        "    \n",
        "    if checkpoint_manager.last_model_local.exists():\n",
        "        export_path = export_dir / 'last_model.pt'\n",
        "        shutil.copy2(checkpoint_manager.last_model_local, export_path)\n",
        "        print(f'Exported last model to: {export_path}')\n",
        "    \n",
        "    if checkpoint_manager.csv_path.exists():\n",
        "        export_csv = export_dir / 'training_progress.csv'\n",
        "        shutil.copy2(checkpoint_manager.csv_path, export_csv)\n",
        "        print(f'\\nExported progress CSV to: {export_csv}')\n",
        "\n",
        "print('\\n=== EXPORT COMPLETE ===')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ebcce99",
      "metadata": {
        "id": "6ebcce99"
      },
      "source": [
        "## Performance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "208e10e5",
      "metadata": {
        "id": "208e10e5"
      },
      "outputs": [],
      "source": [
        "# Load and analyze training progress\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load progress from checkpoint manager\n",
        "progress_df = checkpoint_manager.load_progress()\n",
        "\n",
        "if not progress_df.empty:\n",
        "    print('\\n=== TRAINING PERFORMANCE ANALYSIS ===')\n",
        "    print(f'Total epochs completed: {len(progress_df)}')\n",
        "    \n",
        "    # Get final metrics\n",
        "    final_row = progress_df.iloc[-1]\n",
        "    \n",
        "    print('\\nFinal Epoch Metrics:')\n",
        "    print('-' * 80)\n",
        "    \n",
        "    # Classification metrics\n",
        "    if 'val/cls_loss' in progress_df.columns:\n",
        "        final_cls_loss = final_row['val/cls_loss']\n",
        "        best_cls_loss = progress_df['val/cls_loss'].min()\n",
        "        best_cls_epoch = progress_df['val/cls_loss'].idxmin() + 1\n",
        "        \n",
        "        print(f'Classification Loss:')\n",
        "        print(f'  Final: {final_cls_loss:.4f}')\n",
        "        print(f'  Best:  {best_cls_loss:.4f} (epoch {best_cls_epoch})')\n",
        "    \n",
        "    # OCR metrics\n",
        "    if 'ocr_char_accuracy' in progress_df.columns:\n",
        "        final_ocr_acc = final_row['ocr_char_accuracy']\n",
        "        best_ocr_acc = progress_df['ocr_char_accuracy'].max()\n",
        "        best_ocr_epoch = progress_df['ocr_char_accuracy'].idxmax() + 1\n",
        "        \n",
        "        print(f'\\nOCR Character Accuracy:')\n",
        "        print(f'  Final: {final_ocr_acc:.4f} ({final_ocr_acc*100:.1f}%)')\n",
        "        print(f'  Best:  {best_ocr_acc:.4f} ({best_ocr_acc*100:.1f}%) (epoch {best_ocr_epoch})')\n",
        "    \n",
        "    # Segmentation metrics\n",
        "    if 'metrics/mAP50-95(M)' in progress_df.columns:\n",
        "        final_map = final_row['metrics/mAP50-95(M)']\n",
        "        best_map = progress_df['metrics/mAP50-95(M)'].max()\n",
        "        best_map_epoch = progress_df['metrics/mAP50-95(M)'].idxmax() + 1\n",
        "        \n",
        "        print(f'\\nSegmentation mAP@50-95:')\n",
        "        print(f'  Final: {final_map:.4f}')\n",
        "        print(f'  Best:  {best_map:.4f} (epoch {best_map_epoch})')\n",
        "    \n",
        "    # Plot training curves\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    fig.suptitle(f'Training Performance - {experiment_name}', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # Plot 1: Classification Loss\n",
        "    if 'val/cls_loss' in progress_df.columns:\n",
        "        ax = axes[0, 0]\n",
        "        epochs = progress_df['epoch']\n",
        "        \n",
        "        if 'train/cls_loss' in progress_df.columns:\n",
        "            ax.plot(epochs, progress_df['train/cls_loss'], label='Train', linewidth=2, alpha=0.7)\n",
        "        \n",
        "        ax.plot(epochs, progress_df['val/cls_loss'], label='Validation', linewidth=2)\n",
        "        \n",
        "        # Mark phase transitions\n",
        "        ax.axvline(x=12, color='gray', linestyle=':', alpha=0.5, label='Phase 1→2')\n",
        "        ax.axvline(x=24, color='gray', linestyle=':', alpha=0.5, label='Phase 2→3')\n",
        "        \n",
        "        ax.set_xlabel('Epoch', fontweight='bold')\n",
        "        ax.set_ylabel('Loss', fontweight='bold')\n",
        "        ax.set_title('Classification Loss', fontweight='bold')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        ax.legend()\n",
        "    \n",
        "    # Plot 2: OCR Accuracy\n",
        "    if 'ocr_char_accuracy' in progress_df.columns:\n",
        "        ax = axes[0, 1]\n",
        "        ax.plot(epochs, progress_df['ocr_char_accuracy'] * 100, label='Char Accuracy', linewidth=2)\n",
        "        \n",
        "        if 'ocr_top2_accuracy' in progress_df.columns:\n",
        "            ax.plot(epochs, progress_df['ocr_top2_accuracy'] * 100, label='Top-2 Accuracy', linewidth=2, alpha=0.7)\n",
        "        \n",
        "        if 'ocr_top3_accuracy' in progress_df.columns:\n",
        "            ax.plot(epochs, progress_df['ocr_top3_accuracy'] * 100, label='Top-3 Accuracy', linewidth=2, alpha=0.5)\n",
        "        \n",
        "        ax.axvline(x=12, color='gray', linestyle=':', alpha=0.5)\n",
        "        ax.axvline(x=24, color='gray', linestyle=':', alpha=0.5)\n",
        "        \n",
        "        ax.set_xlabel('Epoch', fontweight='bold')\n",
        "        ax.set_ylabel('Accuracy (%)', fontweight='bold')\n",
        "        ax.set_title('OCR Performance', fontweight='bold')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        ax.legend()\n",
        "        ax.set_ylim([0, 105])\n",
        "    \n",
        "    # Plot 3: Segmentation mAP\n",
        "    if 'metrics/mAP50-95(M)' in progress_df.columns:\n",
        "        ax = axes[1, 0]\n",
        "        ax.plot(epochs, progress_df['metrics/mAP50-95(M)'], linewidth=2, color='green')\n",
        "        \n",
        "        ax.axvline(x=12, color='gray', linestyle=':', alpha=0.5)\n",
        "        ax.axvline(x=24, color='gray', linestyle=':', alpha=0.5)\n",
        "        \n",
        "        ax.set_xlabel('Epoch', fontweight='bold')\n",
        "        ax.set_ylabel('mAP@50-95', fontweight='bold')\n",
        "        ax.set_title('Segmentation Quality', fontweight='bold')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        ax.set_ylim([0, 1])\n",
        "    \n",
        "    # Plot 4: Learning Rate\n",
        "    if 'learning_rate' in progress_df.columns:\n",
        "        ax = axes[1, 1]\n",
        "        ax.plot(epochs, progress_df['learning_rate'], linewidth=2, color='purple')\n",
        "        \n",
        "        ax.axvline(x=12, color='gray', linestyle=':', alpha=0.5)\n",
        "        ax.axvline(x=24, color='gray', linestyle=':', alpha=0.5)\n",
        "        \n",
        "        ax.set_xlabel('Epoch', fontweight='bold')\n",
        "        ax.set_ylabel('Learning Rate', fontweight='bold')\n",
        "        ax.set_title('Learning Rate Schedule', fontweight='bold')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        ax.set_yscale('log')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Save plot\n",
        "    plot_path = export_dir / 'training_analysis.png'\n",
        "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
        "    print(f'\\nTraining analysis plot saved to: {plot_path}')\n",
        "    \n",
        "    plt.show()\n",
        "    \n",
        "else:\n",
        "    print('No training progress data found.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary",
      "metadata": {
        "id": "summary"
      },
      "source": [
        "### Key Improvements:\n",
        "\n",
        "1. **Reproducibility**:\n",
        "   - Dataset downloaded from public Google Drive folder\n",
        "   - No hardcoded paths - works from any account\n",
        "   - Automatic dataset verification and fixing\n",
        "   \n",
        "2. **Robust Checkpoint System**:\n",
        "   - CSV saved EVERY epoch (no exceptions)\n",
        "   - Models saved to Google Drive every epoch (if mounted)\n",
        "   - Automatic resume capability\n",
        "   - Threaded async saves to prevent training slowdown\n",
        "   \n",
        "3. **Clean Output**:\n",
        "   - Clear, professional logging\n",
        "   \n",
        "4. **Sanity Checks**:\n",
        "   - Dataset structure verification\n",
        "   - File existence checks\n",
        "   - Model loading error handling\n",
        "   - Automatic path fixing in data.yaml"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
