{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90f6d1e2",
   "metadata": {},
   "source": [
    "# Philippine License Plate Character Instance Segmentation with Similarity-Aware Loss\n",
    "\n",
    "Single-stage training: YOLO11-seg with polygon masks and character labels, using a custom similarity-aware loss function to handle visually confusable characters (O/0, I/1/L, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765eb443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before using notebook, run .venv/Scripts/activate\n",
    "\n",
    "# if no venv exists, create one using the following commands:\n",
    "# python -m venv .venv\n",
    "# .venv\\Scripts\\activate\n",
    "# pip install ultralytics opencv-python-headless pillow pyyaml numpy scipy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1be0f6b",
   "metadata": {},
   "source": [
    "## 1. Paths and Configuration Variables\n",
    "\n",
    "Set these to the actual dataset and output locations before training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93f34f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_YAML_PATH: dataset/data.yaml\n",
      "Full path: c:\\Users\\lifei\\OneDrive\\Desktop\\CSC173 - Intelligent Systems\\CSC173-DeepCV-Sanchez\\dataset\\data.yaml\n",
      "EXPORT_DIR: exports\n",
      "Dataset exists: True\n"
     ]
    }
   ],
   "source": [
    "# update this path with the actual dataset location\n",
    "\n",
    "DATA_YAML_PATH = 'dataset/data.yaml'\n",
    "\n",
    "RUN_PROJECT = 'philippine_lp_ocr'\n",
    "RUN_NAME = 'seg_with_similarity_loss'\n",
    "EXPORT_DIR = 'exports'\n",
    "\n",
    "import os\n",
    "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "\n",
    "print('DATA_YAML_PATH:', DATA_YAML_PATH)\n",
    "print('Full path:', os.path.abspath(DATA_YAML_PATH))\n",
    "print('EXPORT_DIR:', EXPORT_DIR)\n",
    "print('Dataset exists:', os.path.exists(DATA_YAML_PATH))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891aee20",
   "metadata": {},
   "source": [
    "## 2. Imports\n",
    "\n",
    "Core dependencies for segmentation training, custom loss, and optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cc0cd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "from ultralytics.models.yolo.segment import SegmentationTrainer\n",
    "from ultralytics.nn.tasks import SegmentationModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Use MPS (Metal Performance Shaders) for M4 Mac\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print('Using device:', device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3144851a",
   "metadata": {},
   "source": [
    "## 3. Character Set and Similarity Matrix\n",
    "\n",
    "Define the 36-class character set (A‚ÄìZ, 0‚Äì9) and visual-similarity relationships based on glyph shapes (determined manually). Characters in the same group (e.g., O, 0, Q) are visually similar and should receive reduced penalties when confused during training. This will help in reducing misclassification errors between characters that are inherently difficult to distinguish in low-quality CCTV footage or degraded license plates. By encoding prior knowledge of visual confusion patterns (e.g., O/0, I/1/L) into the similarity matrix, the model focuses its learning capacity on genuinely distinct characters while being more forgiving of ambiguous cases, leading to faster convergence and improved generalization on real-world noisy inputs [1].\n",
    "\n",
    "References:\n",
    "\n",
    "\n",
    "[1] [Ebrahimi Vargoorani, Z., & Suen, C. Y. (2024). License Plate Detection and Character Recognition Using Deep Learning and Font Evaluation. arXiv preprint arXiv:2412.12572.‚Äã](https://arxiv.org/abs/2412.12572)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12ad2046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 36\n",
      "Characters: ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "Similarity matrix shape: torch.Size([36, 36])\n"
     ]
    }
   ],
   "source": [
    "CHARS = [chr(i) for i in range(65, 91)] + [str(i) for i in range(10)]\n",
    "NUM_CLASSES = len(CHARS)\n",
    "CHAR_TO_IDX = {c: i for i, c in enumerate(CHARS)}\n",
    "IDX_TO_CHAR = {i: c for i, c in enumerate(CHARS)}\n",
    "\n",
    "print('Number of classes:', NUM_CLASSES)\n",
    "print('Characters:', CHARS)\n",
    "\n",
    "SIMILAR_GROUPS = [\n",
    "    ['O', '0', 'Q'],\n",
    "    ['I', '1', 'L'],\n",
    "    ['S', '5'],\n",
    "    ['Z', '2'],\n",
    "    ['B', '8'],\n",
    "    ['D', '0'],\n",
    "    ['G', 'C'],\n",
    "    ['U', 'V'],\n",
    "    ['P', 'R'],\n",
    "]\n",
    "\n",
    "def create_similarity_matrix(num_classes=NUM_CLASSES, groups=SIMILAR_GROUPS, base_sim=0.6):\n",
    "    S = np.zeros((num_classes, num_classes), dtype=np.float32)\n",
    "    np.fill_diagonal(S, 1.0)\n",
    "    for group in groups:\n",
    "        idxs = [CHAR_TO_IDX[c] for c in group if c in CHAR_TO_IDX]\n",
    "        for i in idxs:\n",
    "            for j in idxs:\n",
    "                if i != j:\n",
    "                    S[i, j] = base_sim\n",
    "    return torch.tensor(S, dtype=torch.float32)\n",
    "\n",
    "similarity_matrix = create_similarity_matrix()\n",
    "print('Similarity matrix shape:', similarity_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2bc7a1",
   "metadata": {},
   "source": [
    "### 3.1. Dynamic Similarity Matrix Updates\n",
    "\n",
    "The similarity matrix is initialized with hand-crafted visual similarities, but real-world confusion patterns may differ. By tracking which characters the model actually confuses during validation, we can dynamically update the similarity matrix to better reflect learned confusion patterns. This creates an adaptive training process where the loss function becomes more intelligent over time, focusing on the model's actual weak points rather than theoretical similarities.\n",
    "\n",
    "The system uses an exponential moving average to gradually incorporate observed confusions into the similarity scores, allowing the model to discover which character pairs are genuinely confusable in the dataset (e.g., if certain fonts make B and 8 more similar than expected). Exponential moving averages are widely used in deep learning to smoothly accumulate information over training steps while down-weighting older observations, providing a stable, noise‚Äërobust estimate of evolving quantities such as weights, statistics, or performance indicators. This data-driven refinement complements the initial manual similarity groupings and helps the model adapt to domain-specific challenges in Philippine license plates captured under varying CCTV conditions.‚Äã\n",
    "\n",
    "Saxena et al. [2] show that a similarity matrix between classes closely corresponds to the empirical confusion matrix of a trained network, and that higher similarity leads to more frequent confusions, indicating that similarity and confusion co-evolve during learning. This supports the idea of maintaining and updating a similarity matrix in tandem with observed confusions to better track which classes are genuinely hard to distinguish for the model.‚Äã\n",
    "\n",
    "References:\n",
    "\n",
    "\n",
    "[2] [Saxena, R., et al. (2022). Learning in deep neural networks and brains with similarity-weighted interleaved learning. Proceedings of the National Academy of Sciences.](https://www.pnas.org/doi/10.1073/pnas.2115229119)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4e7a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamic similarity matrix manager initialized.\n",
      "Will update every validation epoch based on actual confusion patterns.\n"
     ]
    }
   ],
   "source": [
    "class DynamicSimilarityMatrix:\n",
    "    \"\"\"Tracks confusion during validation and updates similarity matrix dynamically.\"\"\"\n",
    "    def __init__(self, num_classes=NUM_CLASSES, initial_matrix=None, learning_rate=0.1):\n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.confusion_matrix = np.zeros((num_classes, num_classes), dtype=np.float32)\n",
    "        self.similarity_matrix = initial_matrix.cpu().numpy() if initial_matrix is not None else create_similarity_matrix().numpy()\n",
    "        \n",
    "    def update_confusion(self, predictions, targets):\n",
    "        \"\"\"Accumulate confusion from a batch of predictions.\"\"\"\n",
    "        for pred, target in zip(predictions, targets):\n",
    "            if 0 <= target < self.num_classes and 0 <= pred < self.num_classes:\n",
    "                self.confusion_matrix[target, pred] += 1\n",
    "    \n",
    "    def compute_similarity_from_confusion(self):\n",
    "        \"\"\"Convert confusion matrix to similarity scores.\"\"\"\n",
    "        # Normalize each row by the number of times that class appeared\n",
    "        row_sums = self.confusion_matrix.sum(axis=1, keepdims=True)\n",
    "        row_sums[row_sums == 0] = 1  # Avoid division by zero\n",
    "        normalized_confusion = self.confusion_matrix / row_sums\n",
    "        \n",
    "        # High confusion rate = high similarity\n",
    "        # Clip to [0, 1] and exclude diagonal (self-similarity stays 1.0)\n",
    "        similarity_from_confusion = normalized_confusion.copy()\n",
    "        np.fill_diagonal(similarity_from_confusion, 1.0)\n",
    "        \n",
    "        return similarity_from_confusion\n",
    "    \n",
    "    def update_similarity_matrix(self):\n",
    "        \"\"\"Update similarity matrix using exponential moving average of confusion patterns.\"\"\"\n",
    "        new_similarity = self.compute_similarity_from_confusion()\n",
    "        \n",
    "        # Exponential moving average: S_new = (1-lr) * S_old + lr * S_from_confusion\n",
    "        self.similarity_matrix = (1 - self.learning_rate) * self.similarity_matrix + \\\n",
    "                                  self.learning_rate * new_similarity\n",
    "        \n",
    "        # Reset confusion matrix for next validation period\n",
    "        self.confusion_matrix.fill(0)\n",
    "        \n",
    "        return torch.tensor(self.similarity_matrix, dtype=torch.float32)\n",
    "    \n",
    "    def get_similarity_matrix(self):\n",
    "        return torch.tensor(self.similarity_matrix, dtype=torch.float32)\n",
    "\n",
    "# Initialize dynamic similarity matrix manager\n",
    "dynamic_sim_matrix = DynamicSimilarityMatrix(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    initial_matrix=similarity_matrix,\n",
    "    learning_rate=0.1\n",
    ")\n",
    "\n",
    "print('Dynamic similarity matrix manager initialized.')\n",
    "print('Will update every validation epoch based on actual confusion patterns.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d38971",
   "metadata": {},
   "source": [
    "## 4. Custom Similarity-Aware Loss Function\n",
    "\n",
    "Similarity-aware top‚Äëk loss directly rewards the model when visually similar characters appear among its top‚Äëk predictions instead of considering only the single most confident output. If the model is uncertain between O and 0, having both in the top‚Äë2 with high confidence is treated as a near‚Äëcorrect outcome and should be penalized less than confidently predicting an unrelated character like X when the ground truth is O. This behavior aligns with the requirement of using ‚Äútop‚ÄëK outputs (e.g., top‚Äë2) rather than only the single best prediction,‚Äù allowing the loss to reflect graded correctness over a ranked list of hypotheses.\n",
    "\n",
    "Lapin et al. [3] formalize loss functions that explicitly operate on top‚Äëk predictions, showing that evaluating and optimizing with respect to top‚Äëk performance can better match practical retrieval and recognition objectives than standard top‚Äë1 losses.‚Äã\n",
    "\n",
    "References:\n",
    "\n",
    "\n",
    "[3] [Lapin, M., Hein, M., & Schiele, B. (2016). Loss Functions for Top‚Äëk Error: Analysis and Insights. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).](https://openaccess.thecvf.com/content_cvpr_2016/papers/Lapin_Loss_Functions_for_CVPR_2016_paper.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7221daf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity-aware loss defined.\n"
     ]
    }
   ],
   "source": [
    "class SimilarityAwareTopKLoss(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES, similarity_matrix=None,\n",
    "                 k=2, temperature=1.0, base_weight=0.7, topk_weight=0.3):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.k = k\n",
    "        self.temperature = temperature\n",
    "        self.base_weight = base_weight\n",
    "        self.topk_weight = topk_weight\n",
    "        if similarity_matrix is not None:\n",
    "            self.register_buffer('similarity_matrix', similarity_matrix)\n",
    "        else:\n",
    "            self.register_buffer('similarity_matrix', create_similarity_matrix())\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        B = logits.size(0)\n",
    "        device = logits.device\n",
    "\n",
    "        ce_loss = F.cross_entropy(logits, targets, reduction='none')\n",
    "        probs = F.softmax(logits / self.temperature, dim=1)\n",
    "        topk_probs, topk_indices = torch.topk(probs, self.k, dim=1)\n",
    "\n",
    "        sim_loss = torch.zeros(B, device=device)\n",
    "        for i in range(B):\n",
    "            t = targets[i].item()\n",
    "            sims = self.similarity_matrix[t][topk_indices[i]]\n",
    "            penalties = 1.0 - sims\n",
    "            weighted_penalties = topk_probs[i] * penalties\n",
    "            sim_loss[i] = weighted_penalties.sum()\n",
    "\n",
    "        total = self.base_weight * ce_loss + self.topk_weight * sim_loss\n",
    "        return total.mean()\n",
    "\n",
    "print('Similarity-aware loss defined.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff0ae7a",
   "metadata": {},
   "source": [
    "### 4.1. Loss Function Refinements: Temperature Annealing & Adaptive Weighting\n",
    "\n",
    "Temperature scheduling helps the model transition from exploration to exploitation. Early in training (high temperature), the model explores various character hypotheses with softer penalties. As training progresses (lower temperature), the model commits to more confident predictions. [4] This is crucial for OCR where early confusion helps learn feature relationships, but later training needs sharp decisions.\n",
    "\n",
    "Adaptive weighting based on prediction confidence dynamically balances between base cross-entropy and similarity-aware loss. When the model is uncertain (low confidence), we rely more on similarity-aware loss to guide learning with soft constraints. When confident, we trust the model's strong predictions and rely more on standard cross-entropy. This creates a self-regulating loss that adapts to the model's learning stage.\n",
    "\n",
    "References:\n",
    "\n",
    "[4] [Xuan, H. et al., ‚ÄúExploring the Impact of Temperature Scaling in Softmax for Classification and Adversarial Robustness.‚Äù (temperature controls smoothness and gradient behavior of softmax probabilities).](https://arxiv.org/html/2502.20604v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec6ad13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved similarity-aware loss with temperature annealing and adaptive weighting defined.\n"
     ]
    }
   ],
   "source": [
    "class ImprovedSimilarityAwareTopKLoss(nn.Module):\n",
    "    \"\"\"Enhanced loss with temperature annealing and confidence-based adaptive weighting.\"\"\"\n",
    "    def __init__(self, num_classes=NUM_CLASSES, similarity_matrix=None,\n",
    "                 k=2, initial_temperature=1.0, base_weight=0.7, topk_weight=0.3,\n",
    "                 epochs=300):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.k = k\n",
    "        self.initial_temperature = initial_temperature\n",
    "        self.base_weight = base_weight\n",
    "        self.topk_weight = topk_weight\n",
    "        self.epochs = epochs\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "        if similarity_matrix is not None:\n",
    "            self.register_buffer('similarity_matrix', similarity_matrix)\n",
    "        else:\n",
    "            self.register_buffer('similarity_matrix', create_similarity_matrix())\n",
    "\n",
    "    def update_epoch(self, epoch):\n",
    "        \"\"\"Update current epoch for temperature annealing.\"\"\"\n",
    "        self.current_epoch = epoch\n",
    "    \n",
    "    def get_temperature(self):\n",
    "        \"\"\"Anneal temperature from initial_temperature to 0.5 over training.\"\"\"\n",
    "        progress = self.current_epoch / max(self.epochs, 1)\n",
    "        return max(0.5, self.initial_temperature - progress * 0.8)\n",
    "    \n",
    "    def forward(self, logits, targets):\n",
    "        B = logits.size(0)\n",
    "        device = logits.device\n",
    "        \n",
    "        # Get current temperature for this epoch\n",
    "        temperature = self.get_temperature()\n",
    "        \n",
    "        ce_loss = F.cross_entropy(logits, targets, reduction='none')\n",
    "        probs = F.softmax(logits / temperature, dim=1)\n",
    "        topk_probs, topk_indices = torch.topk(probs, self.k, dim=1)\n",
    "        \n",
    "        # Compute similarity-aware loss\n",
    "        sim_loss = torch.zeros(B, device=device)\n",
    "        max_confidences = []\n",
    "        \n",
    "        for i in range(B):\n",
    "            t = targets[i].item()\n",
    "            sims = self.similarity_matrix[t][topk_indices[i]]\n",
    "            penalties = 1.0 - sims\n",
    "            weighted_penalties = topk_probs[i] * penalties\n",
    "            sim_loss[i] = weighted_penalties.sum()\n",
    "            max_confidences.append(topk_probs[i].max().item())\n",
    "        \n",
    "        # Adaptive weighting based on confidence\n",
    "        # Low confidence: rely more on similarity-aware loss (exploratory)\n",
    "        # High confidence: rely more on standard CE loss (exploitation)\n",
    "        confidence = torch.tensor(max_confidences, device=device)\n",
    "        adaptive_base_weight = self.base_weight * confidence + self.topk_weight * (1 - confidence)\n",
    "        adaptive_topk_weight = self.topk_weight * confidence + self.base_weight * (1 - confidence)\n",
    "        \n",
    "        # Normalize weights\n",
    "        total_weight = adaptive_base_weight + adaptive_topk_weight\n",
    "        adaptive_base_weight = adaptive_base_weight / total_weight\n",
    "        adaptive_topk_weight = adaptive_topk_weight / total_weight\n",
    "        \n",
    "        total = adaptive_base_weight * ce_loss + adaptive_topk_weight * sim_loss\n",
    "        return total.mean()\n",
    "\n",
    "print('Improved similarity-aware loss with temperature annealing and adaptive weighting defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd79036a",
   "metadata": {},
   "source": [
    "## 5. Sanity Check for Custom Loss\n",
    "\n",
    "Verify that confusing similar characters (O vs 0) incurs lower penalty than confusing very different characters (O vs X).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b78f7b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (O vs 0): 3.7470\n",
      "Loss (O vs X): 3.8926\n"
     ]
    }
   ],
   "source": [
    "loss_fn = SimilarityAwareTopKLoss(num_classes=NUM_CLASSES, similarity_matrix=similarity_matrix, k=2).to(device)\n",
    "\n",
    "logits_similar = torch.zeros(1, NUM_CLASSES, device=device)\n",
    "logits_similar[0, CHAR_TO_IDX['0']] = 5.0\n",
    "target_O = torch.tensor([CHAR_TO_IDX['O']], device=device)\n",
    "loss_similar = loss_fn(logits_similar, target_O)\n",
    "\n",
    "logits_diff = torch.zeros(1, NUM_CLASSES, device=device)\n",
    "logits_diff[0, CHAR_TO_IDX['X']] = 5.0\n",
    "loss_diff = loss_fn(logits_diff, target_O)\n",
    "\n",
    "print(f'Loss (O vs 0): {loss_similar.item():.4f}')\n",
    "print(f'Loss (O vs X): {loss_diff.item():.4f}')\n",
    "assert loss_similar < loss_diff, 'Expected O/0 confusion < O/X confusion'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce5cc08",
   "metadata": {},
   "source": [
    "the difference of ~0.15 is reasonable given:\n",
    "- base_weight=0.7 (standard cross-entropy dominates)\n",
    "- topk_weight=0.3 (similarity-aware component is 30%)\n",
    "- base_sim=0.6 (O and 0 have 60% similarity in the matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de06f195",
   "metadata": {},
   "source": [
    "## 6. Custom Segmentation Trainer with Similarity-Aware Character Loss\n",
    "\n",
    "Override YOLO's segmentation trainer to inject the similarity-aware loss into the character classification head. The model still outputs masks (via polygon supervision) and boxes, but the character class logits are trained with the custom loss instead of vanilla cross-entropy. This preserves mask quality while handling character confusion intelligently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc75c8f",
   "metadata": {},
   "source": [
    "### 6.1. OCR-Specific Validation Metrics\n",
    "\n",
    "Standard classification metrics (accuracy, precision, recall) don't capture OCR-specific challenges. Character Error Rate (CER) measures individual character mistakes, while Word Error Rate (WER) captures full plate correctness‚Äîcritical for real applications where partial plate reads are often useless. Top-2/3 accuracy shows if the correct character is among top predictions, indicating \"close but not quite\" scenarios. Similarity-aware accuracy gives partial credit for confusing similar characters (O vs 0), providing a more nuanced view of model performance that aligns with the similarity-aware loss. These metrics together give a complete picture of OCR quality beyond simple accuracy. \n",
    "\n",
    "CER and WER are standard OCR metrics, top‚Äëk accuracy is commonly used to capture ‚Äúclose but not quite‚Äù predictions, and the proposed ‚Äúsimilarity‚Äëaware accuracy‚Äù is a reasonable extension that aligns with the similarity‚Äëaware loss, even if it is not yet a standard metric. [5]\n",
    "\n",
    "[5] [Thakur, S. (2025). Evaluating OCR Output Quality with Character Error Rate (CER) and Word Error Rate (WER). Towards Data Science.](https://www.worldscientific.com/doi/abs/10.1142/S0218126623503218)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0534032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR Metrics Test Results:\n",
      "  CER: 0.6667\n",
      "  char_accuracy: 0.3333\n",
      "  top2_accuracy: 1.0000\n",
      "  top3_accuracy: 0.0000\n",
      "  similarity_aware_accuracy: 0.7333\n",
      "\n",
      "OCR metrics module ready for validation.\n"
     ]
    }
   ],
   "source": [
    "class OCRMetrics:\n",
    "    \"\"\"Compute OCR-specific validation metrics.\"\"\"\n",
    "    def __init__(self, similarity_matrix=None):\n",
    "        self.similarity_matrix = similarity_matrix if similarity_matrix is not None else create_similarity_matrix()\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset all accumulated metrics.\"\"\"\n",
    "        self.total_chars = 0\n",
    "        self.correct_chars = 0\n",
    "        self.total_plates = 0\n",
    "        self.correct_plates = 0\n",
    "        self.top2_correct = 0\n",
    "        self.top3_correct = 0\n",
    "        self.similarity_score = 0.0\n",
    "    \n",
    "    def update(self, predictions, targets, top_k_preds=None):\n",
    "        \"\"\"\n",
    "        Update metrics with a batch of predictions.\n",
    "        \n",
    "        Args:\n",
    "            predictions: Tensor of predicted class indices [B]\n",
    "            targets: Tensor of ground truth class indices [B]\n",
    "            top_k_preds: Optional tensor of top-k predictions [B, k] for top-k accuracy\n",
    "        \"\"\"\n",
    "        predictions = predictions.cpu().numpy()\n",
    "        targets = targets.cpu().numpy()\n",
    "        \n",
    "        # Character-level metrics\n",
    "        self.total_chars += len(targets)\n",
    "        self.correct_chars += (predictions == targets).sum()\n",
    "        \n",
    "        # Similarity-aware accuracy (partial credit for similar chars)\n",
    "        for pred, target in zip(predictions, targets):\n",
    "            if 0 <= target < len(self.similarity_matrix) and 0 <= pred < len(self.similarity_matrix):\n",
    "                sim = self.similarity_matrix[target][pred].item()\n",
    "                self.similarity_score += sim\n",
    "        \n",
    "        # Top-k accuracy\n",
    "        if top_k_preds is not None:\n",
    "            top_k_preds = top_k_preds.cpu().numpy()\n",
    "            for i, target in enumerate(targets):\n",
    "                if top_k_preds.shape[1] >= 2 and target in top_k_preds[i, :2]:\n",
    "                    self.top2_correct += 1\n",
    "                if top_k_preds.shape[1] >= 3 and target in top_k_preds[i, :3]:\n",
    "                    self.top3_correct += 1\n",
    "    \n",
    "    def update_plate(self, predicted_plate, target_plate):\n",
    "        \"\"\"\n",
    "        Update plate-level metrics (WER).\n",
    "        \n",
    "        Args:\n",
    "            predicted_plate: String of predicted plate characters\n",
    "            target_plate: String of ground truth plate characters\n",
    "        \"\"\"\n",
    "        self.total_plates += 1\n",
    "        if predicted_plate == target_plate:\n",
    "            self.correct_plates += 1\n",
    "    \n",
    "    def compute(self):\n",
    "        \"\"\"Compute all metrics and return as dictionary.\"\"\"\n",
    "        if self.total_chars == 0:\n",
    "            return {}\n",
    "        \n",
    "        metrics = {\n",
    "            'CER': 1.0 - (self.correct_chars / self.total_chars),  # Character Error Rate\n",
    "            'char_accuracy': self.correct_chars / self.total_chars,\n",
    "            'top2_accuracy': self.top2_correct / self.total_chars if self.total_chars > 0 else 0.0,\n",
    "            'top3_accuracy': self.top3_correct / self.total_chars if self.total_chars > 0 else 0.0,\n",
    "            'similarity_aware_accuracy': self.similarity_score / self.total_chars,\n",
    "        }\n",
    "        \n",
    "        if self.total_plates > 0:\n",
    "            metrics['WER'] = 1.0 - (self.correct_plates / self.total_plates)  # Word Error Rate\n",
    "            metrics['plate_accuracy'] = self.correct_plates / self.total_plates\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "# Initialize OCR metrics tracker\n",
    "ocr_metrics = OCRMetrics(similarity_matrix=similarity_matrix)\n",
    "\n",
    "# Test metrics with dummy data\n",
    "test_preds = torch.tensor([CHAR_TO_IDX['O'], CHAR_TO_IDX['1'], CHAR_TO_IDX['A']])\n",
    "test_targets = torch.tensor([CHAR_TO_IDX['0'], CHAR_TO_IDX['I'], CHAR_TO_IDX['A']])\n",
    "test_topk = torch.tensor([\n",
    "    [CHAR_TO_IDX['O'], CHAR_TO_IDX['0']],\n",
    "    [CHAR_TO_IDX['1'], CHAR_TO_IDX['I']],\n",
    "    [CHAR_TO_IDX['A'], CHAR_TO_IDX['B']],\n",
    "])\n",
    "\n",
    "ocr_metrics.update(test_preds, test_targets, test_topk)\n",
    "test_metrics = ocr_metrics.compute()\n",
    "\n",
    "print('OCR Metrics Test Results:')\n",
    "for key, value in test_metrics.items():\n",
    "    print(f'  {key}: {value:.4f}')\n",
    "\n",
    "print('\\nOCR metrics module ready for validation.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15722e6",
   "metadata": {},
   "source": [
    "### 6.2. Multi-Task Loss Weights\n",
    "\n",
    "The model performs three distinct tasks: segmentation (mask generation), localization (bounding boxes), and classification (character recognition). Default YOLO weighting may not be optimal for OCR, where classification accuracy is paramount. By explicitly balancing these losses (mask_weight=0.4, box_weight=0.3, cls_weight=0.3), we ensure the model doesn't over-prioritize segmentation quality at the expense of character recognition. These weights are tunable based on application needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "587c5e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-task loss weights configured:\n",
      "  Mask (segmentation): 0.4\n",
      "  Box (localization): 0.3\n",
      "  Class (recognition): 0.3\n",
      "  Total: 1.0\n",
      "\n",
      "These weights will be applied in the custom trainer to balance multi-task learning.\n"
     ]
    }
   ],
   "source": [
    "# Multi-task loss weights configuration\n",
    "MASK_WEIGHT = 0.4  # Segmentation mask loss weight\n",
    "BOX_WEIGHT = 0.3   # Bounding box loss weight  \n",
    "CLS_WEIGHT = 0.3   # Character classification loss weight\n",
    "\n",
    "print(f'Multi-task loss weights configured:')\n",
    "print(f'  Mask (segmentation): {MASK_WEIGHT:.1f}')\n",
    "print(f'  Box (localization): {BOX_WEIGHT:.1f}')\n",
    "print(f'  Class (recognition): {CLS_WEIGHT:.1f}')\n",
    "print(f'  Total: {MASK_WEIGHT + BOX_WEIGHT + CLS_WEIGHT:.1f}')\n",
    "\n",
    "print('\\nThese weights will be applied in the custom trainer to balance multi-task learning.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2fd9b7",
   "metadata": {},
   "source": [
    "## 6. Custom Segmentation Trainer with Enhanced Features\n",
    "\n",
    "Integrates all improvements: dynamic similarity matrix updates, temperature annealing, adaptive weighting, OCR metrics, and multi-task loss balancing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e08ad0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom segmentation trainer defined.\n"
     ]
    }
   ],
   "source": [
    "class CustomSegmentationTrainer(SegmentationTrainer):\n",
    "    \"\"\"\n",
    "    Custom trainer with:\n",
    "    - Dynamic similarity matrix updates\n",
    "    - Temperature annealing\n",
    "    - Adaptive loss weighting\n",
    "    - OCR-specific metrics\n",
    "    - Multi-task loss balancing\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg=None, overrides=None, _callbacks=None):\n",
    "        super().__init__(cfg, overrides, _callbacks)\n",
    "        \n",
    "        # Initialize improved loss function\n",
    "        self.character_loss_fn = ImprovedSimilarityAwareTopKLoss(\n",
    "            num_classes=NUM_CLASSES,\n",
    "            similarity_matrix=dynamic_sim_matrix.get_similarity_matrix(),\n",
    "            k=2,\n",
    "            initial_temperature=1.0,\n",
    "            base_weight=0.7,\n",
    "            topk_weight=0.3,\n",
    "            epochs=EPOCHS\n",
    "        ).to(device)\n",
    "        \n",
    "        # Initialize OCR metrics tracker\n",
    "        self.ocr_metrics = OCRMetrics(similarity_matrix=similarity_matrix)\n",
    "        \n",
    "        # Multi-task loss weights\n",
    "        self.mask_weight = MASK_WEIGHT\n",
    "        self.box_weight = BOX_WEIGHT\n",
    "        self.cls_weight = CLS_WEIGHT\n",
    "        \n",
    "    def on_train_epoch_start(self):\n",
    "        \"\"\"Called at the start of each training epoch.\"\"\"\n",
    "        super().on_train_epoch_start()\n",
    "        \n",
    "        # Update temperature in loss function\n",
    "        self.character_loss_fn.update_epoch(self.epoch)\n",
    "    \n",
    "    def on_val_start(self):\n",
    "        \"\"\"Called at the start of validation.\"\"\"\n",
    "        super().on_val_start()\n",
    "        self.ocr_metrics.reset()\n",
    "    \n",
    "    def on_val_end(self):\n",
    "        \"\"\"Called at the end of validation - update similarity matrix and log metrics.\"\"\"\n",
    "        super().on_val_end()\n",
    "        \n",
    "        # Update dynamic similarity matrix every 10 epochs\n",
    "        if self.epoch % 10 == 0 and self.epoch > 0:\n",
    "            new_similarity = dynamic_sim_matrix.update_similarity_matrix()\n",
    "            self.character_loss_fn.similarity_matrix = new_similarity.to(device)\n",
    "            print(f'[Epoch {self.epoch}] Similarity matrix updated from validation confusion patterns.')\n",
    "        \n",
    "        # Compute and log OCR metrics\n",
    "        ocr_results = self.ocr_metrics.compute()\n",
    "        if ocr_results:\n",
    "            print(f'\\n[Epoch {self.epoch}] OCR Metrics:')\n",
    "            for key, value in ocr_results.items():\n",
    "                print(f'  {key}: {value:.4f}')\n",
    "    \n",
    "    def compute_loss(self, preds, batch):\n",
    "        \"\"\"Compute multi-task loss with balanced weights.\"\"\"\n",
    "        # Get base YOLO losses (box, mask, class)\n",
    "        base_loss = super().compute_loss(preds, batch)\n",
    "        \n",
    "        # Apply multi-task weights to base loss components\n",
    "        # Note: This is a simplified approach. In practice, you'd decompose base_loss\n",
    "        # into its components and weight them individually\n",
    "        weighted_base_loss = base_loss * (self.mask_weight + self.box_weight) / 2\n",
    "        \n",
    "        # Add custom similarity-aware character classification loss\n",
    "        if len(preds) > 3:\n",
    "            cls_logits = preds[3]\n",
    "            cls_targets = batch['cls'].long()\n",
    "            \n",
    "            if cls_logits is not None and cls_targets is not None:\n",
    "                cls_logits_flat = cls_logits.view(-1, NUM_CLASSES)\n",
    "                cls_targets_flat = cls_targets.view(-1)\n",
    "                \n",
    "                valid_mask = cls_targets_flat >= 0\n",
    "                if valid_mask.sum() > 0:\n",
    "                    # Compute similarity-aware classification loss\n",
    "                    char_loss = self.character_loss_fn(\n",
    "                        cls_logits_flat[valid_mask],\n",
    "                        cls_targets_flat[valid_mask]\n",
    "                    )\n",
    "                    \n",
    "                    # Apply classification weight\n",
    "                    weighted_char_loss = self.cls_weight * char_loss\n",
    "                    \n",
    "                    # Update confusion matrix for dynamic similarity updates\n",
    "                    with torch.no_grad():\n",
    "                        preds_cls = cls_logits_flat[valid_mask].argmax(dim=1)\n",
    "                        dynamic_sim_matrix.update_confusion(\n",
    "                            preds_cls.cpu().numpy(),\n",
    "                            cls_targets_flat[valid_mask].cpu().numpy()\n",
    "                        )\n",
    "                        \n",
    "                        # Update OCR metrics\n",
    "                        top_k_preds = torch.topk(cls_logits_flat[valid_mask], k=3, dim=1)[1]\n",
    "                        self.ocr_metrics.update(\n",
    "                            preds_cls,\n",
    "                            cls_targets_flat[valid_mask],\n",
    "                            top_k_preds\n",
    "                        )\n",
    "                    \n",
    "                    # Combine losses\n",
    "                    total_loss = weighted_base_loss + weighted_char_loss\n",
    "                    return total_loss\n",
    "        \n",
    "        return weighted_base_loss\n",
    "\n",
    "print('Custom segmentation trainer defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bc7676",
   "metadata": {},
   "source": [
    "## 7. Training Configuration (Hyperparameters & Augmentations)\n",
    "\n",
    "Configure training hyperparameters tuned for character-level OCR on CCTV footage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd3ef3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters configured.\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 300\n",
    "BATCH_SIZE = 16\n",
    "IMG_SIZE = 224\n",
    "\n",
    "LR0 = 0.01\n",
    "LRF = 0.01\n",
    "MOMENTUM = 0.937\n",
    "WEIGHT_DECAY = 5e-4\n",
    "WARMUP_EPOCHS = 3.0\n",
    "WARMUP_MOMENTUM = 0.8\n",
    "WARMUP_BIAS_LR = 0.1\n",
    "\n",
    "AUG_HSV_H = 0.015\n",
    "AUG_HSV_S = 0.7\n",
    "AUG_HSV_V = 0.4\n",
    "AUG_ERASING = 0.4\n",
    "AUG_FLIPLR = 0.0\n",
    "AUG_MOSAIC = 0.0\n",
    "AUG_MIXUP = 0.0\n",
    "AUG_COPY_PASTE = 0.0\n",
    "\n",
    "print('Hyperparameters configured.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32243a00",
   "metadata": {},
   "source": [
    "### 7.1. Hyperparameter and Augmentation Rationale\n",
    "\n",
    "These settings aim to balance robustness, stability, and efficiency for text-level OCR on pre‚Äëaugmented character crops. SGD with momentum and weight decay, combined with cosine‚Äëannealed learning rate and brief warmup (LR0 = 0.01, LRF = 0.01, MOMENTUM = 0.937, WEIGHT_DECAY = 5e-4, WARMUP_EPOCHS = 3), follows recommended YOLO training practice and is known to improve convergence and final accuracy over simple step schedules in vision models [6].\n",
    "\n",
    "Moderate HSV jitter and random erasing (AUG_HSV_*, AUG_ERASING = 0.4) extend lighting and occlusion variability to better match CCTV conditions while preserving character structure [7].\n",
    "\n",
    "Horizontal flips and detection-style augmentations (Mosaic, MixUp, Copy-Paste) are disabled because mirrored or composited text does not occur in the target domain and can degrade OCR performance [8].\n",
    "\n",
    "References:  \n",
    "\n",
    "\n",
    "[6] [Ultralytics. *Hyperparameter Tuning Guide for YOLO Models*.] (https://docs.ultralytics.com/guides/hyperparameter-tuning/).\n",
    "[7] [Zhong, Z., et al. (2020). *Random Erasing Data Augmentation*.](https://arxiv.org/abs/1902.07296)\n",
    "[8] [Eikvil, L. (1993). *Optical Character Recognition*.](https://home.nr.no/~eikvil/OCR.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c091b6d6",
   "metadata": {},
   "source": [
    "## 8. Initialize Model and Attach Custom Trainer\n",
    "\n",
    "Load YOLO11-seg as the backbone and plug in the custom trainer with similarity-aware character loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a3b7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model not found. Downloading yolo11n-seg.pt...\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-seg.pt to 'yolo11n-seg.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5.9MB 3.2MB/s 1.8s.8s<0.0s4.1s1s\n",
      "Model downloaded and moved to: models\\yolo11n-seg.pt\n",
      "Segmentation model initialized with custom trainer from: models\\yolo11n-seg.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = 'yolo11n-seg.pt'\n",
    "MODEL_DIR = 'models'\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, MODEL_NAME)\n",
    "\n",
    "# Custom training checkpoint names\n",
    "CUSTOM_LAST = os.path.join(MODEL_DIR, 'custom_ocr_last.pt')\n",
    "CUSTOM_BEST = os.path.join(MODEL_DIR, 'custom_ocr_best.pt')\n",
    "CUSTOM_CURRENT = os.path.join(MODEL_DIR, 'custom_ocr.pt')\n",
    "\n",
    "# Check if we should resume training from existing checkpoint\n",
    "RESUME_TRAINING = os.path.exists(CUSTOM_LAST)\n",
    "\n",
    "if RESUME_TRAINING:\n",
    "    # Resume from last checkpoint\n",
    "    model_location = CUSTOM_LAST\n",
    "    print(f'üîÑ Resuming training from checkpoint: {CUSTOM_LAST}')\n",
    "else:\n",
    "    # Start fresh - check if base model exists locally\n",
    "    if os.path.exists(MODEL_NAME):\n",
    "        model_location = MODEL_NAME\n",
    "        print(f'Loading base model from: {MODEL_NAME}')\n",
    "    elif os.path.exists(MODEL_PATH):\n",
    "        model_location = MODEL_PATH\n",
    "        print(f'Loading base model from: {MODEL_PATH}')\n",
    "    else:\n",
    "        # Download base model\n",
    "        print(f'Base model not found. Downloading {MODEL_NAME}...')\n",
    "        os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "        model = YOLO(MODEL_NAME)\n",
    "        \n",
    "        # Move to models directory\n",
    "\n",
    "        if os.path.exists(MODEL_NAME) and not os.path.exists(MODEL_PATH):\n",
    "\n",
    "            import shutilprint(f'Training mode: {\"RESUME\" if RESUME_TRAINING else \"NEW\"}')\n",
    "\n",
    "            shutil.move(MODEL_NAME, MODEL_PATH)print(f'Segmentation model initialized with custom trainer')\n",
    "\n",
    "            print(f'Model downloaded and moved to: {MODEL_PATH}')\n",
    "\n",
    "        model.trainer = CustomSegmentationTrainer\n",
    "\n",
    "        model_location = MODEL_PATHmodel = YOLO(model_location)\n",
    "\n",
    "    print(f'Starting new training session')# Load model and attach custom trainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c395c73",
   "metadata": {},
   "source": [
    "## 9. Early Stopping Callback\n",
    "\n",
    "Halt training if validation loss stalls for a prolonged period to prevent overfitting and wasted compute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37cfc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping callback implemented.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Training metrics CSV\n",
    "METRICS_CSV = os.path.join(MODEL_DIR, 'training_metrics.csv')\n",
    "CSV_INITIALIZED = os.path.exists(METRICS_CSV)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "no_improve_epochs = 0\n",
    "EARLY_STOP_PATIENCE = 50\n",
    "\n",
    "def save_metrics_to_csv(epoch, metrics):\n",
    "    \"\"\"Save training metrics to CSV for later analysis.\"\"\"\n",
    "    global CSV_INITIALIZED\n",
    "    \n",
    "    # Prepare row data\n",
    "    row = {\n",
    "        'timestamp': datetime.datetime.now().isoformat(),\n",
    "        'epoch': epoch,\n",
    "        **{k: v for k, v in metrics.items() if isinstance(v, (int, float))}\n",
    "    }\n",
    "    \n",
    "    # Write to CSV\n",
    "    file_exists = os.path.exists(METRICS_CSV)\n",
    "    with open(METRICS_CSV, 'a', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=row.keys())\n",
    "        if not file_exists or not CSV_INITIALIZED:\n",
    "            writer.writeheader()\n",
    "            CSV_INITIALIZED = True\n",
    "        writer.writerow(row)\n",
    "\n",
    "\n",
    "def save_checkpoint_callback(trainer):\n",
    "    \"\"\"Save last checkpoint with custom name after each epoch.\"\"\"\n",
    "    last_weights = os.path.join(str(trainer.save_dir), 'weights', 'last.pt')\n",
    "    \n",
    "    if os.path.exists(last_weights):\n",
    "        shutil.copy2(last_weights, CUSTOM_LAST)\n",
    "        shutil.copy2(last_weights, CUSTOM_CURRENT)\n",
    "\n",
    "\n",
    "def early_stopping_callback(trainer):\n",
    "    global best_val_loss, no_improve_epochs\n",
    "    \n",
    "    metrics = trainer.metrics or {}\n",
    "    val_loss = metrics.get('loss', None)\n",
    "    \n",
    "    # Save metrics to CSV\n",
    "    if metrics:\n",
    "        save_metrics_to_csv(trainer.epoch, metrics)\n",
    "    \n",
    "    if val_loss is None:\n",
    "        return\n",
    "    \n",
    "    if best_val_loss == float('inf'):\n",
    "        best_val_loss = val_loss\n",
    "        no_improve_epochs = 0\n",
    "        return\n",
    "    \n",
    "    improvement = (best_val_loss - val_loss) / max(best_val_loss, 1e-8) * 100.0\n",
    "    \n",
    "    if improvement >= 1.0:\n",
    "        best_val_loss = val_loss\n",
    "        no_improve_epochs = 0\n",
    "        \n",
    "        # Save best model with custom name\n",
    "        best_weights = os.path.join(str(trainer.save_dir), 'weights', 'best.pt')\n",
    "        if os.path.exists(best_weights):\n",
    "            shutil.copy2(best_weights, CUSTOM_BEST)\n",
    "            print(f'Best model saved to: {CUSTOM_BEST}')\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "    \n",
    "    if no_improve_epochs >= EARLY_STOP_PATIENCE:\n",
    "        print(f'Early stopping at epoch {trainer.epoch}')\n",
    "        trainer.stop = True\n",
    "\n",
    "\n",
    "# Configure callbacks\n",
    "model.add_callback('on_epoch_end', save_checkpoint_callback)\n",
    "model.add_callback('on_val_end', early_stopping_callback)\n",
    "\n",
    "print('Training callbacks configured:')\n",
    "print(f'  Metrics logging to: {METRICS_CSV}')\n",
    "print(f'  Auto-save checkpoints to models/')\n",
    "print(f'  Early stopping (patience: {EARLY_STOP_PATIENCE} epochs)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376bd529",
   "metadata": {},
   "source": [
    "## 10. Train Segmentation Model with Similarity-Aware Character Loss\n",
    "\n",
    "Train YOLO11-seg on polygon annotations with the custom trainer. The model learns to segment character regions (mask) while classifying each character (O vs 0 etc.) with reduced penalties for visually similar confusions. Make sure `DATA_YAML_PATH` points to your dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2731dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No checkpoint. Training from scratch.\n",
      "   Starting fresh training session\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Ultralytics 8.3.240  Python-3.11.3 torch-2.9.1+cpu CPU (11th Gen Intel Core(TM) i3-1115G4 3.00GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=False, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=dataset/data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=300, erasing=0.4, exist_ok=True, fliplr=0.0, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=224, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=models\\yolo11n-seg.pt, momentum=0.937, mosaic=0.0, multi_scale=False, name=seg_with_similarity_loss, nbs=64, nms=False, opset=None, optimize=False, optimizer=SGD, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=philippine_lp_ocr, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=C:\\Users\\lifei\\OneDrive\\Desktop\\CSC173 - Intelligent Systems\\CSC173-DeepCV-Sanchez\\philippine_lp_ocr\\seg_with_similarity_loss, save_frames=False, save_json=False, save_period=10, save_txt=False, scale=0.5, seed=42, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=segment, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    690460  ultralytics.nn.modules.head.Segment          [36, 32, 64, [64, 128, 256]]  \n",
      "YOLO11n-seg summary: 203 layers, 2,849,628 parameters, 2,849,612 gradients, 9.8 GFLOPs\n",
      "\n",
      "Transferred 561/561 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.1 ms, read: 811.01058.7 MB/s, size: 218.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\lifei\\OneDrive\\Desktop\\CSC173 - Intelligent Systems\\CSC173-DeepCV-Sanchez\\dataset\\train\\labels... 15960 images, 1 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 15960/15960 45.1it/s 5:54<0.1s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\lifei\\OneDrive\\Desktop\\CSC173 - Intelligent Systems\\CSC173-DeepCV-Sanchez\\dataset\\train\\labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.20.1 ms, read: 11.38.8 MB/s, size: 242.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\lifei\\OneDrive\\Desktop\\CSC173 - Intelligent Systems\\CSC173-DeepCV-Sanchez\\dataset\\val\\labels... 2000 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2000/2000 36.9it/s 54.2s<0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\lifei\\OneDrive\\Desktop\\CSC173 - Intelligent Systems\\CSC173-DeepCV-Sanchez\\dataset\\val\\labels.cache\n",
      "Plotting labels to C:\\Users\\lifei\\OneDrive\\Desktop\\CSC173 - Intelligent Systems\\CSC173-DeepCV-Sanchez\\philippine_lp_ocr\\seg_with_similarity_loss\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.937) with parameter groups 90 weight(decay=0.0), 101 weight(decay=0.0005), 100 bias(decay=0.0)\n",
      "Image sizes 224 train, 224 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mC:\\Users\\lifei\\OneDrive\\Desktop\\CSC173 - Intelligent Systems\\CSC173-DeepCV-Sanchez\\philippine_lp_ocr\\seg_with_similarity_loss\u001b[0m\n",
      "Starting training for 300 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      1/300         0G      2.476      4.136      5.416      1.298        120        224: 1% ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 12/998 4.1s/it 56.0s<1:08:06\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     56\u001b[39m     train_params[\u001b[33m'\u001b[39m\u001b[33mresume\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStarting training...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m results = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining completed!\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     63\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mResults directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults.save_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lifei\\OneDrive\\Desktop\\CSC173 - Intelligent Systems\\CSC173-DeepCV-Sanchez\\.venv\\Lib\\site-packages\\ultralytics\\engine\\model.py:773\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    770\u001b[39m     \u001b[38;5;28mself\u001b[39m.trainer.model = \u001b[38;5;28mself\u001b[39m.trainer.get_model(weights=\u001b[38;5;28mself\u001b[39m.model \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg=\u001b[38;5;28mself\u001b[39m.model.yaml)\n\u001b[32m    771\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m.trainer.model\n\u001b[32m--> \u001b[39m\u001b[32m773\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    774\u001b[39m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[32m    775\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {-\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m}:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lifei\\OneDrive\\Desktop\\CSC173 - Intelligent Systems\\CSC173-DeepCV-Sanchez\\.venv\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:243\u001b[39m, in \u001b[36mBaseTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    240\u001b[39m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lifei\\OneDrive\\Desktop\\CSC173 - Intelligent Systems\\CSC173-DeepCV-Sanchez\\.venv\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:436\u001b[39m, in \u001b[36mBaseTrainer._do_train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    434\u001b[39m \u001b[38;5;28mself\u001b[39m.scaler.scale(\u001b[38;5;28mself\u001b[39m.loss).backward()\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ni - last_opt_step >= \u001b[38;5;28mself\u001b[39m.accumulate:\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    437\u001b[39m     last_opt_step = ni\n\u001b[32m    439\u001b[39m     \u001b[38;5;66;03m# Timed stopping\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lifei\\OneDrive\\Desktop\\CSC173 - Intelligent Systems\\CSC173-DeepCV-Sanchez\\.venv\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:687\u001b[39m, in \u001b[36mBaseTrainer.optimizer_step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    685\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n\u001b[32m    686\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ema:\n\u001b[32m--> \u001b[39m\u001b[32m687\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mema\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lifei\\OneDrive\\Desktop\\CSC173 - Intelligent Systems\\CSC173-DeepCV-Sanchez\\.venv\\Lib\\site-packages\\ultralytics\\utils\\torch_utils.py:-1\u001b[39m, in \u001b[36mModelEMA.update\u001b[39m\u001b[34m(self, model)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Ensure checkpoint paths are defined\n",
    "if 'CUSTOM_LAST' not in globals():\n",
    "    MODEL_DIR = 'models'\n",
    "    CUSTOM_LAST = os.path.join(MODEL_DIR, 'custom_ocr_last.pt')\n",
    "    CUSTOM_BEST = os.path.join(MODEL_DIR, 'custom_ocr_best.pt')\n",
    "    CUSTOM_CURRENT = os.path.join(MODEL_DIR, 'custom_ocr.pt')\n",
    "    METRICS_CSV = os.path.join(MODEL_DIR, 'training_metrics.csv')\n",
    "\n",
    "# Check if checkpoint exists to determine mode\n",
    "RESUME_TRAINING = os.path.exists(CUSTOM_LAST)\n",
    "\n",
    "if RESUME_TRAINING:\n",
    "    print(f'\\nResuming from checkpoint: {CUSTOM_LAST}')\n",
    "    print(f'   Training will continue from last saved epoch')\n",
    "else:\n",
    "    print(f'\\nNo checkpoint. Training from scratch.')\n",
    "    print(f'   Starting fresh training session')\n",
    "\n",
    "# Configure training parameters\n",
    "train_params = dict(\n",
    "    data=DATA_YAML_PATH,\n",
    "    epochs=EPOCHS,\n",
    "    batch=BATCH_SIZE,\n",
    "    imgsz=IMG_SIZE,\n",
    "    optimizer='SGD',\n",
    "    lr0=LR0,\n",
    "    lrf=LRF,\n",
    "    momentum=MOMENTUM,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    warmup_epochs=WARMUP_EPOCHS,\n",
    "    warmup_momentum=WARMUP_MOMENTUM,\n",
    "    warmup_bias_lr=WARMUP_BIAS_LR,\n",
    "    hsv_h=AUG_HSV_H,\n",
    "    hsv_s=AUG_HSV_S,\n",
    "    hsv_v=AUG_HSV_V,\n",
    "    erasing=AUG_ERASING,\n",
    "    fliplr=AUG_FLIPLR,\n",
    "    mosaic=AUG_MOSAIC,\n",
    "    mixup=AUG_MIXUP,\n",
    "    copy_paste=AUG_COPY_PASTE,\n",
    "    project=RUN_PROJECT,\n",
    "    name=RUN_NAME,\n",
    "    exist_ok=True,\n",
    "    val=True,\n",
    "    save=True,\n",
    "    save_period=10,\n",
    "    amp=False,\n",
    "    device=device,  # Use auto-detected device from Section 2\n",
    "    seed=42,\n",
    "    deterministic=True,\n",
    ")\n",
    "\n",
    "if RESUME_TRAINING:\n",
    "    train_params['resume'] = True\n",
    "\n",
    "print(f'\\nStarting training...')\n",
    "print(f'Device: {device}')\n",
    "print()\n",
    "\n",
    "results = model.train(**train_params)\n",
    "\n",
    "print('\\nTraining completed!')\n",
    "print(f'Results directory: {results.save_dir}')\n",
    "print(f'\\nModel checkpoints saved to models/ folder:')\n",
    "print(f'  - Current: {CUSTOM_CURRENT}')\n",
    "print(f'  - Best: {CUSTOM_BEST}')\n",
    "print(f'  - Last: {CUSTOM_LAST}')\n",
    "print(f'\\nTraining metrics: {METRICS_CSV}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff953db",
   "metadata": {},
   "source": [
    "## 11. Export Best Model\n",
    "\n",
    "Copy the best weights to the export directory for inference and deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a73b857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "\n",
    "export_path = os.path.join(EXPORT_DIR, f'{RUN_NAME}_best.pt')\n",
    "\n",
    "if os.path.exists(CUSTOM_BEST):\n",
    "    shutil.copy2(CUSTOM_BEST, export_path)\n",
    "    print(f'Best model exported to: {export_path}')\n",
    "    print(f'  Source: {CUSTOM_BEST}')\n",
    "else:\n",
    "    print('Best checkpoint not found. Training may not have completed.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cb92b2",
   "metadata": {},
   "source": [
    "## 12. Inference on Test Images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
